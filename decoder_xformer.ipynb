{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# baic transformer Decoder model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import xformers.ops as xops\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "from torch import Tensor\n",
    "import random\n",
    "\n",
    "main_df = pd.read_csv('adult.csv')\n",
    "main_df.head()\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>78</td>\n",
       "      <td>97</td>\n",
       "      <td>164</td>\n",
       "      <td>185</td>\n",
       "      <td>199</td>\n",
       "      <td>209</td>\n",
       "      <td>220</td>\n",
       "      <td>225</td>\n",
       "      <td>229</td>\n",
       "      <td>230</td>\n",
       "      <td>253</td>\n",
       "      <td>341</td>\n",
       "      <td>437</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>78</td>\n",
       "      <td>88</td>\n",
       "      <td>174</td>\n",
       "      <td>187</td>\n",
       "      <td>197</td>\n",
       "      <td>207</td>\n",
       "      <td>217</td>\n",
       "      <td>227</td>\n",
       "      <td>229</td>\n",
       "      <td>230</td>\n",
       "      <td>253</td>\n",
       "      <td>351</td>\n",
       "      <td>437</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>76</td>\n",
       "      <td>104</td>\n",
       "      <td>170</td>\n",
       "      <td>190</td>\n",
       "      <td>197</td>\n",
       "      <td>213</td>\n",
       "      <td>217</td>\n",
       "      <td>227</td>\n",
       "      <td>229</td>\n",
       "      <td>230</td>\n",
       "      <td>253</td>\n",
       "      <td>341</td>\n",
       "      <td>437</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>78</td>\n",
       "      <td>93</td>\n",
       "      <td>178</td>\n",
       "      <td>188</td>\n",
       "      <td>197</td>\n",
       "      <td>209</td>\n",
       "      <td>217</td>\n",
       "      <td>225</td>\n",
       "      <td>229</td>\n",
       "      <td>237</td>\n",
       "      <td>253</td>\n",
       "      <td>341</td>\n",
       "      <td>437</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>89</td>\n",
       "      <td>178</td>\n",
       "      <td>188</td>\n",
       "      <td>199</td>\n",
       "      <td>202</td>\n",
       "      <td>220</td>\n",
       "      <td>227</td>\n",
       "      <td>228</td>\n",
       "      <td>230</td>\n",
       "      <td>253</td>\n",
       "      <td>331</td>\n",
       "      <td>437</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt  education  educational-num  marital-status  \\\n",
       "0    8         78      97        164              185             199   \n",
       "1   21         78      88        174              187             197   \n",
       "2   11         76     104        170              190             197   \n",
       "3   27         78      93        178              188             197   \n",
       "4    1         74      89        178              188             199   \n",
       "\n",
       "   occupation  relationship  race  gender  capital-gain  capital-loss  \\\n",
       "0         209           220   225     229           230           253   \n",
       "1         207           217   227     229           230           253   \n",
       "2         213           217   227     229           230           253   \n",
       "3         209           217   225     229           237           253   \n",
       "4         202           220   227     228           230           253   \n",
       "\n",
       "   hours-per-week  native-country  income  \n",
       "0             341             437     440  \n",
       "1             351             437     440  \n",
       "2             341             437     441  \n",
       "3             341             437     441  \n",
       "4             331             437     440  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def POOL_preprocess(df):\n",
    "    '''\n",
    "    input the original dataframe, output the dataframe after preprocessing,\n",
    "    change the numerical columns to categorical columns by qcut and cut\n",
    "    then apply label encoding to all columns\n",
    "    \n",
    "    '''\n",
    "    df = df.copy()\n",
    "    CAT = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country', 'income']\n",
    "    NUM = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "    # qcut on numerical columns\n",
    "    for column in NUM:\n",
    "        if column in ['educational-num','capital-gain','capital-loss','hours-per-week']:\n",
    "            df[column] = pd.cut(df[column], 100)\n",
    "        else:\n",
    "            df[column] = pd.cut(df[column], 100)\n",
    "    # make income column binary\n",
    "    df['income'] = df['income'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "\n",
    "    # lable encoding categorical columns\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    lb = LabelEncoder()\n",
    "    df = df.apply(lambda x: lb.fit_transform(x))\n",
    "\n",
    "    # make all catagory in every column unique\n",
    "\n",
    "    # 迴圈處理多個欄位\n",
    "    offset = 0\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].apply(lambda x: x + offset)\n",
    "        offset += df[column].nunique()\n",
    "    \n",
    "    return df\n",
    "tmp = POOL_preprocess(main_df)\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data num: 48842\n",
      "trian data num: 39074\n",
      "test data num: 9768\n"
     ]
    }
   ],
   "source": [
    "train_size = 4*48842//5\n",
    "test_size = 48842//5\n",
    "train_pool = main_df[test_size:]\n",
    "test_pool = main_df[:test_size]\n",
    "print('total data num:' , main_df.shape[0])\n",
    "print('trian data num:' , train_pool.shape[0])\n",
    "print('test data num:' , test_pool.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notations\n",
    "#   node: number of all nodes = L + S + C + F\n",
    "#   L: number of lable nodes\n",
    "#   S: number of sample nodes\n",
    "#   C: number of catagory nodes\n",
    "#   F: number of field(column) nodes\n",
    "#   hidden: number of hidden representation\n",
    "\n",
    "# data size = (node, hidden)\n",
    "# mask size = (node, node - L) without lable nodes\n",
    "#             for each node, real mask = cat[mask,(node,L)] = (node, node)\n",
    "#             cannot see it's label node\n",
    "\n",
    "# use nn.transformerDecoder(data,mask) to get the output\n",
    "# use the above output as input of MLP to predict the lable   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data num: 48842\n",
      "trian data num: 39073\n",
      "test data num: 9769\n",
      "node_nums {'L': 3, 'S': 39074, 'C': 437, 'F': 14}\n",
      "total 39528 nodes\n",
      "L_input torch.cuda.FloatTensor torch.Size([3, 3])\n",
      "S_input torch.cuda.DoubleTensor torch.Size([39074, 14])\n",
      "C_input torch.cuda.LongTensor torch.Size([437, 437])\n",
      "F_input torch.cuda.FloatTensor torch.Size([14, 14])\n",
      "torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "class HGNN_DataSet():\n",
    "    def __init__(self,\n",
    "                 data_df : pd.DataFrame,\n",
    "                 split_ratio : float ,\n",
    "                 label_column : str,\n",
    "                 ):\n",
    "        test_size = math.ceil(data_df.shape[0] * (1-split_ratio))\n",
    "        train_pool = data_df[test_size:]\n",
    "        test_pool = data_df[:test_size]\n",
    "        print('total data num:' , data_df.shape[0])\n",
    "        print('trian data num:' , train_pool.shape[0])\n",
    "        print('test data num:' , test_pool.shape[0])\n",
    "        \n",
    "        # to-dos:\n",
    "        # train\n",
    "        #   \n",
    "        \n",
    "        TARGET_POOL = POOL_preprocess(train_pool)\n",
    "        LABEL_COLUMN = label_column\n",
    "\n",
    "        # cut feature and lable\n",
    "        FEATURE_POOL = TARGET_POOL.drop(LABEL_COLUMN, axis=1)\n",
    "        LABEL_POOL = TARGET_POOL[LABEL_COLUMN]\n",
    "\n",
    "        # trasform label into one-hot\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        enc = OneHotEncoder()\n",
    "        LABEL_POOL = enc.fit_transform(LABEL_POOL.values.reshape(-1,1)).toarray()\n",
    "\n",
    "        # L: number of lable nodes\n",
    "        # the last node of Lable nodes is served as unknown lable node\n",
    "        L = LABEL_POOL.shape[1] + 1\n",
    "\n",
    "        # S: number of sample nodes\n",
    "        S = FEATURE_POOL.shape[0] + 1\n",
    "        # the last node of sample nodes is served as infering node\n",
    "\n",
    "        # C: number of catagory nodes\n",
    "        C = FEATURE_POOL.apply(lambda x: x.nunique()).sum() # total_unique_labels\n",
    "        C_POOL = sorted(list(set(FEATURE_POOL.values.flatten())))\n",
    "        # the last node of catagory nodes is served as Unseen node\n",
    "        C += 1\n",
    "        C_POOL.append(C_POOL[-1] + 1)\n",
    "\n",
    "        # F: number of field(column) nodes\n",
    "        F = FEATURE_POOL.shape[1]\n",
    "\n",
    "        nodes_num = {'L':L, 'S':S, 'C':C, 'F':F}\n",
    "        print('node_nums', nodes_num)\n",
    "        print('total', L+S+C+F, 'nodes')\n",
    "        \n",
    "        self.TARGET_POOL = TARGET_POOL\n",
    "        self.LABEL_COLUMN = LABEL_COLUMN\n",
    "        self.FEATURE_POOL = FEATURE_POOL\n",
    "        self.LABEL_POOL = LABEL_POOL\n",
    "        self.C_POOL = C_POOL   \n",
    "        self.nodes_num = nodes_num\n",
    "\n",
    "        \n",
    "        self.make_input_tensor()\n",
    "        # self.get_sample(10)        \n",
    "        self.make_mask_all()\n",
    "        \n",
    "        # self.make_mask()\n",
    "        \n",
    "        \n",
    "    def make_mask(self,\n",
    "                  sample_indices: Optional[list] = None,\n",
    "                ):\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "\n",
    "        sample_size = len(sample_indices)\n",
    "        masked_POOL = self.TARGET_POOL.iloc[sample_indices]\n",
    "        # caculate masking\n",
    "        masks = {}\n",
    "        \n",
    "    \n",
    "        \n",
    "        # label to sample \n",
    "        tmp = torch.zeros([math.ceil(sample_size/8) * 8, math.ceil(L/8) * 8], dtype=torch.float).to(DEVICE)\n",
    "        label_ids = self.TARGET_POOL[self.LABEL_COLUMN].unique()\n",
    "        for i, value_df in enumerate(masked_POOL[self.LABEL_COLUMN]):\n",
    "            for j, value_label in enumerate(label_ids):\n",
    "                if value_label == value_df:\n",
    "                    tmp[i][j] = 1\n",
    "                    break\n",
    "        masks['L2S'] = Tensor.contiguous(tmp)\n",
    "\n",
    "        # sample to catagory\n",
    "        tmp = torch.zeros([math.ceil(C/8) * 8, math.ceil(sample_size/8) * 8], dtype=torch.float).to(DEVICE)\n",
    "        tmp_df = masked_POOL.drop(self.LABEL_COLUMN, axis=1)\n",
    "        for i, value_df in enumerate(tmp_df.values):\n",
    "            for j, value in enumerate(value_df):\n",
    "                tmp[value][i] = 1\n",
    "        masks['S2C'] = Tensor.contiguous(tmp)\n",
    "\n",
    "        # catagory to field\n",
    "        masks['C2F'] = Tensor.contiguous(self.MASKS_FULL['C2F'])\n",
    "        \n",
    "        self.MASKS = masks\n",
    "        self.nodes_num['K'] = sample_size\n",
    "        \n",
    "    def make_mask_all(self,\n",
    "                  sample_indices: Optional[torch.tensor] = None,\n",
    "                ):\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        # caculate masking\n",
    "        masks = {}\n",
    "        \n",
    "        # label to sample \n",
    "        tmp = torch.zeros([math.ceil(S/8) * 8, math.ceil(L/8) * 8], dtype=torch.float)\n",
    "        label_ids = self.TARGET_POOL[self.LABEL_COLUMN].unique()\n",
    "        for i, value_df in enumerate(self.TARGET_POOL[self.LABEL_COLUMN]):\n",
    "            for j, value_label in enumerate(label_ids):\n",
    "                if value_label == value_df:\n",
    "                    tmp[i][j] = 1\n",
    "                    break\n",
    "        masks['L2S'] = tmp\n",
    "\n",
    "        # sample to catagory\n",
    "        tmp = torch.zeros([math.ceil(C/8) * 8, math.ceil(S/8) * 8], dtype=torch.float)\n",
    "        tmp_df = self.TARGET_POOL.drop(self.LABEL_COLUMN, axis=1)\n",
    "        for i, value_df in enumerate(tmp_df.values):\n",
    "            for j, value in enumerate(value_df):\n",
    "                tmp[value][i] = 1\n",
    "        masks['S2C'] = tmp\n",
    "\n",
    "        # catagory to field\n",
    "        tmp = torch.zeros([math.ceil(F/8) * 8, math.ceil(C/8) * 8], dtype=torch.float)\n",
    "        unique_items = [(self.TARGET_POOL[column].unique()) for column in (self.TARGET_POOL.columns)]\n",
    "        for i in range(F):\n",
    "            for j in (unique_items[i]):\n",
    "                tmp[i][j] = 1\n",
    "        masks['C2F'] = tmp\n",
    "        \n",
    "        \n",
    "        self.MASKS = masks\n",
    "        self.MASKS_FULL = masks\n",
    "    \n",
    "    def make_input_tensor(self):\n",
    "        # make input tensor\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        # L\n",
    "        L_input = torch.eye(L).to(DEVICE)\n",
    "        print('L_input', L_input.type(), L_input.shape)\n",
    "        # S\n",
    "        S_input = torch.tensor(np.vstack([self.FEATURE_POOL.values, np.zeros(F)])).to(DEVICE)\n",
    "        print('S_input', S_input.type(), S_input.shape)\n",
    "        # C random init\n",
    "        C_input = torch.tensor(np.diag(self.C_POOL)).to(DEVICE)\n",
    "        print('C_input', C_input.type(), C_input.shape)\n",
    "        # F random init\n",
    "        F_input = torch.eye(F).to(DEVICE)\n",
    "        print('F_input', F_input.type(), F_input.shape)\n",
    "        print(L_input.type())\n",
    "        # \n",
    "        self.INPUTS = (L_input, S_input, C_input, F_input)\n",
    "        self.INPUT_DIMS = (L_input.size(1), S_input.size(1), C_input.size(1), F_input.size(1))\n",
    "        \n",
    "    def get_sample(self, sample_size, inculde = []):\n",
    "        # get K samples from S\n",
    "        # return sample node mask\n",
    "        S = self.nodes_num['S']\n",
    "        \n",
    "        # inculde specific nodes (e.g. query nodes), while remaining sample_size\n",
    "        # -1 is infering node\n",
    "        sample_indices = sorted(torch.randperm(S-1)[:sample_size - len(inculde)])\n",
    "        if inculde is not []:\n",
    "            while inculde in sample_indices:\n",
    "                sample_indices = (torch.randperm(S-1)[:sample_size - len(inculde)])\n",
    "            # add inculde nodes into sample_indices\n",
    "            for node in inculde:\n",
    "                sample_indices.append(node)\n",
    "            sample_indices = sorted(sample_indices)\n",
    "        # update mask\n",
    "        self.make_mask(sample_indices)\n",
    "        \n",
    "        # update input tensor\n",
    "        L_input, S_input, C_input, F_input = self.INPUTS\n",
    "        S_input_masked = torch.index_select(S_input, 0, torch.tensor([int(x) for x in sample_indices]).to(DEVICE))\n",
    "        self.MASKED_INPUTS = (L_input, S_input_masked, C_input, F_input)   \n",
    "        return sample_indices\n",
    "            \n",
    "Train_data = HGNN_DataSet( main_df, 0.8, 'income')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14., 14., 14.,  ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Train_data.MASKS['S2C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(3554),\n",
       " tensor(3646),\n",
       " tensor(6067),\n",
       " tensor(11243),\n",
       " tensor(14169),\n",
       " tensor(14817),\n",
       " tensor(17038),\n",
       " tensor(21902),\n",
       " tensor(30528),\n",
       " tensor(33065)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.get_sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Optional, Any, Union, Callable\n",
    "\n",
    "class CustomTransformerDecoderLayer(nn.TransformerDecoderLayer):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu'):\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        # remove defined modules\n",
    "        delattr(self, 'self_attn')\n",
    "        delattr(self, 'norm1')\n",
    "        delattr(self, 'dropout1')\n",
    "    \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        x = tgt\n",
    "        if self.norm_first:\n",
    "            # x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)\n",
    "            x = x + self._mha_block(self.norm2(x), memory, memory_mask)\n",
    "            x = x + self._ff_block(self.norm3(x))\n",
    "        else:\n",
    "            # x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))\n",
    "            x = self.norm2(x + self._mha_block(x, memory, memory_mask))\n",
    "            # x =  x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask)\n",
    "            # x = self.norm3(x + self._ff_block(x))\n",
    "\n",
    "        return x\n",
    "    def _mha_block(self, x: Tensor, mem: Tensor,\n",
    "                   attn_mask: Optional[Tensor],) -> Tensor:\n",
    "        x = xops.memory_efficient_attention(x, mem, mem, attn_mask)\n",
    "        return self.dropout2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = Train_data.MASKS['L2S'].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dims (3, 14, 437, 14)\n",
      "模型輸出的大小: torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "# baic transformer decoder model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "from tqdm import trange\n",
    "\n",
    "class TransformerDecoderModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 target_dataset, \n",
    "                 num_layers, \n",
    "                 embedding_dim,  \n",
    "                 subgraph_masked: Optional[bool] = False,\n",
    "                 K : Optional[int] = 10,\n",
    "                 ):\n",
    "        super(TransformerDecoderModel, self).__init__()\n",
    "\n",
    "        L_dim, S_dim, C_dim, F_dim = target_dataset.INPUT_DIMS\n",
    "        \n",
    "        # 目前b卡在embedding的怎麼用\n",
    "        # Catagory_embedding => 數值類Qcut後用linear來做embedding, 類別用nn.Embedding\n",
    "\n",
    "        self.Lable_embedding = nn.Linear(L_dim, embedding_dim, dtype=torch.float)\n",
    "        self.Sample_embedding = nn.Linear(S_dim, embedding_dim, dtype=torch.float)\n",
    "        self.Catagory_embedding = nn.Linear(C_dim, embedding_dim, dtype=torch.float)\n",
    "        self.Field_embedding = nn.Linear(F_dim, embedding_dim, dtype=torch.float)\n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            CustomTransformerDecoderLayer(embedding_dim,  nhead = 1 ),\n",
    "            num_layers\n",
    "        )\n",
    "        \n",
    "        # downstream task\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 2),\n",
    "            nn.Softmax(dim=2),\n",
    "            # nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.subgraph_masked = subgraph_masked\n",
    "        if subgraph_masked: \n",
    "            self.K = K\n",
    "        else:\n",
    "            # init mask\n",
    "            target_dataset.make_mask_all()\n",
    "        \n",
    "        self.tmpmask_L2S = target_dataset.MASKS['L2S'].clone().to(DEVICE)\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "                target_dataset: HGNN_DataSet, \n",
    "                ):\n",
    "        L, S, C, F = target_dataset.nodes_num['L'], target_dataset.nodes_num['S'], target_dataset.nodes_num['C'], target_dataset.nodes_num['F']\n",
    "        \n",
    "        if self.subgraph_masked: \n",
    "            target_dataset.get_sample(self.K)\n",
    "            L_input, S_input, C_input, F_input = target_dataset.MASKED_INPUTS\n",
    "            masks = target_dataset.MASKS\n",
    "            K = target_dataset.nodes_num['K']\n",
    "            S_ = K\n",
    "        else:\n",
    "            target_dataset.make_input_tensor()\n",
    "            L_input, S_input, C_input, F_input = target_dataset.INPUTS\n",
    "            target_dataset.make_mask_all()\n",
    "            masks = target_dataset.MASKS\n",
    "            S_ = S\n",
    "            \n",
    "    \n",
    "        L_embedded = self.Lable_embedding(L_input.float()).unsqueeze(0)\n",
    "        S_embedded = self.Sample_embedding(S_input.float()).unsqueeze(0)\n",
    "        C_embedded = self.Catagory_embedding(C_input.float()).unsqueeze(0)\n",
    "        F_embedded = self.Field_embedding(F_input.float()).unsqueeze(0)\n",
    "        \n",
    "        for mask in masks.keys():\n",
    "            masks[mask] = masks[mask].to(DEVICE)\n",
    "        \n",
    "        # propagate steps: L→S→C→F\n",
    "        #                  L←S←C←\n",
    "        # more steps more menory usage\n",
    "        PROPAGATE_STEPS = 1\n",
    "        for i in range(PROPAGATE_STEPS):\n",
    "            S_embedded = self.transformer_decoder(S_embedded,L_embedded, \n",
    "                                                memory_mask = self.tmpmask_L2S[:S_,:L]) \n",
    "            C_embedded = self.transformer_decoder(C_embedded,S_embedded,\n",
    "                                                memory_mask = masks['S2C'][:C,:S_])\n",
    "            F_embedded = self.transformer_decoder(F_embedded,C_embedded,\n",
    "                                                memory_mask = masks['C2F'][:F,:C])\n",
    "            C_embedded = self.transformer_decoder(C_embedded,F_embedded,\n",
    "                                                memory_mask = Tensor.contiguous(masks['C2F'].transpose(0, 1))[:C,:F])\n",
    "            S_embedded = self.transformer_decoder(S_embedded,C_embedded,\n",
    "                                                memory_mask = Tensor.contiguous(masks['S2C'].transpose(0, 1))[:S_,:C])\n",
    "            L_embedded = self.transformer_decoder(L_embedded,S_embedded, \n",
    "                                                memory_mask = Tensor.contiguous(self.tmpmask_L2S.transpose(0, 1))[:L,:S_])\n",
    "        \n",
    "        \n",
    "        output = self.MLP(S_embedded)[0]\n",
    "        return output\n",
    "\n",
    "# 測試模型\n",
    "num_layers = 1  # TransformerDecoder 的層數\n",
    "embedding_dim = 2*128  # 嵌入維度\n",
    "hidden_dim = 64  \n",
    "\n",
    "print('input_dims', Train_data.INPUT_DIMS)\n",
    "model = TransformerDecoderModel(Train_data, num_layers, embedding_dim, subgraph_masked = True, K = 10).to(DEVICE)\n",
    "outputs = model(Train_data)\n",
    "\n",
    "print(\"模型輸出的大小:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L2S': tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0'),\n",
       " 'S2C': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'C2F': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.MASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_label = torch.argmax(outputs, dim=1)\n",
    "output_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/39073 [00:00<06:46, 96.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 12122/39073 [02:14<05:05, 88.24it/s]"
     ]
    }
   ],
   "source": [
    "# training\n",
    "from torch import autograd\n",
    "from torcheval.metrics.aggregation.auc import AUC\n",
    "from torcheval.metrics import BinaryAUROC\n",
    "\n",
    "\n",
    "def train(model, datset):\n",
    "    LABEL_POOL = datset.LABEL_POOL\n",
    "    \n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        Original_L2S_mask = datset.MASKS['L2S'].clone()\n",
    "        # logs\n",
    "        loss_log = []\n",
    "        AUC_metric = BinaryAUROC()\n",
    "        \n",
    "        for index in trange(datset.nodes_num['S']-1): # query through all sample nodes (not infering node)\n",
    "            # for all query, input = sample K + 1 query smaple\n",
    "            sample_indices = datset.get_sample(10, inculde = [index])\n",
    "            \n",
    "            \n",
    "            # modify the mask to mask out the queries node's edge to it's label node\n",
    "            L = datset.nodes_num['L']\n",
    "            query_index = sample_indices.index(index)\n",
    "            datset.tmpmask_L2S = Original_L2S_mask.clone()\n",
    "            datset.tmpmask_L2S[query_index] = 0\n",
    "            datset.tmpmask_L2S[query_index][L-1] = 1 # make it as unseen label\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(datset)[query_index]\n",
    "            # for trainning, only the query node's output is used\n",
    "\n",
    "            # caculate loss\n",
    "            if model.subgraph_masked:\n",
    "                # get the real label fo query node\n",
    "                LABEL_POOL_ = LABEL_POOL[index]\n",
    "            else:\n",
    "                LABEL_POOL_ = LABEL_POOL\n",
    "            batch_loss = criterion(outputs, torch.tensor(LABEL_POOL_).to(DEVICE))\n",
    "            loss_log.append(batch_loss.item())\n",
    "            \n",
    "            # backpropagation\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            # TRUE = torch.argmax(torch.tensor(LABEL_POOL_).to(DEVICE), dim=1)\n",
    "            TRUE = torch.argmax(torch.tensor(LABEL_POOL_).to(DEVICE))\n",
    "            # y_pred_first = torch.tensor([x[0] for x in outputs]).to(DEVICE)\n",
    "            y_pred_first = outputs[0].to(DEVICE)\n",
    "            # AUC_metric.update(TRUE,y_pred_first)\n",
    "            AUC_metric.update(torch.Tensor(TRUE),torch.Tensor(y_pred_first))\n",
    "            \n",
    "        epoch_loss = sum(loss_log)/(datset.nodes_num['S']-1)\n",
    "        epoch_AUC = float(AUC_metric.compute())\n",
    "\n",
    "        print(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC: {epoch_AUC}\")\n",
    "model = TransformerDecoderModel(Train_data, num_layers, embedding_dim, subgraph_masked = True).to(DEVICE)\n",
    "train(model, Train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.Sample_embedding = nn.Linear(S_dim, embedding_dim, dtype=torch.half)\n",
    "# self.Catagory_embedding = nn.Linear(C_dim, embedding_dim, dtype=torch.half)\n",
    "# S_embedded = self.Sample_embedding(S_input.half()).unsqueeze(0)\n",
    "# C_embedded = self.Catagory_embedding(C_input.half()).unsqueeze(0)\n",
    "# xops.memory_efficient_attention(C_embedded, S_embedded, S_embedded,attn_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
