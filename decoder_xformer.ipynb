{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# baic transformer Decoder model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import xformers.ops as xops\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "from torch import Tensor\n",
    "import random\n",
    "\n",
    "main_df = pd.read_csv('adult.csv')\n",
    "main_df.head()\n",
    "DEVICE = 'cuda'\n",
    "# DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_interval(interval_str, number):\n",
    "    # decompose the interval string\n",
    "    lower_bound = float(interval_str[1:interval_str.index(\",\")])\n",
    "    upper_bound = float(interval_str[interval_str.index(\",\")+1:-1])\n",
    "    inclusive_lower = interval_str[0] == \"(\"\n",
    "    inclusive_upper = interval_str[-1] == \"]\"\n",
    "\n",
    "    # judge if the number is in the interval\n",
    "    if inclusive_lower:\n",
    "        is_in_range = number >= lower_bound\n",
    "    else:\n",
    "        is_in_range = number > lower_bound\n",
    "\n",
    "    if inclusive_upper:\n",
    "        is_in_range = is_in_range and number <= upper_bound\n",
    "    else:\n",
    "        is_in_range = is_in_range and number < upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dicationary for the categorical features\n",
    "def build_dict(df, dump = False):\n",
    "    dict = {}\n",
    "    for col in df.columns:\n",
    "        dict_col = {}\n",
    "        for i, item in enumerate(df[col].unique()):\n",
    "            dict_col[str(item)] = i\n",
    "        dict_col['UNSEEN'] = i + 1\n",
    "        dict[str(col)] = dict_col\n",
    "    # offset the item index\n",
    "    offset = 0\n",
    "    for column in dict.keys():\n",
    "        for key in dict[column].keys():\n",
    "            dict[column][key] += offset\n",
    "        offset += len(dict[column].keys())\n",
    "        \n",
    "    if dump:\n",
    "        import json\n",
    "        with open('dict.json', 'w') as f:\n",
    "            f.write(json.dumps(dict, indent=4))\n",
    "    return dict\n",
    "\n",
    "# build_dict(train_pool,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>114</td>\n",
       "      <td>240</td>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "      <td>539</td>\n",
       "      <td>604</td>\n",
       "      <td>611</td>\n",
       "      <td>631</td>\n",
       "      <td>642</td>\n",
       "      <td>654</td>\n",
       "      <td>660</td>\n",
       "      <td>665</td>\n",
       "      <td>706</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>105</td>\n",
       "      <td>253</td>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "      <td>550</td>\n",
       "      <td>604</td>\n",
       "      <td>621</td>\n",
       "      <td>629</td>\n",
       "      <td>640</td>\n",
       "      <td>651</td>\n",
       "      <td>662</td>\n",
       "      <td>665</td>\n",
       "      <td>706</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>121</td>\n",
       "      <td>273</td>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "      <td>539</td>\n",
       "      <td>602</td>\n",
       "      <td>617</td>\n",
       "      <td>629</td>\n",
       "      <td>646</td>\n",
       "      <td>651</td>\n",
       "      <td>662</td>\n",
       "      <td>665</td>\n",
       "      <td>706</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>110</td>\n",
       "      <td>260</td>\n",
       "      <td>307</td>\n",
       "      <td>400</td>\n",
       "      <td>539</td>\n",
       "      <td>604</td>\n",
       "      <td>625</td>\n",
       "      <td>629</td>\n",
       "      <td>642</td>\n",
       "      <td>651</td>\n",
       "      <td>660</td>\n",
       "      <td>665</td>\n",
       "      <td>706</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>260</td>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "      <td>529</td>\n",
       "      <td>600</td>\n",
       "      <td>625</td>\n",
       "      <td>631</td>\n",
       "      <td>635</td>\n",
       "      <td>654</td>\n",
       "      <td>662</td>\n",
       "      <td>664</td>\n",
       "      <td>706</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  educational-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0   10     114              240           300           400             539   \n",
       "1   28     105              253           300           400             550   \n",
       "2   15     121              273           300           400             539   \n",
       "3   36     110              260           307           400             539   \n",
       "4    1     106              260           300           400             529   \n",
       "\n",
       "   workclass  education  marital-status  occupation  relationship  race  \\\n",
       "0        604        611             631         642           654   660   \n",
       "1        604        621             629         640           651   662   \n",
       "2        602        617             629         646           651   662   \n",
       "3        604        625             629         642           651   660   \n",
       "4        600        625             631         635           654   662   \n",
       "\n",
       "   gender  native-country  income  \n",
       "0     665             706     710  \n",
       "1     665             706     710  \n",
       "2     665             706     711  \n",
       "3     665             706     711  \n",
       "4     664             706     710  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "def POOL_preprocess(df, N_BINS = 100):\n",
    "    \n",
    "    CAT = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country', 'income']\n",
    "    NUM = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "    \n",
    "    num_CAT = len(CAT)\n",
    "    num_NUM = len(NUM)  \n",
    "    \n",
    "    ct = ColumnTransformer([\n",
    "        (\"age\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"age\"]),\n",
    "        (\"fnlwgt\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"fnlwgt\"]),\n",
    "        (\"educational-num\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"educational-num\"]),\n",
    "        (\"capital-gain\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"capital-gain\"]),\n",
    "        (\"capital-loss\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"capital-loss\"]),\n",
    "        (\"hours-per-week\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"hours-per-week\"]),\n",
    "         ],remainder = 'passthrough', verbose_feature_names_out = False) # make sure columns are unique\n",
    "    ct.set_output(transform = 'pandas')\n",
    "    X_trans = ct.fit_transform(df) \n",
    "    \n",
    "    # make catagoy in NUM columns unique\n",
    "    # each NUM column has N_BINS unique values, that is, each NUM column represents as N_BINS node\n",
    "    offset = 0\n",
    "    for column in NUM:\n",
    "        X_trans[column] = X_trans[column].apply(lambda x: x + offset)\n",
    "        offset += N_BINS\n",
    "    \n",
    "    # apply lable encoding on CAT columns\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    lb = LabelEncoder()\n",
    "    X_trans[CAT] = X_trans[CAT].apply(lambda x: lb.fit_transform(x))\n",
    "    \n",
    "    # make catagoy all columns unique\n",
    "    # each column has it's own number of unique values. '+1' is for unseen values\n",
    "    offset = len(NUM) * N_BINS\n",
    "    for column in CAT:\n",
    "        X_trans[column] = X_trans[column].apply(lambda x: x + offset)\n",
    "        offset += X_trans[column].nunique() + 1\n",
    "    \n",
    "    X_trans = X_trans.astype(int)\n",
    "    return X_trans, ct, (num_NUM, num_CAT - 1) # -1 is for the income column (label)\n",
    "X_trans, _, _= POOL_preprocess(main_df)\n",
    "X_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def POOL_preprocess(df):\n",
    "#     '''\n",
    "#     input the original dataframe, output the dataframe after preprocessing,\n",
    "#     change the numerical columns to categorical columns by qcut and cut\n",
    "#     then apply label encoding to all columns\n",
    "    \n",
    "#     '''\n",
    "#     df_ = df.copy()\n",
    "#     CAT = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country', 'income']\n",
    "#     NUM = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "#     # qcut on numerical columns\n",
    "#     for column in NUM:\n",
    "#         if column in ['educational-num','capital-gain','capital-loss','hours-per-week']:\n",
    "#             df_[column] = pd.cut(df_[column], 100)\n",
    "#         else:\n",
    "#             df_[column] = pd.cut(df_[column], 100)\n",
    "#     # make income column binary\n",
    "#     # df['income'] = df['income'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "    \n",
    "#     build_dict(df_, True)\n",
    "\n",
    "#     # convert df into unique index\n",
    "#     import json\n",
    "#     index_dict = open('dict.json', 'r')\n",
    "#     index_dict = json.load(index_dict)\n",
    "#     for column in CAT:\n",
    "#         df[column] = df[column].replace(index_dict[column])\n",
    "#     from tqdm import tqdm\n",
    "#     for column in NUM:\n",
    "        \n",
    "#         # 這個時間太久了，要改\n",
    "#         for row in tqdm(df[column].index):\n",
    "#             for i, item in enumerate(index_dict[column]):\n",
    "#                 if item == 'UNSEEN':\n",
    "#                     break\n",
    "#                 if is_in_interval(item, df[column][row]):\n",
    "#                     df[column][row] = item\n",
    "#                     break\n",
    "#                 # if not in the interval, assign the unseen index\n",
    "#                 df[column][row] = index_dict[column]['UNSEEN']\n",
    "\n",
    "    \n",
    "#     return df\n",
    "# tmp = POOL_preprocess(main_df)\n",
    "# tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data num: 48842\n",
      "trian data num: 39074\n",
      "test data num: 9768\n"
     ]
    }
   ],
   "source": [
    "train_size = 4*48842//5\n",
    "test_size = 48842//5\n",
    "train_pool = main_df[test_size:]\n",
    "test_pool = main_df[:test_size]\n",
    "print('total data num:' , main_df.shape[0])\n",
    "print('trian data num:' , train_pool.shape[0])\n",
    "print('test data num:' , test_pool.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notations\n",
    "#   node: number of all nodes = L + S + C + F\n",
    "#   L: number of lable nodes\n",
    "#   S: number of sample nodes\n",
    "#   C: number of catagory nodes\n",
    "#   F: number of field(column) nodes\n",
    "#   hidden: number of hidden representation\n",
    "\n",
    "# data size = (node, hidden)\n",
    "# mask size = (node, node - L) without lable nodes\n",
    "#             for each node, real mask = cat[mask,(node,L)] = (node, node)\n",
    "#             cannot see it's label node\n",
    "\n",
    "# use nn.transformerDecoder(data,mask) to get the output\n",
    "# use the above output as input of MLP to predict the lable   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data num: 48842\n",
      "trian data num: 39073\n",
      "test data num: 9769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_nums {'L': 3, 'S': 39074, 'C': 708, 'F': 14}\n",
      "total 39799 nodes\n",
      "L_input torch.cuda.LongTensor torch.Size([3, 1])\n",
      "S_input torch.cuda.FloatTensor torch.Size([39074, 14])\n",
      "C_input torch.cuda.LongTensor torch.Size([708, 1])\n",
      "F_input torch.cuda.LongTensor torch.Size([14, 1])\n"
     ]
    }
   ],
   "source": [
    "class HGNN_DataSet():\n",
    "    def __init__(self,\n",
    "                 data_df : pd.DataFrame,\n",
    "                 split_ratio : float ,\n",
    "                 label_column : str,\n",
    "                 ):\n",
    "        test_size = math.ceil(data_df.shape[0] * (1-split_ratio))\n",
    "        train_pool = data_df[test_size:]\n",
    "        test_pool = data_df[:test_size]\n",
    "        print('total data num:' , data_df.shape[0])\n",
    "        print('trian data num:' , train_pool.shape[0])\n",
    "        print('test data num:' , test_pool.shape[0])\n",
    "        \n",
    "        # to-dos:\n",
    "        # train\n",
    "        #   \n",
    "        N_BINS = 100\n",
    "        TARGET_POOL, CT, NUM_vs_CAT = POOL_preprocess(train_pool, N_BINS = N_BINS)\n",
    "        # TEST_POOL = POOL_preprocess(test_pool)\n",
    "        LABEL_COLUMN = label_column\n",
    "\n",
    "        # cut feature and lable\n",
    "        FEATURE_POOL = TARGET_POOL.drop(LABEL_COLUMN, axis=1)\n",
    "        LABEL_POOL = TARGET_POOL[LABEL_COLUMN]\n",
    "\n",
    "        # trasform label into one-hot\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        enc = OneHotEncoder()\n",
    "        LABEL_POOL = enc.fit_transform(LABEL_POOL.values.reshape(-1,1)).toarray()\n",
    "\n",
    "        # L: number of lable nodes\n",
    "        # the last node of Lable nodes is served as unknown lable node\n",
    "        L = LABEL_POOL.shape[1] + 1\n",
    "\n",
    "        # S: number of sample nodes\n",
    "        S = FEATURE_POOL.shape[0] + 1\n",
    "        # the last node of sample nodes is served as infering node\n",
    "        \n",
    "        # F: number of field(column) nodes\n",
    "        F = FEATURE_POOL.shape[1]\n",
    "\n",
    "        # C: number of catagory nodes\n",
    "        C = FEATURE_POOL.to_numpy().max()  # total_unique_labels, which includes unseen nodes\n",
    "        C_POOL = range(int(C))\n",
    "\n",
    "        nodes_num = {'L':L, 'S':S, 'C':C, 'F':F}\n",
    "        print('node_nums', nodes_num)\n",
    "        print('total', L+S+C+F, 'nodes')\n",
    "        \n",
    "        # get true training sample\n",
    "        self.labe_to_index = {}\n",
    "        tmp_pool = TARGET_POOL.copy().reset_index()\n",
    "        for label in tmp_pool['income'].unique():\n",
    "            self.labe_to_index[label] = (tmp_pool[tmp_pool['income'] == label].index).tolist()\n",
    "        \n",
    "        self.TARGET_POOL = TARGET_POOL\n",
    "        # self.TEST_POOL = TEST_POOL\n",
    "        self.LABEL_COLUMN = LABEL_COLUMN\n",
    "        self.FEATURE_POOL = FEATURE_POOL\n",
    "        self.LABEL_POOL = LABEL_POOL\n",
    "        self.C_POOL = C_POOL   \n",
    "        self.nodes_num = nodes_num\n",
    "        self.NUM_vs_CAT = NUM_vs_CAT\n",
    "        self.CT = CT\n",
    "        self.N_BINS = N_BINS\n",
    "\n",
    "        \n",
    "        self.make_input_tensor()\n",
    "        # self.get_sample(10)        \n",
    "        self.make_mask_all()\n",
    "        \n",
    "        # self.make_mask()\n",
    "        \n",
    "        \n",
    "    def make_mask(self,\n",
    "                  sample_indices: Optional[list] = None,\n",
    "                ):\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "\n",
    "        sample_size = len(sample_indices)\n",
    "        masked_POOL = self.TARGET_POOL.iloc[sample_indices]\n",
    "        # caculate masking\n",
    "        masks = {}\n",
    "\n",
    "        # label to sample \n",
    "        tmp = torch.zeros([math.ceil(sample_size/8) * 8, math.ceil(L/8) * 8], dtype=torch.float).to(DEVICE)\n",
    "        label_ids = self.TARGET_POOL[self.LABEL_COLUMN].unique()\n",
    "        for i, value_df in enumerate(masked_POOL[self.LABEL_COLUMN]):\n",
    "            for j, value_label in enumerate(label_ids):\n",
    "                if value_label == value_df:\n",
    "                    tmp[i][j] = 1\n",
    "                    break\n",
    "        masks['L2S'] = Tensor.contiguous(tmp)\n",
    "\n",
    "        # sample to catagory\n",
    "        tmp = torch.zeros([math.ceil(C/8) * 8, math.ceil(sample_size/8) * 8], dtype=torch.float).to(DEVICE)\n",
    "        tmp_df = masked_POOL.drop(self.LABEL_COLUMN, axis=1)\n",
    "        for i, value_df in enumerate(tmp_df.values):\n",
    "            for j, value in enumerate(value_df):\n",
    "                tmp[value][i] = 1\n",
    "        masks['S2C'] = Tensor.contiguous(tmp)\n",
    "\n",
    "        # catagory to field\n",
    "        masks['C2F'] = Tensor.contiguous(self.MASKS_FULL['C2F'])\n",
    "        \n",
    "        self.MASKS = masks\n",
    "        self.nodes_num['K'] = sample_size\n",
    "        \n",
    "    def make_mask_all(self,\n",
    "                  sample_indices: Optional[torch.tensor] = None,\n",
    "                ):\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        # caculate masking\n",
    "        masks = {}\n",
    "        \n",
    "        # label to sample \n",
    "        tmp = torch.zeros([math.ceil(S/8) * 8, math.ceil(L/8) * 8], dtype=torch.float)\n",
    "        label_ids = self.TARGET_POOL[self.LABEL_COLUMN].unique()\n",
    "        for i, value_df in enumerate(self.TARGET_POOL[self.LABEL_COLUMN]):\n",
    "            for j, value_label in enumerate(label_ids):\n",
    "                if value_label == value_df:\n",
    "                    tmp[i][j] = 1\n",
    "                    break\n",
    "        masks['L2S'] = tmp\n",
    "\n",
    "        # sample to catagory\n",
    "        tmp = torch.zeros([math.ceil(C/8) * 8, math.ceil(S/8) * 8], dtype=torch.float)\n",
    "        tmp_df = self.TARGET_POOL.drop(self.LABEL_COLUMN, axis=1)\n",
    "        for i, value_df in enumerate(tmp_df.values):\n",
    "            for j, value in enumerate(value_df):\n",
    "                tmp[value][i] = 1\n",
    "        masks['S2C'] = tmp\n",
    "\n",
    "        # catagory to field\n",
    "        tmp = torch.zeros([math.ceil(F/8) * 8, math.ceil(C/8) * 8], dtype=torch.float)\n",
    "        unique_items = [(self.TARGET_POOL[column].unique()) for column in (self.TARGET_POOL.columns)]\n",
    "        for i in range(F):\n",
    "            for j in (unique_items[i]):\n",
    "                tmp[i][j] = 1\n",
    "        masks['C2F'] = tmp\n",
    "        \n",
    "        \n",
    "        self.MASKS = masks\n",
    "        self.MASKS_FULL = masks\n",
    "    \n",
    "    def make_mask_all_infer(self,\n",
    "                            # index of infering nodes in test_pool\n",
    "                            infering_node_index: Optional[int] = None , \n",
    "                            \n",
    "                        ):\n",
    "        '''\n",
    "        currently, only one infering node is supported\n",
    "        not implemented yet\n",
    "        unseen node\n",
    "        '''\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        # caculate masking\n",
    "        masks = {}\n",
    "        \n",
    "        # label to sample \n",
    "        tmp = torch.zeros([math.ceil(S/8) * 8, math.ceil(L/8) * 8], dtype=torch.float)\n",
    "        label_ids = self.TARGET_POOL[self.LABEL_COLUMN].unique()\n",
    "        for i, value_df in enumerate(self.TARGET_POOL[self.LABEL_COLUMN]):\n",
    "            for j, value_label in enumerate(label_ids):\n",
    "                if value_label == value_df:\n",
    "                    tmp[i][j] = 1\n",
    "                    break\n",
    "        # infering node is located at the last S \n",
    "        tmp[S - 1][label_ids + 1] = 1\n",
    "        masks['L2S'] = tmp\n",
    "\n",
    "        # sample to catagory\n",
    "        tmp = torch.zeros([math.ceil(C/8) * 8, math.ceil(S/8) * 8], dtype=torch.float)\n",
    "        tmp_df = self.TARGET_POOL.drop(self.LABEL_COLUMN, axis=1)\n",
    "        for i, value_df in enumerate(tmp_df.values):\n",
    "            for j, value in enumerate(value_df):\n",
    "                tmp[value][i] = 1\n",
    "        # infering node \n",
    "        # get the infering node's catagory\n",
    "        tmp_df = self.TEST_POOL.drop(self.LABEL_COLUMN, axis=1)[infering_node_index]\n",
    "        for i, value_df in enumerate(tmp_df.values):\n",
    "            for j, value in enumerate(value_df):\n",
    "                # # out of range 為解決\n",
    "                # 目前問題：無法檢測到unseen catagory\n",
    "                # 因為沒看過的會被lable成有看過的，只有最後一個會是unseen\n",
    "                if value > C:\n",
    "                    tmp[value][i] = 1\n",
    "        \n",
    "        masks['S2C'] = tmp\n",
    "\n",
    "        # catagory to field\n",
    "        tmp = torch.zeros([math.ceil(F/8) * 8, math.ceil(C/8) * 8], dtype=torch.float)\n",
    "        unique_items = [(self.TARGET_POOL[column].unique()) for column in (self.TARGET_POOL.columns)]\n",
    "        for i in range(F):\n",
    "            for j in (unique_items[i]):\n",
    "                tmp[i][j] = 1\n",
    "        masks['C2F'] = tmp\n",
    "        \n",
    "        \n",
    "        self.MASKS = masks\n",
    "        self.MASKS_FULL = masks\n",
    "    \n",
    "    def make_input_tensor(self):\n",
    "        # make input tensor\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        # L\n",
    "        L_input = torch.tensor([range(L)]).to(DEVICE).reshape(-1,1)\n",
    "        print('L_input', L_input.type(), L_input.shape)\n",
    "        # S (normalized)\n",
    "        normalized_features = Fun.normalize(torch.tensor(self.FEATURE_POOL.values, dtype = torch.float).to(DEVICE), p=2, dim=0)\n",
    "        S_input = torch.cat([normalized_features, torch.tensor([[0]*F]).to(DEVICE)],dim = 0).to(DEVICE) # add infering node\n",
    "        print('S_input', S_input.type(), S_input.shape)\n",
    "        # C \n",
    "        C_input = torch.tensor([self.C_POOL]).to(DEVICE).reshape(-1,1)\n",
    "        print('C_input', C_input.type(), C_input.shape)\n",
    "        # F \n",
    "        F_input = torch.tensor([range(F)]).to(DEVICE).reshape(-1,1)\n",
    "        print('F_input', F_input.type(), F_input.shape)\n",
    "        # \n",
    "        self.INPUTS = (L_input, S_input, C_input, F_input)\n",
    "        self.INPUT_DIMS = (L_input.size(1), S_input.size(1), C_input.size(1), F_input.size(1))\n",
    "        \n",
    "    def sample_with_distrubution(self, sample_size):\n",
    "        # sample with distrubution\n",
    "        \"\"\"\n",
    "        currently, only support binary label\n",
    "        \"\"\"\n",
    "        indices = []\n",
    "        for i in range(sample_size):\n",
    "            label = random.choice(list(self.labe_to_index.keys()))\n",
    "            indices.append(torch.tensor(random.choice(self.labe_to_index[label])))\n",
    "            # print(label)\n",
    "        return indices\n",
    "            \n",
    "        \n",
    "    def get_sample(self, sample_size, inculde = []):\n",
    "        # get K samples from S\n",
    "        # return sample node mask\n",
    "        S = self.nodes_num['S']\n",
    "        \n",
    "        # inculde specific nodes (e.g. query nodes), while remaining sample_size\n",
    "        # -1 is infering node\n",
    "        sample_indices = self.sample_with_distrubution(sample_size - len(inculde))\n",
    "        if inculde is not []:\n",
    "            while inculde in sample_indices:\n",
    "                sample_indices = self.sample_with_distrubution(sample_size - len(inculde))\n",
    "            # add inculde nodes into sample_indices\n",
    "            for node in inculde:\n",
    "                sample_indices.append(torch.tensor(node))\n",
    "            sample_indices = sorted(sample_indices)\n",
    "        # update mask\n",
    "        self.make_mask(sample_indices)\n",
    "        \n",
    "        # update input tensor\n",
    "        L_input, S_input, C_input, F_input = self.INPUTS\n",
    "        S_input_masked = torch.index_select(S_input, 0, torch.tensor([int(x) for x in sample_indices]).to(DEVICE))\n",
    "        self.MASKED_INPUTS = (L_input, S_input_masked, C_input, F_input)   \n",
    "        return sample_indices\n",
    "            \n",
    "Train_data = HGNN_DataSet( main_df, 0.8, 'income')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0123, 0.0050, 0.0043,  ..., 0.0051, 0.0051, 0.0051],\n",
       "        [0.0031, 0.0056, 0.0058,  ..., 0.0050, 0.0051, 0.0050],\n",
       "        [0.0038, 0.0051, 0.0045,  ..., 0.0051, 0.0051, 0.0051],\n",
       "        ...,\n",
       "        [0.0009, 0.0051, 0.0049,  ..., 0.0051, 0.0051, 0.0051],\n",
       "        [0.0069, 0.0053, 0.0049,  ..., 0.0051, 0.0051, 0.0051],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.INPUTS[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(10),\n",
       " tensor(7742),\n",
       " tensor(8797),\n",
       " tensor(20439),\n",
       " tensor(23942),\n",
       " tensor(25196),\n",
       " tensor(32721),\n",
       " tensor(34924),\n",
       " tensor(37391),\n",
       " tensor(37395)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.get_sample(10, inculde=[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Optional, Any, Union, Callable\n",
    "\n",
    "class CustomTransformerDecoderLayer(nn.TransformerDecoderLayer):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu'):\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        # remove defined modules\n",
    "        delattr(self, 'self_attn')\n",
    "        delattr(self, 'norm1')\n",
    "        delattr(self, 'dropout1')\n",
    "    \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        x = tgt\n",
    "        if self.norm_first:\n",
    "            # x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)\n",
    "            x = x + self._mha_block(self.norm2(x), memory, memory_mask)\n",
    "            x = x + self._ff_block(self.norm3(x))\n",
    "        else:\n",
    "            # x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))\n",
    "            x = self.norm2(x + self._mha_block(x, memory, memory_mask))\n",
    "            # x =  x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask)\n",
    "            # x = self.norm3(x + self._ff_block(x))\n",
    "\n",
    "        return x\n",
    "    def _mha_block(self, x: Tensor, mem: Tensor,\n",
    "                   attn_mask: Optional[Tensor],) -> Tensor:\n",
    "        x = xops.memory_efficient_attention(x, mem, mem, attn_mask)\n",
    "        return self.dropout2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dims (1, 14, 1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型輸出的大小: torch.Size([10, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baic transformer decoder model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "from tqdm import trange\n",
    "\n",
    "class TransformerDecoderModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 target_dataset, \n",
    "                 num_layers, \n",
    "                 embedding_dim,  \n",
    "                 subgraph_masked: Optional[bool] = False,\n",
    "                 K : Optional[int] = 10,\n",
    "                 ):\n",
    "        super(TransformerDecoderModel, self).__init__()\n",
    "\n",
    "        L_dim, S_dim, C_dim, F_dim = target_dataset.INPUT_DIMS\n",
    "        L, S, C, F = target_dataset.nodes_num['L'], target_dataset.nodes_num['S'], target_dataset.nodes_num['C'], target_dataset.nodes_num['F']\n",
    "        num_NUM , num_CAT = target_dataset.NUM_vs_CAT\n",
    "        \n",
    "        # check input dims\n",
    "        if num_CAT + num_NUM != S_dim:\n",
    "            raise ValueError('num_CAT + num_NUM != number of columns (S_dim)   {} + {} != {}'.format(num_CAT, num_NUM, S_dim))\n",
    "        \n",
    "        # 目前b卡在embedding的怎麼用\n",
    "        # Catagory_embedding => 數值類Qcut後用linear來做embedding, 類別用nn.Embedding\n",
    "        catagories = C - num_NUM * target_dataset.N_BINS\n",
    "\n",
    "        \n",
    "        # nn.Embedding( number of possible catagories, embedding_dim, )\n",
    "        # nn.Linear( number of input dimantion, embedding_dim, )\n",
    "\n",
    "        self.Lable_embedding = nn.Embedding(L, embedding_dim, dtype=torch.float)\n",
    "        \n",
    "        self.Sample_embedding_num = nn.Linear(num_NUM, embedding_dim, dtype=torch.float)\n",
    "        # use MLP projector to project sample feature from 8 dim to 128 dim\n",
    "        # self.Sample_embedding_cat = nn.Linear(num_CAT, embedding_dim, dtype=torch.float)\n",
    "        self.Sample_embedding_cat = nn.Sequential(\n",
    "                                        nn.Linear(num_CAT, 64),  \n",
    "                                        nn.ReLU(),        \n",
    "                                        nn.Linear(64, embedding_dim) \n",
    "                                    )\n",
    "                                    \n",
    "        \n",
    "        self.Catagory_embedding_num = nn.Linear(C_dim, embedding_dim, dtype=torch.float)\n",
    "        self.Catagory_embedding_cat = nn.Embedding(catagories, embedding_dim, dtype=torch.float)\n",
    "        \n",
    "        self.Field_embedding = nn.Embedding(F, embedding_dim, dtype=torch.float)\n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            CustomTransformerDecoderLayer(embedding_dim,  nhead = 1 ),\n",
    "            num_layers\n",
    "        )\n",
    "        \n",
    "        # downstream task\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 2),\n",
    "            # nn.Softmax(dim=2),\n",
    "            # nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.subgraph_masked = subgraph_masked\n",
    "        if subgraph_masked: \n",
    "            self.K = K\n",
    "        else:\n",
    "            # init mask\n",
    "            target_dataset.make_mask_all()\n",
    "        \n",
    "        self.tmpmask_L2S = target_dataset.MASKS['L2S'].clone().to(DEVICE)\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "                target_dataset: HGNN_DataSet, \n",
    "                ):\n",
    "        L, S, C, F = target_dataset.nodes_num['L'], target_dataset.nodes_num['S'], target_dataset.nodes_num['C'], target_dataset.nodes_num['F']\n",
    "        num_NUM, num_CAT = target_dataset.NUM_vs_CAT\n",
    "        N_bins = target_dataset.N_BINS\n",
    "        \n",
    "        if self.subgraph_masked: \n",
    "            target_dataset.get_sample(self.K)\n",
    "            L_input, S_input, C_input, F_input = target_dataset.MASKED_INPUTS\n",
    "            masks = target_dataset.MASKS\n",
    "            K = target_dataset.nodes_num['K']\n",
    "            S_ = K\n",
    "        else:\n",
    "            target_dataset.make_input_tensor()\n",
    "            L_input, S_input, C_input, F_input = target_dataset.INPUTS\n",
    "            target_dataset.make_mask_all()\n",
    "            masks = target_dataset.MASKS\n",
    "            S_ = S\n",
    "            \n",
    "        # for S and C, we use two different embedding methods, for CAT and NUM, respectively\n",
    "        # Squeeze for making batch dimantion\n",
    "        L_embedded = self.Lable_embedding(L_input.long()).squeeze(1).unsqueeze(0).float()\n",
    "        \n",
    "        S_embedded_num = self.Sample_embedding_num(S_input[:,:num_NUM]).unsqueeze(0).float()\n",
    "        S_embedded_cat = self.Sample_embedding_cat(S_input[:,num_NUM:].float()).unsqueeze(0).float()\n",
    "        S_embedded = S_embedded_num + S_embedded_cat\n",
    "\n",
    "        C_embedded_num = self.Catagory_embedding_num(C_input[:num_NUM * N_bins].float()).unsqueeze(0).float()\n",
    "        C_embedded_cat = self.Catagory_embedding_cat(C_input[num_NUM * N_bins:].squeeze(1).long() - num_NUM*N_bins).unsqueeze(0).float()\n",
    "        C_embedded = torch.cat([C_embedded_num, C_embedded_cat], dim = 1)\n",
    "        \n",
    "        F_embedded = self.Field_embedding(F_input.long()).squeeze(1).unsqueeze(0).float()\n",
    "        \n",
    "        # print(L_embedded.shape, S_embedded.shape, C_embedded.shape, F_embedded.shape)\n",
    "        \n",
    "\n",
    "        for mask in masks.keys():\n",
    "            masks[mask] = masks[mask].to(DEVICE)\n",
    "        \n",
    "        # propagate steps: L→S→C→F\n",
    "        #                  L←S←C←\n",
    "        # more steps more menory usage\n",
    "        PROPAGATE_STEPS = 2\n",
    "        for i in range(PROPAGATE_STEPS):\n",
    "            S_embedded = self.transformer_decoder(S_embedded,L_embedded, \n",
    "                                                memory_mask = self.tmpmask_L2S[:S_,:L]) \n",
    "            C_embedded = self.transformer_decoder(C_embedded,S_embedded,\n",
    "                                                memory_mask = masks['S2C'][:C,:S_])\n",
    "            F_embedded = self.transformer_decoder(F_embedded,C_embedded,\n",
    "                                                memory_mask = masks['C2F'][:F,:C])\n",
    "            C_embedded = self.transformer_decoder(C_embedded,F_embedded,\n",
    "                                                memory_mask = Tensor.contiguous(masks['C2F'].transpose(0, 1))[:C,:F])\n",
    "            S_embedded = self.transformer_decoder(S_embedded,C_embedded,\n",
    "                                                memory_mask = Tensor.contiguous(masks['S2C'].transpose(0, 1))[:S_,:C])\n",
    "            L_embedded = self.transformer_decoder(L_embedded,S_embedded, \n",
    "                                                memory_mask = Tensor.contiguous(self.tmpmask_L2S.transpose(0, 1))[:L,:S_])\n",
    "        \n",
    "        \n",
    "        output = self.MLP(S_embedded)[0]\n",
    "        return output\n",
    "\n",
    "# 測試模型\n",
    "num_layers = 1  # TransformerDecoder 的層數\n",
    "embedding_dim = 2*128  # 嵌入維度\n",
    "hidden_dim = 64  \n",
    "\n",
    "print('input_dims', Train_data.INPUT_DIMS)\n",
    "model = TransformerDecoderModel(Train_data, num_layers, embedding_dim, subgraph_masked = True, K = 10).to(DEVICE)\n",
    "outputs = model(Train_data)\n",
    "\n",
    "print(\"模型輸出的大小:\", outputs.shape)\n",
    "output_label = torch.argmax(outputs, dim=1)\n",
    "output_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39073 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1302/39073 [00:12<06:06, 103.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m | Loss: \u001b[39m\u001b[39m{\u001b[39;00mepoch_loss\u001b[39m}\u001b[39;00m\u001b[39m | AUC: \u001b[39m\u001b[39m{\u001b[39;00mepoch_AUC\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m model \u001b[39m=\u001b[39m TransformerDecoderModel(Train_data, num_layers, embedding_dim, subgraph_masked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 85\u001b[0m train(model, Train_data)\n",
      "Cell \u001b[0;32mIn[30], line 54\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, datset)\u001b[0m\n\u001b[1;32m     51\u001b[0m loss_log\u001b[39m.\u001b[39mappend(batch_loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     53\u001b[0m \u001b[39m# backpropagation\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m batch_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     55\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     57\u001b[0m \u001b[39m# TRUE = (torch.argmax(torch.tensor(LABEL_POOL_)))\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "from torch import autograd\n",
    "from torcheval.metrics.aggregation.auc import AUC\n",
    "from torcheval.metrics import BinaryAUROC\n",
    "\n",
    "\n",
    "def train(model, datset):\n",
    "    LABEL_POOL = datset.LABEL_POOL\n",
    "    \n",
    "    tmp_log = []\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        Original_L2S_mask = datset.MASKS['L2S'].clone()\n",
    "        # logs\n",
    "        loss_log = []\n",
    "        AUC_metric = BinaryAUROC().to(DEVICE)\n",
    "        \n",
    "        for index in trange(datset.nodes_num['S']-1): # query through all sample nodes (not infering node)\n",
    "            # for all query, input = sample K + 1 query smaple\n",
    "            sample_indices = datset.get_sample(10, inculde = [index])\n",
    "            \n",
    "            \n",
    "            # modify the mask to mask out the queries node's edge to it's label node\n",
    "            L = datset.nodes_num['L']\n",
    "            query_index = sample_indices.index(index) # query_index: index of query node in sample_indices\n",
    "            datset.tmpmask_L2S = Original_L2S_mask.clone()\n",
    "            datset.tmpmask_L2S[query_index] = 0\n",
    "            datset.tmpmask_L2S[query_index][L-1] = 1 # make it as unseen label\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # outputs = model(datset)[query_index]\n",
    "            outputs = model(datset)\n",
    "            # for trainning, only the query node's output is used\n",
    "\n",
    "            # caculate loss\n",
    "            if model.subgraph_masked:\n",
    "                # get the real label fo query node\n",
    "                LABEL_POOL_ = LABEL_POOL[index]\n",
    "            else:\n",
    "                LABEL_POOL_ = LABEL_POOL\n",
    "            \n",
    "            \n",
    "            # batch_loss = criterion(outputs, torch.tensor(LABEL_POOL_).to(DEVICE))\n",
    "            batch_loss = criterion(outputs, torch.tensor(LABEL_POOL[sample_indices]).to(DEVICE))\n",
    "            loss_log.append(batch_loss.item())\n",
    "            \n",
    "            # backpropagation\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # TRUE = (torch.argmax(torch.tensor(LABEL_POOL_)))\n",
    "            TRUE = (torch.argmax(torch.tensor(LABEL_POOL[sample_indices]), dim=1))\n",
    "            # print(TRUE)\n",
    "            \n",
    "            outputs = outputs.softmax(dim=0)\n",
    "            # y_pred_first = outputs[1].to(DEVICE).int()\n",
    "            y_pred_first = [z[1] for z in outputs]\n",
    "            \n",
    "            # tmp_log.append(y_pred_first)\n",
    "            # AUC_metric.update(torch.Tensor([y_pred_first]),torch.Tensor([TRUE]))\n",
    "            AUC_metric.update(torch.Tensor(y_pred_first),torch.Tensor(TRUE))\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            if epoch >= 200:\n",
    "                break\n",
    "            \n",
    "        if sum(tmp_log)==0:\n",
    "            print('all zero')\n",
    "        # AUC_metric.update(torch.Tensor(AUC_true_log),torch.Tensor(AUC_pred_log))\n",
    "        # tmp_log = []\n",
    "        epoch_loss = sum(loss_log)/(datset.nodes_num['S']-1)\n",
    "        epoch_AUC = float(AUC_metric.compute())\n",
    "        AUC_metric.reset()\n",
    "        # break\n",
    "        del loss_log, AUC_metric\n",
    "\n",
    "        print(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC: {epoch_AUC}\")\n",
    "model = TransformerDecoderModel(Train_data, num_layers, embedding_dim, subgraph_masked = True).to(DEVICE)\n",
    "train(model, Train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.Sample_embedding = nn.Linear(S_dim, embedding_dim, dtype=torch.half)\n",
    "# self.Catagory_embedding = nn.Linear(C_dim, embedding_dim, dtype=torch.half)\n",
    "# S_embedded = self.Sample_embedding(S_input.half()).unsqueeze(0)\n",
    "# C_embedded = self.Catagory_embedding(C_input.half()).unsqueeze(0)\n",
    "# xops.memory_efficient_attention(C_embedded, S_embedded, S_embedded,attn_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
