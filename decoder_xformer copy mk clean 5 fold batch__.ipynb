{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# baic transformer Decoder model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import xformers.ops as xops\n",
    "import math \n",
    "from typing import Optional, Union\n",
    "from torch import Tensor\n",
    "import random\n",
    "\n",
    "main_df = pd.read_csv('adult.csv')\n",
    "main_df.head()\n",
    "DEVICE = 'cuda'\n",
    "# DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set seed failed,details are  No module named 'tensorflow'\n",
      "Pytorch seed set successfully\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.random.set_random_seed(seed)\n",
    "        print(\"Tensorflow seed set successfully\")\n",
    "    except Exception as e:\n",
    "        print(\"Set seed failed,details are \", e)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            # torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "        print(\"Pytorch seed set successfully\")\n",
    "    except Exception as e:\n",
    "        print(\"Set seed failed,details are \", e)\n",
    "        pass\n",
    "    import numpy as np\n",
    "    np.random.seed(seed)\n",
    "    import random as python_random\n",
    "    python_random.seed(seed)\n",
    "    # cuda env\n",
    "    import os\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_DataFrame_distribution(X_trans):\n",
    "    columns_range = {}\n",
    "    print('%15s' % '', '%6s' % 'min','%6s' % 'max', '%6s' % 'nunique')\n",
    "    \n",
    "    for column in X_trans.columns:\n",
    "        print('%15s' % column, '%6s' % X_trans[column].min(),'%6s' % X_trans[column].max(), '%6s' % X_trans[column].nunique())\n",
    "        columns_range[column] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsou/.conda/envs/xformers/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:313: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>123</td>\n",
       "      <td>187</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>303</td>\n",
       "      <td>365</td>\n",
       "      <td>380</td>\n",
       "      <td>390</td>\n",
       "      <td>400</td>\n",
       "      <td>412</td>\n",
       "      <td>423</td>\n",
       "      <td>426</td>\n",
       "      <td>467</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>110</td>\n",
       "      <td>184</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>303</td>\n",
       "      <td>365</td>\n",
       "      <td>386</td>\n",
       "      <td>390</td>\n",
       "      <td>403</td>\n",
       "      <td>412</td>\n",
       "      <td>423</td>\n",
       "      <td>426</td>\n",
       "      <td>467</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>123</td>\n",
       "      <td>180</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>367</td>\n",
       "      <td>371</td>\n",
       "      <td>390</td>\n",
       "      <td>399</td>\n",
       "      <td>412</td>\n",
       "      <td>423</td>\n",
       "      <td>426</td>\n",
       "      <td>467</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>121</td>\n",
       "      <td>183</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>303</td>\n",
       "      <td>363</td>\n",
       "      <td>382</td>\n",
       "      <td>390</td>\n",
       "      <td>399</td>\n",
       "      <td>412</td>\n",
       "      <td>423</td>\n",
       "      <td>426</td>\n",
       "      <td>467</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>103</td>\n",
       "      <td>183</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>303</td>\n",
       "      <td>365</td>\n",
       "      <td>382</td>\n",
       "      <td>392</td>\n",
       "      <td>397</td>\n",
       "      <td>413</td>\n",
       "      <td>423</td>\n",
       "      <td>425</td>\n",
       "      <td>467</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  educational-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0   35     123              187           191           215             303   \n",
       "1   25     110              184           191           215             303   \n",
       "2   35     123              180           191           215             298   \n",
       "3   17     121              183           191           215             303   \n",
       "4   11     103              183           191           215             303   \n",
       "\n",
       "   workclass  education  marital-status  occupation  relationship  race  \\\n",
       "0        365        380             390         400           412   423   \n",
       "1        365        386             390         403           412   423   \n",
       "2        367        371             390         399           412   423   \n",
       "3        363        382             390         399           412   423   \n",
       "4        365        382             392         397           413   423   \n",
       "\n",
       "   gender  native-country  income  \n",
       "0     426             467     472  \n",
       "1     426             467     471  \n",
       "2     426             467     471  \n",
       "3     426             467     471  \n",
       "4     425             467     471  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "def POOL_preprocess(df, N_BINS = 100):\n",
    "    '''\n",
    "    Preprocess the DataFrame \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        N_BINS: number of bins for each numerical column (will not be the exact number of bins, differ by distribution)\n",
    "    Return:\n",
    "        X_trans: DataFrame after preprocessing\n",
    "        ct: ColumnTransformer object, for inference and inverse transform\n",
    "        NUM_vs_CAT: tuple, (number of numerical columns, number of categorical columns - 1) \"in feature field, do not include label column\"\n",
    "        existing_values: dict, {column name: sorted list of existing values}\n",
    "    '''\n",
    "    \n",
    "    CAT = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country', 'income']\n",
    "    NUM = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "    \n",
    "    num_CAT = len(CAT)\n",
    "    num_NUM = len(NUM)  \n",
    "    \n",
    "    ct = ColumnTransformer([\n",
    "        (\"age\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"age\"]),\n",
    "        (\"fnlwgt\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='quantile', subsample=None), [\"fnlwgt\"]),\n",
    "        (\"educational-num\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='quantile', subsample=None), [\"educational-num\"]),\n",
    "        (\"capital-gain\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"capital-gain\"]),\n",
    "        (\"capital-loss\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"capital-loss\"]),\n",
    "        (\"hours-per-week\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"hours-per-week\"]),\n",
    "         ],remainder = 'passthrough', verbose_feature_names_out = False) # make sure columns are unique\n",
    "    ct.set_output(transform = 'pandas')\n",
    "    X_trans = ct.fit_transform(df) \n",
    "\n",
    "    # print(X_trans[NUM])\n",
    "    C_pool = np.array([])\n",
    "\n",
    "    for column in NUM:\n",
    "        values = X_trans[column].to_numpy().reshape(-1,1)\n",
    "        values = ct.named_transformers_[column].inverse_transform(values)\n",
    "        values = np.unique(values).flatten()\n",
    "        C_pool = np.concatenate((C_pool, values, np.array([-1])))\n",
    "        # print(values)\n",
    "    catagory_count = 0\n",
    "    for column in CAT:\n",
    "        if column == 'income':\n",
    "            continue\n",
    "        catagory_count += len(X_trans[column].unique()) + 1\n",
    "    C_pool = np.concatenate((C_pool, np.arange(catagory_count)))\n",
    "\n",
    "\n",
    "    # store the numrical columns' existing values for identifying unseen values\n",
    "    existing_values = {}\n",
    "    for column in NUM:\n",
    "        existing_values[column] = sorted(X_trans[column].unique().astype(int))\n",
    "    for column in CAT:\n",
    "        existing_values[column] = sorted(X_trans[column].unique().astype(str))\n",
    "    \n",
    "    # apply Ordinal encoding on columns\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    OE_list = {}\n",
    "    for column in NUM + CAT:\n",
    "        OE = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value = -1)\n",
    "        X_trans[column] = OE.fit_transform(X_trans[[column]])\n",
    "        OE_list[column] = OE\n",
    "    \n",
    "    # make all columns' catagory unique\n",
    "    # 7/19: each NUM column has its own number of unique values, plus 1 for unseen values\n",
    "    # each column has it's own number of unique values. '+1' is for unseen values\n",
    "    offset = 0\n",
    "    for column in NUM + CAT:\n",
    "        X_trans[column] = X_trans[column].apply(lambda x: x + offset)\n",
    "        offset += (X_trans[column].max() - X_trans[column].min() + 1) + 1\n",
    "    \n",
    "    X_trans = X_trans.astype(int).reset_index(drop = True)\n",
    "    return X_trans, (ct, OE_list, NUM, CAT, existing_values), (num_NUM, num_CAT - 1), C_pool\n",
    "    # -1 is for the income column (label)\n",
    "main_df_SHUFFLE = main_df.sample(frac=1).reset_index(drop=True)\n",
    "X_trans, inference_package , _, C_pool = POOL_preprocess(main_df_SHUFFLE[48842//5:])\n",
    "X_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preprocess]: detected unseen values in column capital-loss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77</td>\n",
       "      <td>183</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>303</td>\n",
       "      <td>365</td>\n",
       "      <td>382</td>\n",
       "      <td>388</td>\n",
       "      <td>404</td>\n",
       "      <td>416</td>\n",
       "      <td>423</td>\n",
       "      <td>425</td>\n",
       "      <td>467</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>97</td>\n",
       "      <td>183</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>303</td>\n",
       "      <td>365</td>\n",
       "      <td>382</td>\n",
       "      <td>390</td>\n",
       "      <td>410</td>\n",
       "      <td>415</td>\n",
       "      <td>422</td>\n",
       "      <td>426</td>\n",
       "      <td>467</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>120</td>\n",
       "      <td>187</td>\n",
       "      <td>204</td>\n",
       "      <td>215</td>\n",
       "      <td>303</td>\n",
       "      <td>365</td>\n",
       "      <td>380</td>\n",
       "      <td>388</td>\n",
       "      <td>406</td>\n",
       "      <td>413</td>\n",
       "      <td>423</td>\n",
       "      <td>425</td>\n",
       "      <td>467</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>129</td>\n",
       "      <td>183</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>303</td>\n",
       "      <td>365</td>\n",
       "      <td>382</td>\n",
       "      <td>390</td>\n",
       "      <td>410</td>\n",
       "      <td>412</td>\n",
       "      <td>423</td>\n",
       "      <td>426</td>\n",
       "      <td>467</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>158</td>\n",
       "      <td>187</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>293</td>\n",
       "      <td>365</td>\n",
       "      <td>380</td>\n",
       "      <td>390</td>\n",
       "      <td>408</td>\n",
       "      <td>417</td>\n",
       "      <td>423</td>\n",
       "      <td>425</td>\n",
       "      <td>467</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  educational-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0   39      77              183           191           215             303   \n",
       "1    8      97              183           191           215             303   \n",
       "2   26     120              187           204           215             303   \n",
       "3   15     129              183           191           215             303   \n",
       "4   22     158              187           191           215             293   \n",
       "\n",
       "   workclass  education  marital-status  occupation  relationship  race  \\\n",
       "0        365        382             388         404           416   423   \n",
       "1        365        382             390         410           415   422   \n",
       "2        365        380             388         406           413   423   \n",
       "3        365        382             390         410           412   423   \n",
       "4        365        380             390         408           417   423   \n",
       "\n",
       "   gender  native-country  income  \n",
       "0     425             467     471  \n",
       "1     426             467     471  \n",
       "2     425             467     472  \n",
       "3     426             467     471  \n",
       "4     425             467     471  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def POOL_preprocess_inference(df: pd.DataFrame,\n",
    "                              inference_package: tuple,\n",
    "                                # ct: ColumnTransformer,\n",
    "                                # OE_list: dict,\n",
    "                                # NUM: list,\n",
    "                                # CAT: list,\n",
    "                                # existing_values: dict,\n",
    "                              ):\n",
    "    '''Preprocess the DataFrame when inference\n",
    "    \n",
    "    Args:\n",
    "        `df`: DataFrame to be processed.\\n\n",
    "        `inference_package`: tuple, containing the following objects.\n",
    "            `ct`: ColumnTransformer object required for inference, which makes sure values are in the same range as training data\n",
    "            `OE_list`: dict, {column name: OrdinalEncoder object}\\n\n",
    "            `NUM`: list of numerical columns \\n\n",
    "            `CAT`: list of categorical columns\\n\n",
    "            `existing_values`: dict, {column name: sorted list of existing values}\n",
    "    '''\n",
    "    (ct, OE_list, NUM, CAT, existing_values) = inference_package\n",
    "    X_trans_ori = ct.transform(df)\n",
    "    \n",
    "    # caculate the loaction of unseen values\n",
    "    unseen_node_indexs = {}\n",
    "    offset = 0\n",
    "    for col in NUM + CAT:\n",
    "        unseen_node_indexs[col] = (int(len(existing_values[col])) + offset )\n",
    "        offset += int(len(existing_values[col])) + 1\n",
    "    \n",
    "    X_trans = X_trans_ori\n",
    "    \n",
    "    # apply Ordinal encoding on columns, and make all columns' catagory unique\n",
    "    offset = 0\n",
    "    for column in NUM + CAT:\n",
    "        OE = OE_list[column]\n",
    "        X_trans[column] = OE.transform(X_trans[[column]]) # use fitted OE to transform, the unseen values will be encoded as -1\n",
    "        if -1 in X_trans[column].tolist():\n",
    "            print('[preprocess]: detected unseen values in column', column)\n",
    "        X_trans[column] = X_trans[column].apply(lambda x: x + offset if x != -1 else unseen_node_indexs[column])\n",
    "        offset = unseen_node_indexs[column] + 1  \n",
    "\n",
    "    \n",
    "    X_trans = X_trans.astype(int).reset_index(drop = True) \n",
    "    return X_trans, unseen_node_indexs \n",
    "X_trans_ , unseen_node_indexs= POOL_preprocess_inference(main_df_SHUFFLE[:48842//5], inference_package)\n",
    "X_trans_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   min    max nunique\n",
      "            age      0     73     71\n",
      "         fnlwgt     75    174    100\n",
      "educational-num    176    189     14\n",
      "   capital-gain    191    213     22\n",
      "   capital-loss    215    263     43\n",
      " hours-per-week    264    359     85\n",
      "      workclass    361    369      9\n",
      "      education    371    386     16\n",
      " marital-status    388    394      7\n",
      "     occupation    396    410     15\n",
      "   relationship    412    417      6\n",
      "           race    419    423      5\n",
      "         gender    425    426      2\n",
      " native-country    428    469     41\n",
      "         income    471    472      2\n"
     ]
    }
   ],
   "source": [
    "check_DataFrame_distribution(X_trans_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   min    max nunique\n",
      "            age      0     73     74\n",
      "         fnlwgt     75    174    100\n",
      "educational-num    176    189     14\n",
      "   capital-gain    191    213     23\n",
      "   capital-loss    215    262     48\n",
      " hours-per-week    264    359     96\n",
      "      workclass    361    369      9\n",
      "      education    371    386     16\n",
      " marital-status    388    394      7\n",
      "     occupation    396    410     15\n",
      "   relationship    412    417      6\n",
      "           race    419    423      5\n",
      "         gender    425    426      2\n",
      " native-country    428    469     42\n",
      "         income    471    472      2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[74, 175, 190, 214, 264, 360, 370, 387, 395, 411, 418, 424, 427, 470, 473]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_DataFrame_distribution(X_trans)\n",
    "'[74, 175, 190, 214, 264, 360, 370, 387, 395, 411, 418, 424, 427, 470, 473]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Notations\\n  node: number of all nodes = L + S + C + F\\n  L: number of lable nodes + 1 (for unseen lable)\\n  S: number of sample nodes + 1 (for inference)\\n  C: number of catagory nodes + F (for each field(column)\\n  F: number of field(column) nodes (no unseen field is allowed)\\n  hidden: number of hidden representation\\n\\ndata size = \\nmask size =\\nuse nn.transformerDecoder(data,mask) to get the output\\nuse the above output as input of MLP to predict the lable   \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Notations\n",
    "  node: number of all nodes = L + S + C + F\n",
    "  L: number of lable nodes + 1 (for unseen lable)\n",
    "  S: number of sample nodes + 1 (for inference)\n",
    "  C: number of catagory nodes + F (for each field(column)\n",
    "  F: number of field(column) nodes (no unseen field is allowed)\n",
    "  hidden: number of hidden representation\n",
    "\n",
    "data size = \n",
    "mask size =\n",
    "use nn.transformerDecoder(data,mask) to get the output\n",
    "use the above output as input of MLP to predict the lable   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data num: 48842\n",
      "trian data num: 39073\n",
      "test data num: 9769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preprocess]: detected unseen values in column capital-loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsou/.conda/envs/xformers/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:313: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_nums {'L': 3, 'S': 39074, 'C': 471, 'F': 14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_271765/1297991968.py:271: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  C_input = torch.tensor([self.C_POOL], device=DEVICE).reshape(-1,1)\n"
     ]
    }
   ],
   "source": [
    "class HGNN_dataset():\n",
    "    def __init__(self,\n",
    "                 data_df : pd.DataFrame,\n",
    "                 label_column : str,\n",
    "                 test_df : pd.DataFrame = None,\n",
    "                 split_ratio : float = None,\n",
    "                 embedding_dim : int = 128,\n",
    "                 N_BINS : int = 100\n",
    "                 ):\n",
    "        if test_df is None:\n",
    "            # shuffle and cut data\n",
    "            data_df = data_df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "            test_size = math.ceil(data_df.shape[0] * (1-split_ratio))\n",
    "            train_pool = data_df[test_size:]\n",
    "            test_pool = data_df[:test_size]\n",
    "            print('total data num:' , data_df.shape[0])\n",
    "            print('trian data num:' , train_pool.shape[0])\n",
    "            print('test data num:' , test_pool.shape[0])\n",
    "        else: \n",
    "            # given train and test data, seperated (K-fold)\n",
    "            train_pool = data_df\n",
    "            test_pool = test_df\n",
    "            print('trian data num:' , train_pool.shape[0])\n",
    "            print('test data num:' , test_pool.shape[0])\n",
    "\n",
    "        \n",
    "        # to-dos:\n",
    "        # train\n",
    "        #   \n",
    "        TRAIN_POOL, self.inference_package, self.NUM_vs_CAT, C_POOL = POOL_preprocess(train_pool, N_BINS = N_BINS)\n",
    "        TEST_POOL, self.unseen_node_indexs_C = POOL_preprocess_inference(test_pool, self.inference_package)\n",
    "        LABEL_COLUMN = label_column\n",
    "\n",
    "        # cut feature and lable\n",
    "        FEATURE_POOL = TRAIN_POOL.drop(LABEL_COLUMN, axis=1)\n",
    "        LABEL_POOL = TRAIN_POOL[LABEL_COLUMN]\n",
    "        TEST_LABEL_POOL = TEST_POOL[LABEL_COLUMN]\n",
    "        \n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        enc = OneHotEncoder()\n",
    "        LABEL_POOL = enc.fit_transform(LABEL_POOL.values.reshape(-1,1)).toarray()\n",
    "        TEST_LABEL_POOL = enc.fit_transform(TEST_LABEL_POOL.values.reshape(-1,1)).toarray()\n",
    "\n",
    "        # L: number of lable nodes, the last node of Lable nodes is served as unknown lable node\n",
    "        L = LABEL_POOL.shape[1] + 1\n",
    "\n",
    "        # S: number of sample nodes, the last node of sample nodes is served as infering node\n",
    "        S = FEATURE_POOL.shape[0] + 1\n",
    "        \n",
    "        # F: number of field (column) nodes\n",
    "        F = FEATURE_POOL.shape[1]\n",
    "\n",
    "        # C: number of catagory nodes, each field(column) has its own \"unseen\" catagory nodes\n",
    "        self.nodes_of_fields = []\n",
    "        for column in FEATURE_POOL.columns:\n",
    "            self.nodes_of_fields.append(FEATURE_POOL[column].nunique()+1)\n",
    "        C = sum(self.nodes_of_fields) # the total number of nodes equals to the sum of nodes of each field\n",
    "        # C_POOL = range(int(C))\n",
    "\n",
    "        nodes_num = {'L':L, 'S':S, 'C':C, 'F':F}\n",
    "        print('node_nums', nodes_num)\n",
    "        # print('total', L+S+C+F, 'nodes')\n",
    "        \n",
    "        # get samples indexs for each label\n",
    "        self.labe_to_index = {}\n",
    "        tmp_pool = TRAIN_POOL.copy().reset_index(drop=True)\n",
    "        for label in tmp_pool['income'].unique():\n",
    "            self.labe_to_index[label] = (tmp_pool[tmp_pool['income'] == label].index).tolist()\n",
    "        \n",
    "        self.TRAIN_POOL = TRAIN_POOL\n",
    "        self.TEST_POOL = TEST_POOL\n",
    "        self.TEST_LABEL_POOL = TEST_LABEL_POOL\n",
    "        self.LABEL_COLUMN = LABEL_COLUMN\n",
    "        self.FEATURE_POOL = FEATURE_POOL\n",
    "        self.LABEL_POOL = LABEL_POOL\n",
    "        self.C_POOL = C_POOL   \n",
    "        self.nodes_num = nodes_num\n",
    "        self.N_BINS = N_BINS\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.make_input_tensor()\n",
    "        # self.get_sample(10)        \n",
    "        self.make_mask_all()\n",
    "        \n",
    "        # self.make_mask()\n",
    "        \n",
    "        \n",
    "    def make_mask_subgraph(self,\n",
    "                  sample_indices: Optional[list] = None,\n",
    "                  query_indices: Optional[list] = None,\n",
    "                ):\n",
    "        '''Makeing masks for subgraph. Mask values are 1 if two nodes are connected, otherwise 0.\n",
    "        \n",
    "        Args:\n",
    "            sample_indices: list of list of sample node indices, in shape of `[batch_size, sample_size]`\n",
    "            query_indices: list of query node indices for each batch, in shape of `[batch_size]`\n",
    "        \n",
    "        for example, with:\n",
    "            {'L': 3, 'S': 39074, 'C': 470, 'F': 14, 'K': 10}\n",
    "            \n",
    "        the masks will be:\n",
    "            masks['L2S'] = torch.Size([16, 8]), values in torch.Size([10, 3])\\\\\n",
    "            masks['S2C'] = torch.Size([472, 16]), values in torch.Size([470, 10])\\\\\n",
    "            masls['C2F'] = torch.Size([16, 472]), values in torch.Size([14, 470])\\\\\n",
    "        Notice: xformer require the mask's tensor must align on memory, and should be slice of a tensor if shape cannot be divided by 8\n",
    "        '''\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "\n",
    "        sample_size = len(sample_indices[0])\n",
    "        # caculate masking\n",
    "        masks = {}\n",
    "        \n",
    "        tmp_L2S = []\n",
    "        tmp_S2C = []\n",
    "        for batch_indices in sample_indices:\n",
    "            # masked_POOL = self.TRAIN_POOL.iloc[batch_indices] # sample dataframe into shape (10,14)\n",
    "            \n",
    "            # label to sample\n",
    "            tmp = self.MASKS_FULL['L2S']\n",
    "            tmp = torch.index_select(tmp, 0, torch.tensor(batch_indices, device=DEVICE)) #The returned tensor does not use the same storage as the original tensor\n",
    "            # tmp = torch.zeros([math.ceil(sample_size/8) * 8, math.ceil(L/8) * 8], dtype=torch.float, device=DEVICE) \n",
    "            # label_value = masked_POOL[self.LABEL_COLUMN].values\n",
    "            # tmp[torch.arange(sample_size, device=DEVICE), torch.tensor(label_value - min(label_value), device=DEVICE)] = 1\n",
    "            shape = (math.ceil(sample_size/8) * 8, math.ceil(L/8) * 8)\n",
    "            new_tensor = torch.zeros(*shape, device=DEVICE)\n",
    "            new_tensor[:tmp.shape[0], :tmp.shape[1]] = tmp\n",
    "            tmp = new_tensor.view(*shape)\n",
    "            tmp_L2S.append(tmp)\n",
    "            # masks['L2S'] = tmp.repeat(batch_size,1,1)\n",
    "            \n",
    "\n",
    "            # sample to catagory\n",
    "            tmp = self.MASKS_FULL['S2C']\n",
    "            tmp = torch.index_select(tmp, 1, torch.tensor(batch_indices, device=DEVICE)) # The returned tensor does not use the same storage as the original tensor\n",
    "            # tmp = torch.zeros([math.ceil(C/8) * 8, math.ceil(sample_size/8) * 8], dtype=torch.float, device=DEVICE).T\n",
    "            # tmp_df = masked_POOL.drop(self.LABEL_COLUMN, axis=1)\n",
    "            # tmp[torch.arange(sample_size, device=DEVICE).unsqueeze(-1), torch.tensor(tmp_df.values, device=DEVICE)] = 1\n",
    "            # tmp = tmp.T.contiguous()\n",
    "            shape = (math.ceil(C/8) * 8, math.ceil(sample_size/8) * 8)\n",
    "            new_tensor = torch.zeros(*shape,device=DEVICE)\n",
    "            new_tensor[:tmp.shape[0], :tmp.shape[1]] = tmp\n",
    "            tmp = new_tensor.view(*shape)\n",
    "            tmp_S2C.append(tmp)\n",
    "        \n",
    "        masks['L2S'] = torch.stack(tmp_L2S, dim = 0)\n",
    "        masks['S2C'] = torch.stack(tmp_S2C, dim = 0)\n",
    "        \n",
    "        # masks['S2C'] = Tensor.contiguous(tmp.repeat(batch_size,1,1))\n",
    "        # masks['S2C'] = torch.stack(tmp_, dim = 0)\n",
    "\n",
    "        # catagory to field\n",
    "        masks['C2F'] = self.MASKS_FULL['C2F'].repeat(len(query_indices),1,1)\n",
    "        self.MASKS = masks\n",
    "        self.nodes_num['K'] = sample_size\n",
    "        \n",
    "    def make_mask_all(self):\n",
    "        '''Makeing masks for the entire graph. Mask values are 1 if two nodes are connected, otherwise 0.\n",
    "\n",
    "        for example, with:\n",
    "            {'L': 3, 'S': 39074, 'C': 470, 'F': 14, 'K': 10}.\n",
    "            \n",
    "        the masks will be:\n",
    "            masks['L2S']: torch.Size([39080, 8]), values in torch.Size([39074, 3]).\\\\\n",
    "            masks['S2C']: torch.Size([472, 39080]), values in torch.Size([470, 39074]).\\\\\n",
    "            masls['C2F']: torch.Size([16, 472]), values in torch.Size([14, 470]).\\\\\n",
    "            \n",
    "        Notice: xformer require the mask's tensor must align on memory, and should be slice of a tensor if shape cannot be divided by 8\n",
    "        '''\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        # caculate masking\n",
    "        masks = {}\n",
    "        \n",
    "        # label to sample \n",
    "        tmp = torch.zeros([math.ceil(S/8) * 8, math.ceil(L/8) * 8], dtype=torch.float, device=DEVICE)\n",
    "        label_ids = self.TRAIN_POOL[self.LABEL_COLUMN].unique()\n",
    "        for i, value_df in enumerate(self.TRAIN_POOL[self.LABEL_COLUMN]):\n",
    "            for j, value_label in enumerate(label_ids):\n",
    "                if value_label == value_df:\n",
    "                    tmp[i][j] = 1\n",
    "                    break\n",
    "        masks['L2S'] = tmp\n",
    "\n",
    "        # sample to catagory\n",
    "        tmp = torch.zeros([math.ceil(C/8) * 8, math.ceil(S/8) * 8], dtype=torch.float, device=DEVICE).T\n",
    "        tmp_df = self.FEATURE_POOL\n",
    "        tmp[torch.arange(len(self.FEATURE_POOL), device=DEVICE).unsqueeze(-1), torch.tensor(tmp_df.values, device=DEVICE)] = 1\n",
    "        tmp = tmp.T.contiguous()\n",
    "        masks['S2C'] = tmp\n",
    "\n",
    "        # catagory to field\n",
    "        # to do : this is wrong , should connect all catagory nodes (even unseen nodes))\n",
    "        tmp = torch.zeros([math.ceil(F/8) * 8, math.ceil(C/8) * 8], dtype=torch.float, device=DEVICE)\n",
    "        unique_items = [sorted(self.FEATURE_POOL[column].unique()) for column in (self.FEATURE_POOL.columns)]\n",
    "        for i in range(F):\n",
    "            for j in (unique_items[i]):\n",
    "                tmp[i][j] = 1\n",
    "        masks['C2F'] = tmp\n",
    "        self.MASKS = masks\n",
    "        self.MASKS_FULL = masks\n",
    "        \n",
    "    def make_mask_test(self, \n",
    "                       indexs_in_test_pool : list\n",
    "                       ):\n",
    "        '''Make mask tensor for the testing scenario. \\n\n",
    "        In testing scenario, L, S, C, F remain the same, while all INPUTs are the same (sience they are initialized fixed vlaues\\n\n",
    "        All we need to do is to update masks(L2S, S2C) for the new inference node\n",
    "        '''\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        \n",
    "        masks = {}\n",
    "        tmp_L2S = []\n",
    "        tmp_S2C = []\n",
    "        # for i query nodes\n",
    "        for index_in_test_pool in indexs_in_test_pool:\n",
    "            # L2S shape: torch.Size([39080, 8]), values in torch.Size([39074, 3]).\n",
    "            # number of sample nodes : 39073 + 1 (inference node)\n",
    "            # S = 39074, -1 to convert to index of last node\n",
    "            tmp = self.MASKS_FULL['L2S'].clone().detach()\n",
    "            tmp[S-1, L-1] = 1 # connect inference node to unseen lable nodes\n",
    "            tmp_L2S.append(tmp)\n",
    "            # masks['L2S'] = tmp.unsqueeze(0)\n",
    "        \n",
    "            # S2C shape: torch.Size([472, 39080]), values in torch.Size([470, 39074]).\n",
    "            # self.MASKS_FULL['S2C'].T :[39080, 472], values in [39074, 470]\n",
    "            # self.TEST_POOL.drop(self.LABEL_COLUMN, axis=1).values[index_in_test_pool]\n",
    "            tmp = self.MASKS_FULL['S2C'].T.clone().detach()\n",
    "            # connect the last sample node (inference node) with it's catagory nodes\n",
    "            tmp[S-1, self.TEST_POOL.drop(self.LABEL_COLUMN, axis=1).values[index_in_test_pool]] = 1  \n",
    "            tmp_S2C.append(tmp.T)\n",
    "            \n",
    "            # masks['S2C'] = tmp.T.contiguous().unsqueeze(0)\n",
    "            \n",
    "        masks['L2S'] = torch.stack(tmp_L2S, dim = 0)\n",
    "        masks['S2C'] = torch.stack(tmp_S2C, dim = 0)\n",
    "        # C2F remains the same\n",
    "        masks['C2F'] = self.MASKS_FULL['C2F'].repeat(len(indexs_in_test_pool),1,1)\n",
    "        \n",
    "        self.MASKS = masks\n",
    "        # print('masks[\\'L2S\\']',masks['L2S'].shape)\n",
    "        # print('masks[\\'S2C\\']',masks['S2C'].shape)\n",
    "        # print('masks[\\'C2F\\']',masks['C2F'].shape)\n",
    "        \n",
    "        \n",
    "    def make_input_tensor(self):\n",
    "        '''Makeing input tensor for the entire graph.\n",
    "            \n",
    "        for example, with:\n",
    "            {'L': 3, 'S': 39074, 'C': 470, 'F': 14, 'K': 10}.\n",
    "                \n",
    "        the input tensor will be:\n",
    "            L_input: torch.Size([3, 1]).\n",
    "            S_input: torch.Size([39074, 128]).\n",
    "            C_input: torch.Size([470, 1]).\n",
    "            F_input: torch.Size([14, 1]).\n",
    "        '''\n",
    "        # make input tensor\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        # L\n",
    "        L_input = torch.tensor([range(L)], device=DEVICE).reshape(-1,1)\n",
    "        # print('L_input', L_input.type(), L_input.shape)\n",
    "        \n",
    "        # S (normalized by standard scaler)\n",
    "        # features = torch.tensor(self.FEATURE_POOL.values, device=DEVICE).float()\n",
    "        # normalized_features = (features - torch.mean(features, dim = 0)) / torch.std(features, dim = 0)\n",
    "        # S_input = torch.cat([normalized_features, torch.tensor([[0]*F], device=DEVICE)],dim = 0).float() # add infering node\n",
    "        \n",
    "        # S (initialize by random)\n",
    "        S_input = torch.rand(self.embedding_dim, device=DEVICE).repeat(S,1)\n",
    "        # print('S_input', S_input.type(), S_input.shape)\n",
    "        # C \n",
    "        C_input = torch.tensor([self.C_POOL], device=DEVICE).reshape(-1,1)\n",
    "        # print('C_input', C_input.type(), C_input.shape)\n",
    "        # F \n",
    "        F_input = torch.tensor([range(F)], device=DEVICE).reshape(-1,1)\n",
    "        # print('F_input', F_input.type(), F_input.shape)\n",
    "        # \n",
    "        self.INPUTS = (L_input, S_input, C_input, F_input)\n",
    "        self.INPUT_DIMS = (L_input.size(1), S_input.size(1), C_input.size(1), F_input.size(1))\n",
    "\n",
    "    def sample_with_distrubution(self, sample_size):\n",
    "        '''\n",
    "        Sample equally from each label with required sample size\\\\\n",
    "        forced to make balenced sample\n",
    "        '''\n",
    "        # decide each label's number of samples (fourced to be balenced if possible) \n",
    "        label_list = []\n",
    "        label_unique = list(self.labe_to_index.keys())\n",
    "        count = sample_size // len(label_unique)\n",
    "        remainder = sample_size % len(label_unique)\n",
    "        label_list = [item for item in label_unique for _ in range(count)]\n",
    "        label_list.extend(random.sample(label_unique, remainder))\n",
    "        # sample from indexes\n",
    "        indices = [random.choice(self.labe_to_index[label]) for label in label_list]\n",
    "        return indices     \n",
    "        \n",
    "    def get_sample(self, sample_size, query_indices = []):\n",
    "        '''get sample nodes indices, and update mask and input tensor\n",
    "        \n",
    "        Args:\n",
    "            sample_size: number of sample nodes required for each batch.\n",
    "            query_indices (optional): list of nodes indices that must be included in nodes indices (one for each batch).\n",
    "        Return:\n",
    "            sample_indices: list of sample nodes indices, in shape of `[batch_size, sample_size]`\n",
    "        \n",
    "        For example, with `sample_size = 3`, `query_indices = [1,2,3]`\\n\n",
    "        means that there are `batch_size = 3` batches, each batch has `3` nodes.\\n\n",
    "        particularly, the three batches' `sample_indices` could be:\\n\n",
    "        `[1,324,656]`, `[2, 435, 9867]`, `[3, 789, 1343]`\n",
    "        \n",
    "        The included nodes shold not and will not be repeated, in case of the lable leakage.\n",
    "        '''\n",
    "        # include specific nodes (e.g. query nodes), while remaining sample_size\n",
    "        sample_indices = []\n",
    "        if query_indices is not []:\n",
    "            for query in query_indices:\n",
    "                indices = self.sample_with_distrubution(sample_size - 1)\n",
    "                while query in indices:\n",
    "                    indices = self.sample_with_distrubution(sample_size - 1)\n",
    "                # add query nodes into sample_indices\n",
    "                indices.append(query)\n",
    "                sample_indices.append(sorted(indices))\n",
    "        else:\n",
    "            indices = self.sample_with_distrubution(sample_size - len(query_indices))\n",
    "            sample_indices.append(sorted(indices))\n",
    "        # update mask\n",
    "        # modify input tensor\n",
    "        L_input, S_input, C_input, F_input = self.INPUTS\n",
    "        S_input_masked = []\n",
    "        for i in range(len(query_indices)):\n",
    "            S_input_masked.append(torch.index_select(S_input, 0, torch.tensor(sample_indices[i], device=DEVICE)))\n",
    "        S_input_masked = torch.stack(S_input_masked, dim = 0) # convert back to tensor\n",
    "        self.MASKED_INPUTS = (L_input, S_input_masked, C_input, F_input) \n",
    "          \n",
    "        return sample_indices\n",
    "\n",
    "    def random_connect_unseen(self, \n",
    "                              mask, \n",
    "                              mask_ratio = 0.1,\n",
    "                              strategy: Optional[str] = 'unseen',\n",
    "                              ):\n",
    "        '''\n",
    "        Randomly connect nodes to unseen nodes.\\\\\n",
    "        Args:\n",
    "            mask: mask tensor to be updated, shape of `[B, C, S]`\n",
    "            mask_ratio: the ratio of nodes to be connected to unseen nodes\n",
    "            \n",
    "        By changing the mask into `SpareTensor` and updating the indices, we can randomly connect nodes to unseen nodes\n",
    "        '''\n",
    "        if mask_ratio == 0:\n",
    "            return mask\n",
    "        unseen_node_indexs_list = list(self.unseen_node_indexs_C.values())\n",
    "        # print('unseen_node_indexs_list', unseen_node_indexs_list)\n",
    "        mask_sparse = torch.clone(mask).to_sparse()\n",
    "        indices = torch.clone(mask_sparse.indices()[1])\n",
    "        num_batches = mask_sparse.indices()[0].max()+1\n",
    "        edges_per_batch = indices.shape[0] // num_batches\n",
    "        \n",
    "        \n",
    "        replacement_table = None\n",
    "        if strategy == 'unseen':\n",
    "            # create replacement table, which is like...    \n",
    "            # [74, 74, 74, ........74,\n",
    "            #  128,128,128,........128,\n",
    "            #  .......................] as a tensor\n",
    "            replacement_table = [[unseen_index]*(mask_sparse.indices()[2].max()+1) for unseen_index in unseen_node_indexs_list]\n",
    "            replacement_table = torch.tensor(replacement_table, device=DEVICE).flatten()\n",
    "        elif strategy == 'random':\n",
    "            replacement_table = []\n",
    "            lower_bound = 0\n",
    "            for unseen_node_index in unseen_node_indexs_list:\n",
    "                for _ in range(mask_sparse.indices()[2].max()+1):\n",
    "                    replacement_table.append(random.randint(lower_bound, unseen_node_index+1))\n",
    "                lower_bound = unseen_node_index+1\n",
    "            replacement_table = torch.tensor(replacement_table, device=DEVICE).flatten()\n",
    "        else:\n",
    "            raise ValueError('strategy should be either unseen or random')\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            this_batch_indices = indices[batch*edges_per_batch:(batch+1)*edges_per_batch]\n",
    "            num_samples = (torch.rand(len(this_batch_indices), device=DEVICE) < mask_ratio).int() * torch.add(torch.arange(len(this_batch_indices), device=DEVICE),1)\n",
    "            to_be_replace = torch.add(num_samples[num_samples.nonzero()].squeeze(-1), -1 )\n",
    "            # to_be_replace is a mask of indices to be replaced\n",
    "\n",
    "            indices[to_be_replace + batch*edges_per_batch] = replacement_table[to_be_replace]\n",
    "\n",
    "        mask_sparse.indices()[1]  = indices\n",
    "        # print(indices)\n",
    "        return mask_sparse.to_dense()\n",
    "\n",
    "Train_data = HGNN_dataset( main_df, 'income', split_ratio = 0.8)\n",
    "query_indices=[100,111,222]\n",
    "# Train_data.get_sample(10, query_indices=query_indices)\n",
    "Train_data.make_mask_subgraph(Train_data.get_sample(10, query_indices=query_indices), query_indices = query_indices)\n",
    "# Train_data.make_mask_subgraph(Train_data.get_sample(10, query_indices=query_indices), query_indices = query_indices)\n",
    "# Train_data.make_mask_test([0,1,3])\n",
    "\n",
    "# # print(Train_data.MASKS['L2S'].shape)\n",
    "# # print(Train_data.MASKS['S2C'].shape)\n",
    "# # print(Train_data.MASKS['C2F'].shape)\n",
    "# # print(Train_data.MASKED_INPUTS[1].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = copy.deepcopy(torch.clone(Train_data.MASKS['S2C'].detach()))\n",
    "result =Train_data.random_connect_unseen(Train_data.MASKS['S2C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.MASKS['S2C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [False,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True, False,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True]]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(tmp, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Optional, Any, Union, Callable\n",
    "class TabTransformerDecoder(nn.TransformerDecoder):\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_ori:Tensor, tgt_mask: Tensor | None = None, memory_mask: Tensor | None = None, tgt_key_padding_mask: Tensor | None = None, memory_key_padding_mask: Tensor | None = None) -> Tensor:\n",
    "        '''Pass the inputs (and mask) through the decoder layer in turn.\n",
    "\n",
    "        Args:\n",
    "            tgt: the sequence to the decoder (required).\n",
    "            memory: the sequence from the last layer of the encoder (required).\n",
    "            tgt_mask: the mask for the tgt sequence (optional).\n",
    "            memory_mask: the mask for the memory sequence (optional).\n",
    "            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        '''\n",
    "        output = tgt\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, memory, tgt_ori, tgt_mask=tgt_mask,\n",
    "                         memory_mask=memory_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                         memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "class TabHyperformer_Layer(nn.TransformerDecoderLayer):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=512, dropout=0, activation='relu'):\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        # remove defined modules\n",
    "        delattr(self, 'self_attn')\n",
    "        delattr(self, 'norm1')\n",
    "        delattr(self, 'dropout1')\n",
    "    \n",
    "    def forward(self, tgt, memory, tgt_ori, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        x = tgt\n",
    "        x_ori = tgt_ori\n",
    "        # x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))\n",
    "        x = self.norm2(x + x_ori + self._mha_block(x, memory, memory_mask))\n",
    "        # x =  x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask)\n",
    "        # x = self.norm3(x + self._ff_block(x))\n",
    "\n",
    "        return x\n",
    "    def _mha_block(self, x: Tensor, mem: Tensor,\n",
    "                   attn_mask: Optional[Tensor],) -> Tensor:\n",
    "        x = xops.memory_efficient_attention(x, mem, mem, attn_mask)\n",
    "        # return self.dropout2(x)\n",
    "        return (x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C2S', 'C2F', 'S2C', 'S2L', 'L2S', 'F2C', 'C2S']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "possible_steps = ['S2C', 'C2F', 'F2C', 'C2S', 'S2L', 'L2S']\n",
    "end_steps = ['C2S', 'L2S']\n",
    "selected_steps = np.random.permutation(possible_steps)\n",
    "selected_steps = list(selected_steps)\n",
    "selected_steps.append(random.choice(end_steps))\n",
    "selected_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baic transformer decoder model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "class TransformerDecoderModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dataset : HGNN_dataset, \n",
    "                 num_layers = 1, \n",
    "                 embedding_dim = 128, \n",
    "                 propagation_steps = 1,\n",
    "                 ):\n",
    "        super(TransformerDecoderModel, self).__init__()\n",
    "\n",
    "        L_dim, S_dim, C_dim, F_dim = dataset.INPUT_DIMS\n",
    "        L, S, C, F = dataset.nodes_num['L'], dataset.nodes_num['S'], dataset.nodes_num['C'], dataset.nodes_num['F']\n",
    "        num_NUM , num_CAT = dataset.NUM_vs_CAT\n",
    "        \n",
    "\n",
    "        self.Lable_embedding = nn.Embedding(L, embedding_dim, dtype=torch.float)\n",
    "    \n",
    "        # self.Catagory_embedding_num = nn.Linear(C_dim, embedding_dim, dtype=torch.float)\n",
    "        # for every numrical filed, construct it's own Linear embedding layer\n",
    "        self.Catagory_embedding_nums = []\n",
    "        for i in range(num_NUM):\n",
    "            self.Catagory_embedding_nums.append(\n",
    "                nn.Linear(C_dim, embedding_dim, dtype=torch.float, device=DEVICE)\n",
    "            )\n",
    "        catagories = dataset.nodes_of_fields[-num_CAT:] # number of all possible catagories nodes\n",
    "        self.Catagory_embedding_cat = nn.Embedding(sum(catagories), embedding_dim, dtype=torch.float)\n",
    "        \n",
    "        self.Field_embedding = nn.Embedding(F, embedding_dim, dtype=torch.float)\n",
    "        \n",
    "        self.transformer_decoder = TabTransformerDecoder(\n",
    "            TabHyperformer_Layer(embedding_dim,  nhead = 2 ),\n",
    "            num_layers\n",
    "        )\n",
    "        \n",
    "        # downstream task\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 2),\n",
    "        )\n",
    "        \n",
    "        # initialize MASK_FULL\n",
    "        dataset.make_mask_all()\n",
    "        # dataset.make_input_tensor()\n",
    "        \n",
    "        self.tmpmask_L2S = dataset.MASKS['L2S'].clone()\n",
    "\n",
    "        self.propagation_steps = propagation_steps\n",
    "\n",
    "    def maskout_lable(self,\n",
    "                      dataset: HGNN_dataset,\n",
    "                      query_indices: list, # must be sorted\n",
    "                      sample_indices: Optional[list] = None, \n",
    "                      ):\n",
    "        L = dataset.nodes_num['L']\n",
    "        if sample_indices is not None:\n",
    "            self.tmpmask_L2S = dataset.MASKS['L2S'].clone().detach()\n",
    "            for index, sample_indice in enumerate(sample_indices): # sample_indice in length K\n",
    "                # modify the mask to mask out the queries node's edge to it's label node\n",
    "                query_index = sample_indice.index(query_indices[index]) # query_index: index of query node in sample_indice of the batch\n",
    "                # L2S mask shape : B, S, L\n",
    "                # self.tmpmask_L2S[index, query_index][:-1] = 1 # connect to all label nodes except the unseen label\n",
    "                self.tmpmask_L2S[index, query_index] = 0\n",
    "                self.tmpmask_L2S[index, query_index][L-1] = 1 # make it as unseen label\n",
    "        else:\n",
    "            self.tmpmask_L2S = dataset.MASKS['L2S'].clone().detach()\n",
    "            for index, query in enumerate(query_indices):\n",
    "                # self.tmpmask_L2S[index, query][:-1] = 1 # connect to all label nodes except the unseen label\n",
    "                self.tmpmask_L2S[index, query] = 0\n",
    "                self.tmpmask_L2S[index, query][L-1] = 1 # make it as unseen label\n",
    "                \n",
    "                \n",
    "    def forward(self, \n",
    "                dataset: HGNN_dataset, \n",
    "                mode : str = 'train',\n",
    "                query_indices: list = None,  # must be sorted\n",
    "                K : Optional[int] = 10,\n",
    "                unseen_rate : Optional[float] = 0.1,\n",
    "                aug_strategy: Optional[str] = 'random',\n",
    "                ):\n",
    "        L, S, C, F = dataset.nodes_num['L'], dataset.nodes_num['S'], dataset.nodes_num['C'], dataset.nodes_num['F']\n",
    "        num_NUM, num_CAT = dataset.NUM_vs_CAT\n",
    "        batch_size = len(query_indices)\n",
    "        # decide scenario\n",
    "        if mode == 'train':\n",
    "            if batch_size > 1:\n",
    "                # batch mode\n",
    "                # to-do: batch mode\n",
    "                \n",
    "                pass \n",
    "            # generate subgraph with K nodes, including query_indices\n",
    "            # update mask and input tensor\n",
    "            sample_indices = dataset.get_sample(K, query_indices = query_indices) # update mask\n",
    "            dataset.make_mask_subgraph(sample_indices, query_indices)\n",
    "            \n",
    "            masks = dataset.MASKS\n",
    "            \n",
    "            # data_augmentation\n",
    "            masks['S2C'] = dataset.random_connect_unseen(masks['S2C'], mask_ratio = unseen_rate, strategy = aug_strategy)\n",
    "            \n",
    "            # get updated masked input tensor and mask \n",
    "            L_input, S_input, C_input, F_input = dataset.MASKED_INPUTS\n",
    "            L_input = L_input.clone().detach().repeat(batch_size,1,1)\n",
    "            # S_input is already in shape [batch_size, sample_size, embedding_dim], see get_sample()\n",
    "            S_input = S_input.clone().detach()\n",
    "            C_input = C_input.clone().detach().repeat(batch_size,1,1)\n",
    "            F_input = F_input.clone().detach().repeat(batch_size,1,1)\n",
    "            \n",
    "            # mask out the queries node's edge to it's label node, prevent label leakage\n",
    "            self.maskout_lable(dataset, query_indices, sample_indices)\n",
    "            \n",
    "            # the query node's indexs in sample_indices\n",
    "            query_indexs = [sample_indices[i].index(query) for i, query in enumerate(query_indices)]\n",
    "            S_ = K # the S used in transformer decoder\n",
    "            \n",
    "        elif mode == 'inferring':\n",
    "            # use all nodes in the graph \n",
    "            # get input tensor (no need to update)\n",
    "            L_input, S_input, C_input, F_input = dataset.INPUTS\n",
    "            L_input = L_input.repeat(batch_size,1,1)\n",
    "            S_input = S_input.repeat(batch_size,1,1)\n",
    "            C_input = C_input.repeat(batch_size,1,1)\n",
    "            F_input = F_input.repeat(batch_size,1,1)\n",
    "            # updata mask for inference node\n",
    "            dataset.make_mask_test(query_indices) # query node equal to inference node, only one query node is allowed\n",
    "            masks = dataset.MASKS\n",
    "            \n",
    "            self.maskout_lable(dataset, query_indices, None)\n",
    "            \n",
    "            \n",
    "            # the query node's indexs in sample_indices\n",
    "            # query_indexs = [S-1]\n",
    "            query_indexs = [S-1]*batch_size\n",
    "            S_ = S # the S used in transformer decoder\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # for S and C, we use two different embedding methods, for CAT and NUM, respectively\n",
    "        # Squeeze for making batch dimantion\n",
    "        L_embedded = self.Lable_embedding(L_input.long()).squeeze(2).float()\n",
    "        \n",
    "        S_embedded = S_input.float()\n",
    "\n",
    "        # for every numrical filed, use it's own Linear embedding layer\n",
    "        C_embedded_nums = []\n",
    "        field = dataset.nodes_of_fields\n",
    "        start = 0\n",
    "        for index, nodes in enumerate(field[:num_NUM]): # pick numrical fields\n",
    "            end = start + nodes\n",
    "            C_embedded_nums.append(self.Catagory_embedding_nums[index](C_input[:,start:end].float()))\n",
    "            start = end\n",
    "        \n",
    "        C_embedded_num = torch.cat(C_embedded_nums, dim = 1)\n",
    "        \n",
    "        catagorical_filed_nodes = sum(field[-num_CAT:]) # pick catagory fields\n",
    "        C_embedded_cat = self.Catagory_embedding_cat(C_input[:,-catagorical_filed_nodes:].squeeze(2).long()).float() \n",
    "        # print(C_embedded_num.shape, C_embedded_cat.shape)\n",
    "        C_embedded = torch.cat([C_embedded_num, C_embedded_cat], dim = 1)\n",
    "        \n",
    "        F_embedded = self.Field_embedding(F_input.long()).squeeze(2).float()\n",
    "        \n",
    "        # print(query_indices, K)\n",
    "        # print(L_embedded.shape, S_embedded.shape, C_embedded.shape, F_embedded.shape)\n",
    "        \n",
    "        \n",
    "        # propagate steps: LSCF\n",
    "        #                  LSC\n",
    "        # more steps more menory usage\n",
    "        PROPAGATE_STEPS = self.propagation_steps\n",
    "        origin_S = S_embedded.clone()\n",
    "        origin_C = C_embedded.clone()\n",
    "        origin_F = F_embedded.clone()\n",
    "        origin_L = L_embedded.clone()\n",
    "        \n",
    "        RANDOM_PROPAGATE = False\n",
    "        if RANDOM_PROPAGATE:\n",
    "            # random propagate\n",
    "            for _ in range(PROPAGATE_STEPS):\n",
    "                possible_steps = ['S2C', 'C2F', 'F2C', 'C2S', 'S2L', 'L2S']\n",
    "                selected_steps = np.random.choice(possible_steps, len(possible_steps), replace = False)\n",
    "                \n",
    "                for step in selected_steps:\n",
    "                    if step == 'L2S':\n",
    "                        S_embedded = self.transformer_decoder(S_embedded,L_embedded, origin_S,\n",
    "                                                        memory_mask = self.tmpmask_L2S.clone().detach()[:,:S_,:L])# + S_embedded\n",
    "                    elif step == 'S2C':\n",
    "                        C_embedded = self.transformer_decoder(C_embedded,S_embedded, origin_C,\n",
    "                                                        memory_mask = masks['S2C'].clone().detach()[:,:C,:S_])# + C_embedded\n",
    "                    elif step == 'C2F':\n",
    "                        F_embedded = self.transformer_decoder(F_embedded,C_embedded, origin_F,\n",
    "                                                        memory_mask = masks['C2F'].clone().detach()[:,:F,:C])# + F_embedded\n",
    "                    elif step == 'F2C':\n",
    "                        C_embedded = self.transformer_decoder(C_embedded,F_embedded, origin_C,\n",
    "                                                        memory_mask = Tensor.contiguous(masks['C2F'].clone().detach().transpose(1, 2))[:,:C,:F])# + C_embedded\n",
    "                    elif step == 'C2S':\n",
    "                        S_embedded = self.transformer_decoder(S_embedded,C_embedded, origin_S,\n",
    "                                                        memory_mask = Tensor.contiguous(masks['S2C'].clone().detach().transpose(1, 2))[:,:S_,:C])# + S_embedded\n",
    "                    elif step == 'S2L':\n",
    "                        L_embedded = self.transformer_decoder(L_embedded,S_embedded, origin_L,\n",
    "                                                        memory_mask = Tensor.contiguous(self.tmpmask_L2S.clone().detach().transpose(1, 2))[:,:L,:S_])# + L_embedded\n",
    "        else:\n",
    "            for i in range(PROPAGATE_STEPS):\n",
    "                S_embedded = self.transformer_decoder(S_embedded,L_embedded, origin_S,\n",
    "                                                    memory_mask = self.tmpmask_L2S.clone().detach()[:,:S_,:L])# + S_embedded\n",
    "                C_embedded = self.transformer_decoder(C_embedded,S_embedded, origin_C,\n",
    "                                                    memory_mask = masks['S2C'].clone().detach()[:,:C,:S_])# + C_embedded   \n",
    "                F_embedded = self.transformer_decoder(F_embedded,C_embedded, origin_F,\n",
    "                                                    memory_mask = masks['C2F'].clone().detach()[:,:F,:C])# + F_embedded\n",
    "                C_embedded = self.transformer_decoder(C_embedded,F_embedded, origin_C,\n",
    "                                                    memory_mask = Tensor.contiguous(masks['C2F'].clone().detach().transpose(1, 2))[:,:C,:F])# + C_embedded\n",
    "                S_embedded = self.transformer_decoder(S_embedded,C_embedded, origin_S,\n",
    "                                                    memory_mask = Tensor.contiguous(masks['S2C'].clone().detach().transpose(1, 2))[:,:S_,:C])# + S_embedded\n",
    "                L_embedded = self.transformer_decoder(L_embedded,S_embedded, origin_L,\n",
    "                                                    memory_mask = Tensor.contiguous(self.tmpmask_L2S.clone().detach().transpose(1, 2))[:,:L,:S_])# + L_embedded\n",
    "        \n",
    "        # print('after',S_embedded[0][0])\n",
    "        output = self.MLP(S_embedded)\n",
    "        outputs = []\n",
    "        for index, query in enumerate(query_indexs):\n",
    "            outputs.append(output[index, query])\n",
    "        outputs = torch.stack(outputs, dim = 0)\n",
    "        # output_batch = [output[:,query_indexs][query_indexs[i]] for i in range(batch_size)]\n",
    "        # print(output_batch)\n",
    "        return outputs\n",
    "  \n",
    "\n",
    "# # \n",
    "# num_layers = 1  # TransformerDecoder \n",
    "# embedding_dim = 128  # \n",
    "# hidden_dim = 64  \n",
    "\n",
    "# print('input_dims', Train_data.INPUT_DIMS)\n",
    "# model = TransformerDecoderModel(Train_data, num_layers, embedding_dim).to(DEVICE)\n",
    "\n",
    "\n",
    "# outputs = model(Train_data, mode = 'inferring', query_indices = [10,20,3000], K = 50)\n",
    "# # outputs = model(Train_data, mode = 'inferring', query_indices = [10], K = 50)\n",
    "# print(\"[q,2]:\", outputs.shape)\n",
    "# print(outputs)\n",
    "# print(outputs.softmax(dim=1))\n",
    "# output_labels = torch.argmax(outputs.softmax(dim=1), dim=1)\n",
    "# output_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "from torch import autograd\n",
    "from torcheval.metrics.aggregation.auc import AUC\n",
    "from torcheval.metrics import BinaryAUROC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "tmp_log = []\n",
    "tmp__log = []\n",
    "def train(model : nn.Module, \n",
    "          datset : HGNN_dataset, \n",
    "          epochs : int = 20,\n",
    "          batch_size : int = 8,\n",
    "          batch_size_test : int = 2,\n",
    "          lr : float = 0.0001,\n",
    "          K : int = 10,\n",
    "          unseen_rate : float = 0.1,\n",
    "          aug_strategy: str = 'random',\n",
    "          verbose : int = 1,\n",
    "            # verbose = 0: no printed log\n",
    "            # verbose = 1: print loss and AUC per train\n",
    "            # verbose = 2: print loss and AUC per epoch\n",
    "          wandb_log : bool = False,\n",
    "            # inited outside\n",
    "          log_name : str = 'unnamed',\n",
    "          ):\n",
    "    LABEL_POOL = datset.LABEL_POOL\n",
    "    TEST_LABEL_POOL = datset.TEST_LABEL_POOL\n",
    "    weight = torch.from_numpy(np.array([0.2, 1])).float().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    epoch_AUC = 0\n",
    "    epoch_AUC_test = 0\n",
    "    \n",
    "    if verbose == 1:\n",
    "        stepper_epoch = trange(epochs)\n",
    "    else:\n",
    "        stepper_epoch = range(epochs) \n",
    "    for epoch in stepper_epoch:\n",
    "        \n",
    "        '''------------------------training------------------------'''\n",
    "        \n",
    "        QUERY_POOL = list(range(len(datset.FEATURE_POOL)))\n",
    "        random.shuffle(QUERY_POOL)\n",
    "        # train\n",
    "        model.train()\n",
    "        # logs\n",
    "        loss_log = 0\n",
    "        AUC_metric = BinaryAUROC().to(DEVICE)\n",
    "        AUC_metric_test = BinaryAUROC().to(DEVICE)\n",
    "        \n",
    "        iter = 0\n",
    "        if verbose >= 2:\n",
    "            stepper = trange(len(datset.FEATURE_POOL)//batch_size)\n",
    "        else:\n",
    "            stepper = range(len(datset.FEATURE_POOL)//batch_size)\n",
    "        for index in stepper: # query through all sample nodes (not infering node)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # pick query nodes\n",
    "            query_indices = QUERY_POOL[:batch_size]\n",
    "            QUERY_POOL = QUERY_POOL[batch_size:]\n",
    "\n",
    "            outputs = model(datset,\n",
    "                            mode = 'train',\n",
    "                            query_indices = query_indices,\n",
    "                            K = K,\n",
    "                            unseen_rate = unseen_rate,\n",
    "                            aug_strategy = aug_strategy,\n",
    "                            )\n",
    "            \n",
    "            # output shape:[q,2], example: torch.Size( 2, 2]\n",
    "            # tensor([[-0.6845, -0.6323],\n",
    "            #          [-0.7770, -0.4703]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
    "                \n",
    "            # for trainning, only the query node's output is used\n",
    "            # caculate loss\n",
    "            LABEL_POOL_ = LABEL_POOL[query_indices] # shape:[q,2] ,example [[1. 0.], [1. 0.]]\n",
    "                        \n",
    "            # caculate loss\n",
    "            batch_loss = criterion(outputs, torch.tensor(LABEL_POOL_,device=DEVICE))\n",
    "            loss_log += batch_loss.item()\n",
    "            # backpropagation\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            TRUE = np.argmax(LABEL_POOL_,axis=1)\n",
    "            \n",
    "            outputs = outputs.softmax(dim=1)\n",
    "            # print(query_indices)\n",
    "            # print(outputs)\n",
    "            # print(TRUE)\n",
    "            # break\n",
    "            pred_prob_of_is_1 = [probs[1] for probs in outputs] \n",
    "            # the probability of the query node is 1 (from model output)\n",
    "            \n",
    "            # tmp_log.append(float(pred_prob_of_is_1))\n",
    "            # tmp__log.append((TRUE))\n",
    "            AUC_metric.update(torch.Tensor(pred_prob_of_is_1),torch.Tensor(TRUE))\n",
    "            torch.cuda.empty_cache()\n",
    "            # break\n",
    "            iter += 1\n",
    "            # if iter >= 100:\n",
    "            #     break\n",
    "            if index == len(datset.FEATURE_POOL)//batch_size -1 and verbose >= 2:\n",
    "                stepper.set_postfix(AUC=float(AUC_metric.compute()))\n",
    "                stepper.update()\n",
    "                \n",
    "        epoch_loss = loss_log / batch_size\n",
    "        epoch_AUC = float(AUC_metric.compute()) \n",
    "        if verbose == 1:\n",
    "            stepper_epoch.set_postfix({'loss_train':epoch_loss, 'AUC_train':epoch_AUC, 'AUC_test':epoch_AUC_test})\n",
    "        \n",
    "        '''------------------------evaluate------------------------'''\n",
    "        # evaluate\n",
    "        model.eval()\n",
    "        iter = 0\n",
    "        with torch.no_grad():\n",
    "            if verbose >= 2:\n",
    "                stepper = trange(len(datset.TEST_POOL)//batch_size_test)\n",
    "            else:\n",
    "                stepper = range(len(datset.TEST_POOL)//batch_size_test)\n",
    "                \n",
    "                \n",
    "            QUERY_POOL = list(range(len(datset.TEST_POOL)))\n",
    "            random.shuffle(QUERY_POOL)\n",
    "            \n",
    "            for index in stepper:\n",
    "                # get the last 'batch_size_test' nodes as query nodes\n",
    "                query_indices = QUERY_POOL[:batch_size_test]\n",
    "                QUERY_POOL = QUERY_POOL[batch_size_test:]\n",
    "\n",
    "                outputs = model(datset, mode = 'inferring', query_indices = query_indices, K = None)\n",
    "                LABEL_POOL_ = TEST_LABEL_POOL[query_indices]\n",
    "                TRUE = np.argmax(LABEL_POOL_,axis=1)\n",
    "                outputs = outputs.softmax(dim=1)\n",
    "                pred_prob_of_is_1 = [probs[1] for probs in outputs] \n",
    "                AUC_metric_test.update(torch.Tensor(pred_prob_of_is_1),torch.Tensor(TRUE))\n",
    "                torch.cuda.empty_cache()\n",
    "                iter += 1\n",
    "                # if iter >= 3:\n",
    "                #     break\n",
    "        epoch_AUC_test = float(AUC_metric_test.compute()) \n",
    "        if verbose == 1:\n",
    "            stepper_epoch.set_postfix({'loss_train':epoch_loss, 'AUC_train':epoch_AUC, 'AUC_test':epoch_AUC_test})\n",
    "\n",
    "        AUC_metric.reset()\n",
    "        AUC_metric_test.reset()\n",
    "        # break\n",
    "        del loss_log, AUC_metric\n",
    "        tmp_log.append(float(epoch_loss))\n",
    "        tmp__log.append(float(epoch_AUC))\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({'loss': epoch_loss, 'AUC_train': epoch_AUC, 'AUC_test': epoch_AUC_test, 'epoch': epoch})\n",
    "        \n",
    "        # print(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC: {epoch_AUC} |\")\n",
    "        if verbose >= 2:\n",
    "            print(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC_train: {epoch_AUC} | AUC_test: {epoch_AUC_test}\")\n",
    "        \n",
    "        \n",
    "        with open('logs/' + log_name + '.txt', 'a') as f:\n",
    "            # f.write(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC: {epoch_AUC}| \")\n",
    "            f.write(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC_train: {epoch_AUC}| AUC_test: {epoch_AUC_test}\\n \")\n",
    "    if verbose >=1:\n",
    "        print(f\"{log_name} | Loss: {epoch_loss} | AUC_train: {epoch_AUC} | AUC_test: {epoch_AUC_test}\")\n",
    "\n",
    "# model = TransformerDecoderModel(Main_data, \n",
    "#                                     num_layers = 1 , \n",
    "#                                     embedding_dim = 128,\n",
    "#                                     propagation_steps = 1\n",
    "#                                     ).to(DEVICE)\n",
    "# train(model, Main_data,\n",
    "#         epochs= 5,\n",
    "#         lr = 0.001,\n",
    "#         batch_size = 128,\n",
    "#         batch_size_test = 12,\n",
    "#         K = 100,\n",
    "#         unseen_rate = 0.1,\n",
    "#         verbose = 2,\n",
    "#         log_name = \"non\",\n",
    "#         wandb_log = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# plt.plot(tmp_log)\n",
    "# plt.plot(tmp__log)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 fold + wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 fold] processing...\n",
      "Set seed failed,details are  No module named 'tensorflow'\n",
      "Pytorch seed set successfully\n",
      "trian data num: 39073\n",
      "test data num: 9769\n",
      "[preprocess]: detected unseen values in column capital-loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsou/.conda/envs/xformers/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:313: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_nums {'L': 3, 'S': 39074, 'C': 471, 'F': 14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:26<00:00, 11.65it/s, AUC=0.513]\n",
      "100%|| 814/814 [02:13<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1/20 | Loss: 0.6270490967976508 | AUC_train: 0.5132692320879244 | AUC_test: 0.5839429137697654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.81it/s, AUC=0.644]\n",
      "100%|| 814/814 [02:06<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch2/20 | Loss: 0.5936343409357285 | AUC_train: 0.6437247574343148 | AUC_test: 0.7228112896976541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.94it/s, AUC=0.725]\n",
      "100%|| 814/814 [02:01<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch3/20 | Loss: 0.5489238762197094 | AUC_train: 0.7251703120563212 | AUC_test: 0.7616903272077209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.92it/s, AUC=0.748]\n",
      "100%|| 814/814 [02:04<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch4/20 | Loss: 0.5372764525360776 | AUC_train: 0.7479172690958334 | AUC_test: 0.7651575626203676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.01it/s, AUC=0.76]\n",
      "100%|| 814/814 [02:04<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch5/20 | Loss: 0.5250656209885806 | AUC_train: 0.7600024765322084 | AUC_test: 0.770268332961549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.98it/s, AUC=0.767]\n",
      "100%|| 814/814 [02:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch6/20 | Loss: 0.5199457643244869 | AUC_train: 0.766759027489687 | AUC_test: 0.7769331815830074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.78it/s, AUC=0.773]\n",
      "100%|| 814/814 [02:00<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch7/20 | Loss: 0.5139957039610891 | AUC_train: 0.772642610969536 | AUC_test: 0.7725618918058667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.81it/s, AUC=0.775]\n",
      "100%|| 814/814 [02:03<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch8/20 | Loss: 0.5096487121345953 | AUC_train: 0.774556300680566 | AUC_test: 0.7913139678427901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.88it/s, AUC=0.787]\n",
      "100%|| 814/814 [02:05<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch9/20 | Loss: 0.4970806551373176 | AUC_train: 0.786712196943882 | AUC_test: 0.7919156746251887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.91it/s, AUC=0.795]\n",
      "100%|| 814/814 [02:04<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch10/20 | Loss: 0.4852918131815768 | AUC_train: 0.7952772613962356 | AUC_test: 0.8033772136257622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.89it/s, AUC=0.793]\n",
      "100%|| 814/814 [02:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch11/20 | Loss: 0.4903783815976661 | AUC_train: 0.7928941322888438 | AUC_test: 0.8090842891696283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.91it/s, AUC=0.794]\n",
      "100%|| 814/814 [02:05<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch12/20 | Loss: 0.48768192399623067 | AUC_train: 0.7941901165819794 | AUC_test: 0.8060584410048223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.87it/s, AUC=0.802]\n",
      "100%|| 814/814 [02:02<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch13/20 | Loss: 0.478327659675619 | AUC_train: 0.8023354849276748 | AUC_test: 0.8115441685713318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.79it/s, AUC=0.816]\n",
      "100%|| 814/814 [02:00<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch14/20 | Loss: 0.46352788486298296 | AUC_train: 0.8163782256939077 | AUC_test: 0.8206504436302821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.73it/s, AUC=0.808]\n",
      "100%|| 814/814 [02:04<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch15/20 | Loss: 0.47347561479953854 | AUC_train: 0.8081445128054996 | AUC_test: 0.8202069009414515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.55it/s, AUC=0.821]\n",
      "100%|| 814/814 [02:05<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch16/20 | Loss: 0.4605924046983162 | AUC_train: 0.8206740136523553 | AUC_test: 0.8268222594136395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.90it/s, AUC=0.817]\n",
      "100%|| 814/814 [02:02<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch17/20 | Loss: 0.4635312951069028 | AUC_train: 0.8173587894769465 | AUC_test: 0.823483878766361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.77it/s, AUC=0.826]\n",
      "100%|| 814/814 [02:01<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch18/20 | Loss: 0.45364964960809334 | AUC_train: 0.8258756313382243 | AUC_test: 0.8293057597774433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.73it/s, AUC=0.826]\n",
      "100%|| 814/814 [02:05<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch19/20 | Loss: 0.45375717989938386 | AUC_train: 0.8255981914393807 | AUC_test: 0.8330416382962075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.71it/s, AUC=0.828]\n",
      "100%|| 814/814 [02:00<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch20/20 | Loss: 0.4506586127361029 | AUC_train: 0.8279165619479093 | AUC_test: 0.8367841446992692\n",
      "v9 random prob 1 | Loss: 0.4506586127361029 | AUC_train: 0.8279165619479093 | AUC_test: 0.8367841446992692\n",
      "[ 2 fold] processing...\n",
      "Set seed failed,details are  No module named 'tensorflow'\n",
      "Pytorch seed set successfully\n",
      "trian data num: 39073\n",
      "test data num: 9769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsou/.conda/envs/xformers/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:313: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preprocess]: detected unseen values in column capital-loss\n",
      "node_nums {'L': 3, 'S': 39074, 'C': 471, 'F': 14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:22<00:00, 13.76it/s, AUC=0.514]\n",
      "100%|| 814/814 [02:06<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1/20 | Loss: 0.6264629898658598 | AUC_train: 0.514335354895176 | AUC_test: 0.5815401816909644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:21<00:00, 13.88it/s, AUC=0.643]\n",
      "100%|| 814/814 [02:03<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch2/20 | Loss: 0.5933417650126763 | AUC_train: 0.6426229136475389 | AUC_test: 0.7246699950335886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.96it/s, AUC=0.723]\n",
      "100%|| 814/814 [02:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch3/20 | Loss: 0.5509356455244707 | AUC_train: 0.7226708136821437 | AUC_test: 0.7577943790793726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.97it/s, AUC=0.747]\n",
      "100%|| 814/814 [02:01<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch4/20 | Loss: 0.5353408568024064 | AUC_train: 0.7471970370437041 | AUC_test: 0.7552437577321912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.15it/s, AUC=0.754]\n",
      "100%|| 814/814 [02:03<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch5/20 | Loss: 0.5278087229375356 | AUC_train: 0.7543005253628176 | AUC_test: 0.7719691605673256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.15it/s, AUC=0.76]\n",
      "100%|| 814/814 [02:00<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch6/20 | Loss: 0.5230854170463904 | AUC_train: 0.7598565562454213 | AUC_test: 0.7724143854159277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.16it/s, AUC=0.775]\n",
      "100%|| 814/814 [01:57<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch7/20 | Loss: 0.5104123225852151 | AUC_train: 0.7750169683035163 | AUC_test: 0.7804021213384206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.09it/s, AUC=0.775]\n",
      "100%|| 814/814 [02:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch8/20 | Loss: 0.5081264151007391 | AUC_train: 0.7754501202041346 | AUC_test: 0.790720915056417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.12it/s, AUC=0.783]\n",
      "100%|| 814/814 [02:04<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch9/20 | Loss: 0.49789927435696196 | AUC_train: 0.7832195820704184 | AUC_test: 0.7856313430015741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.12it/s, AUC=0.795]\n",
      "100%|| 814/814 [02:02<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch10/20 | Loss: 0.48522822480272393 | AUC_train: 0.7945854990173251 | AUC_test: 0.8005523373299205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 14.96it/s, AUC=0.793]\n",
      "100%|| 814/814 [02:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch11/20 | Loss: 0.489629765437826 | AUC_train: 0.7926487627799083 | AUC_test: 0.7969071809597726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.09it/s, AUC=0.803]\n",
      "100%|| 814/814 [02:08<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch12/20 | Loss: 0.48034712655566264 | AUC_train: 0.8029438586435375 | AUC_test: 0.8058017393642246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.05it/s, AUC=0.81]\n",
      "100%|| 814/814 [02:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch13/20 | Loss: 0.4717303513175568 | AUC_train: 0.8100445629720784 | AUC_test: 0.8072933287795927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.20it/s, AUC=0.82]\n",
      "100%|| 814/814 [02:00<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch14/20 | Loss: 0.4593830391647357 | AUC_train: 0.8200633163440815 | AUC_test: 0.8173216594420557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.19it/s, AUC=0.811]\n",
      "100%|| 814/814 [02:03<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch15/20 | Loss: 0.47129452824429807 | AUC_train: 0.8114808920307329 | AUC_test: 0.8196777031683062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.10it/s, AUC=0.826]\n",
      "100%|| 814/814 [02:01<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch16/20 | Loss: 0.45463412771422074 | AUC_train: 0.8263866868325028 | AUC_test: 0.8250612184166828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.14it/s, AUC=0.827]\n",
      "100%|| 814/814 [01:58<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch17/20 | Loss: 0.45455587179588486 | AUC_train: 0.8265616091389897 | AUC_test: 0.8308076924513133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.14it/s, AUC=0.829]\n",
      "100%|| 814/814 [02:02<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch18/20 | Loss: 0.4500360274886723 | AUC_train: 0.8294148377981081 | AUC_test: 0.8275215011548194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.12it/s, AUC=0.83]\n",
      "100%|| 814/814 [02:01<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch19/20 | Loss: 0.45034366906864687 | AUC_train: 0.8297449838068092 | AUC_test: 0.8320459058415511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.14it/s, AUC=0.829]\n",
      "100%|| 814/814 [02:02<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch20/20 | Loss: 0.4506079810715207 | AUC_train: 0.8285879787765398 | AUC_test: 0.8252176503215529\n",
      "v9 random prob 2 | Loss: 0.4506079810715207 | AUC_train: 0.8285879787765398 | AUC_test: 0.8252176503215529\n",
      "[ 3 fold] processing...\n",
      "Set seed failed,details are  No module named 'tensorflow'\n",
      "Pytorch seed set successfully\n",
      "trian data num: 39074\n",
      "test data num: 9768\n",
      "[preprocess]: detected unseen values in column age\n",
      "[preprocess]: detected unseen values in column hours-per-week\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsou/.conda/envs/xformers/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:313: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_nums {'L': 3, 'S': 39075, 'C': 468, 'F': 14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.21it/s, AUC=0.509]\n",
      "100%|| 814/814 [02:02<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1/20 | Loss: 0.6263497311177638 | AUC_train: 0.5090712219332142 | AUC_test: 0.5694649501781881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.16it/s, AUC=0.625]\n",
      "100%|| 814/814 [02:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch2/20 | Loss: 0.5980853078894204 | AUC_train: 0.6252384817226757 | AUC_test: 0.7263344969474783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.13it/s, AUC=0.727]\n",
      "100%|| 814/814 [02:05<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch3/20 | Loss: 0.5496916653342276 | AUC_train: 0.7265466921049105 | AUC_test: 0.7419940095785672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.08it/s, AUC=0.744]\n",
      "100%|| 814/814 [02:01<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch4/20 | Loss: 0.5385984813130487 | AUC_train: 0.7443261904903689 | AUC_test: 0.7588806949508686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:20<00:00, 15.15it/s, AUC=0.757]\n",
      "100%|| 814/814 [02:09<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch5/20 | Loss: 0.5261336632553456 | AUC_train: 0.7573588507267126 | AUC_test: 0.7677802751792877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:22<00:00, 13.65it/s, AUC=0.765]\n",
      "100%|| 814/814 [02:29<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch6/20 | Loss: 0.520352111827608 | AUC_train: 0.7648013236648621 | AUC_test: 0.770643061076184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:25<00:00, 11.82it/s, AUC=0.774]\n",
      "100%|| 814/814 [02:31<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch7/20 | Loss: 0.512378603383383 | AUC_train: 0.7737627560400115 | AUC_test: 0.7719624384077893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:22<00:00, 13.67it/s, AUC=0.773]\n",
      "100%|| 814/814 [02:29<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch8/20 | Loss: 0.5106925035030841 | AUC_train: 0.7726687834456545 | AUC_test: 0.7877006254530473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:23<00:00, 13.10it/s, AUC=0.779]\n",
      "100%|| 814/814 [02:36<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch9/20 | Loss: 0.5033087910121118 | AUC_train: 0.7792797419657268 | AUC_test: 0.797676837409325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:23<00:00, 13.14it/s, AUC=0.792]\n",
      "100%|| 814/814 [02:23<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch10/20 | Loss: 0.48857648546208265 | AUC_train: 0.7919576789726946 | AUC_test: 0.8020708404706199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:33<00:00,  9.02it/s, AUC=0.787]\n",
      "100%|| 814/814 [02:40<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch11/20 | Loss: 0.49467841287351905 | AUC_train: 0.7872221341910669 | AUC_test: 0.8047951058475655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 305/305 [00:26<00:00, 11.71it/s, AUC=0.796]\n",
      " 78%|  | 634/814 [02:03<00:27,  6.57it/s]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "main_df = pd.read_csv('adult.csv')\n",
    "DEVICE = 'cuda'\n",
    "# 5-fold\n",
    "for index, (train_index, test_index) in enumerate(kf.split(main_df)):\n",
    "    # if index == 0:\n",
    "    #     continue\n",
    "    print('[', index+1, 'fold] processing...')\n",
    "    train_pool, test_pool = main_df.iloc[train_index], main_df.iloc[test_index]\n",
    "    os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "    config = {\n",
    "        \"project\": \"K_fold test 0727 \",\n",
    "        \"name\" : \"v9 baseline \" + str(index+1),\n",
    "        \"Max_epoch\" : 20,\n",
    "        \"group\": \"v9 baseline \",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 128,\n",
    "        \"batch_size_test\": 12,\n",
    "        \"K\" : 100,\n",
    "        \"num_layers\": 1,\n",
    "        \"embedding_dim\": 128,\n",
    "        \"propagation_steps\": 2,\n",
    "        \"unseen_rate\": 0,\n",
    "        \"aug_strategy\": \"unseen\",\n",
    "        \"N_BINS\": 100,\n",
    "        \"random_state\": 42,\n",
    "        \"notes\": 'v9.2',\n",
    "    }\n",
    "    set_seed(config['random_state'])\n",
    "    \n",
    "    wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "        project = config['project'], \n",
    "        name = config['name'],\n",
    "        notes = config['notes'],\n",
    "        entity = 'tabhyperformer',\n",
    "        group = config['group'],\n",
    "        # track hyperparameters and run metadata\n",
    "        config = config\n",
    "    )\n",
    "\n",
    "\n",
    "    Main_data = HGNN_dataset( train_pool, \n",
    "                             label_column = 'income', \n",
    "                             test_df = test_pool, \n",
    "                             embedding_dim = config['embedding_dim'],\n",
    "                             N_BINS = config['N_BINS'],\n",
    "                             )\n",
    "    model = TransformerDecoderModel(Main_data, \n",
    "                                    config['num_layers'], \n",
    "                                    config['embedding_dim'],\n",
    "                                    config['propagation_steps']\n",
    "                                    ).to(DEVICE)\n",
    "    train(model, Main_data,\n",
    "        epochs= config['Max_epoch'],\n",
    "        lr = config['learning_rate'],\n",
    "        batch_size = config['batch_size'],\n",
    "        batch_size_test = config['batch_size_test'],\n",
    "        K = config['K'],\n",
    "        unseen_rate = config['unseen_rate'],\n",
    "        aug_strategy = config['aug_strategy'],\n",
    "        verbose = 2,\n",
    "        log_name = config['name'],\n",
    "        wandb_log = True)\n",
    "    del Main_data, model\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
