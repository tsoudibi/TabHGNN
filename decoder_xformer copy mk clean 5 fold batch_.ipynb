{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# baic transformer Decoder model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import xformers.ops as xops\n",
    "import math \n",
    "from typing import Optional, Union\n",
    "from torch import Tensor\n",
    "import random\n",
    "\n",
    "CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "\n",
    "main_df = pd.read_csv('adult.csv')\n",
    "main_df.head()\n",
    "DEVICE = 'cuda'\n",
    "# DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_DataFrame_distribution(X_trans):\n",
    "    columns_range = {}\n",
    "    print('%15s' % '', '%6s' % 'min','%6s' % 'max', '%6s' % 'nunique')\n",
    "    \n",
    "    for column in X_trans.columns:\n",
    "        print('%15s' % column, '%6s' % X_trans[column].min(),'%6s' % X_trans[column].max(), '%6s' % X_trans[column].nunique())\n",
    "        columns_range[column] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsou/.conda/envs/xformers/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:313: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>86</td>\n",
       "      <td>188</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>304</td>\n",
       "      <td>366</td>\n",
       "      <td>384</td>\n",
       "      <td>391</td>\n",
       "      <td>407</td>\n",
       "      <td>413</td>\n",
       "      <td>423</td>\n",
       "      <td>427</td>\n",
       "      <td>448</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>184</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>324</td>\n",
       "      <td>362</td>\n",
       "      <td>387</td>\n",
       "      <td>393</td>\n",
       "      <td>397</td>\n",
       "      <td>416</td>\n",
       "      <td>424</td>\n",
       "      <td>427</td>\n",
       "      <td>468</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>160</td>\n",
       "      <td>183</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>309</td>\n",
       "      <td>366</td>\n",
       "      <td>383</td>\n",
       "      <td>393</td>\n",
       "      <td>401</td>\n",
       "      <td>414</td>\n",
       "      <td>424</td>\n",
       "      <td>427</td>\n",
       "      <td>468</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>137</td>\n",
       "      <td>184</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>324</td>\n",
       "      <td>367</td>\n",
       "      <td>387</td>\n",
       "      <td>389</td>\n",
       "      <td>401</td>\n",
       "      <td>414</td>\n",
       "      <td>424</td>\n",
       "      <td>427</td>\n",
       "      <td>468</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>137</td>\n",
       "      <td>186</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>304</td>\n",
       "      <td>366</td>\n",
       "      <td>379</td>\n",
       "      <td>389</td>\n",
       "      <td>398</td>\n",
       "      <td>414</td>\n",
       "      <td>422</td>\n",
       "      <td>426</td>\n",
       "      <td>468</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  educational-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0   46      86              188           191           215             304   \n",
       "1    1     132              184           191           215             324   \n",
       "2   18     160              183           191           215             309   \n",
       "3   18     137              184           191           215             324   \n",
       "4   40     137              186           191           215             304   \n",
       "\n",
       "   workclass  education  marital-status  occupation  relationship  race  \\\n",
       "0        366        384             391         407           413   423   \n",
       "1        362        387             393         397           416   424   \n",
       "2        366        383             393         401           414   424   \n",
       "3        367        387             389         401           414   424   \n",
       "4        366        379             389         398           414   422   \n",
       "\n",
       "   gender  native-country  income  \n",
       "0     427             448     473  \n",
       "1     427             468     472  \n",
       "2     427             468     472  \n",
       "3     427             468     472  \n",
       "4     426             468     472  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "def POOL_preprocess(df, N_BINS = 100):\n",
    "    '''\n",
    "    Preprocess the DataFrame \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        N_BINS: number of bins for each numerical column (will not be the exact number of bins, differ by distribution)\n",
    "    Return:\n",
    "        X_trans: DataFrame after preprocessing\n",
    "        ct: ColumnTransformer object, for inference and inverse transform\n",
    "        NUM_vs_CAT: tuple, (number of numerical columns, number of categorical columns - 1) \"in feature field, do not include label column\"\n",
    "        existing_values: dict, {column name: sorted list of existing values}\n",
    "    '''\n",
    "    \n",
    "    CAT = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country', 'income']\n",
    "    NUM = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "    \n",
    "    num_CAT = len(CAT)\n",
    "    num_NUM = len(NUM)  \n",
    "    \n",
    "    ct = ColumnTransformer([\n",
    "        (\"age\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"age\"]),\n",
    "        (\"fnlwgt\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='quantile', subsample=None), [\"fnlwgt\"]),\n",
    "        (\"educational-num\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='quantile', subsample=None), [\"educational-num\"]),\n",
    "        (\"capital-gain\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"capital-gain\"]),\n",
    "        (\"capital-loss\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"capital-loss\"]),\n",
    "        (\"hours-per-week\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"hours-per-week\"]),\n",
    "         ],remainder = 'passthrough', verbose_feature_names_out = False) # make sure columns are unique\n",
    "    ct.set_output(transform = 'pandas')\n",
    "    X_trans = ct.fit_transform(df) \n",
    "    \n",
    "    # store the numrical columns' existing values for identifying unseen values\n",
    "    existing_values = {}\n",
    "    for column in NUM:\n",
    "        existing_values[column] = sorted(X_trans[column].unique().astype(int))\n",
    "    for column in CAT:\n",
    "        existing_values[column] = sorted(X_trans[column].unique().astype(str))\n",
    "    \n",
    "    # apply Ordinal encoding on columns\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    OE_list = {}\n",
    "    for column in NUM + CAT:\n",
    "        OE = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value = -1)\n",
    "        X_trans[column] = OE.fit_transform(X_trans[[column]])\n",
    "        OE_list[column] = OE\n",
    "    \n",
    "    # make all columns' catagory unique\n",
    "    # 7/19: each NUM column has its own number of unique values, plus 1 for unseen values\n",
    "    # each column has it's own number of unique values. '+1' is for unseen values\n",
    "    offset = 0\n",
    "    for column in NUM + CAT:\n",
    "        X_trans[column] = X_trans[column].apply(lambda x: x + offset)\n",
    "        offset += (X_trans[column].max() - X_trans[column].min() + 1) + 1\n",
    "    \n",
    "    X_trans = X_trans.astype(int).reset_index(drop = True)\n",
    "    return X_trans, (ct, OE_list, NUM, CAT, existing_values), (num_NUM, num_CAT - 1)\n",
    "    # -1 is for the income column (label)\n",
    "main_df_SHUFFLE = main_df.sample(frac=1).reset_index(drop=True)\n",
    "X_trans, inference_package , _  = POOL_preprocess(main_df_SHUFFLE[48842//5:])\n",
    "X_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>153</td>\n",
       "      <td>189</td>\n",
       "      <td>205</td>\n",
       "      <td>215</td>\n",
       "      <td>304</td>\n",
       "      <td>369</td>\n",
       "      <td>386</td>\n",
       "      <td>391</td>\n",
       "      <td>401</td>\n",
       "      <td>413</td>\n",
       "      <td>424</td>\n",
       "      <td>427</td>\n",
       "      <td>468</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>111</td>\n",
       "      <td>183</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>297</td>\n",
       "      <td>367</td>\n",
       "      <td>383</td>\n",
       "      <td>389</td>\n",
       "      <td>405</td>\n",
       "      <td>417</td>\n",
       "      <td>424</td>\n",
       "      <td>426</td>\n",
       "      <td>468</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>116</td>\n",
       "      <td>183</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>314</td>\n",
       "      <td>366</td>\n",
       "      <td>383</td>\n",
       "      <td>394</td>\n",
       "      <td>404</td>\n",
       "      <td>414</td>\n",
       "      <td>422</td>\n",
       "      <td>427</td>\n",
       "      <td>468</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>117</td>\n",
       "      <td>183</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>307</td>\n",
       "      <td>366</td>\n",
       "      <td>383</td>\n",
       "      <td>389</td>\n",
       "      <td>401</td>\n",
       "      <td>417</td>\n",
       "      <td>424</td>\n",
       "      <td>426</td>\n",
       "      <td>468</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>173</td>\n",
       "      <td>183</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>304</td>\n",
       "      <td>366</td>\n",
       "      <td>383</td>\n",
       "      <td>393</td>\n",
       "      <td>400</td>\n",
       "      <td>416</td>\n",
       "      <td>424</td>\n",
       "      <td>427</td>\n",
       "      <td>468</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  educational-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0   29     153              189           205           215             304   \n",
       "1   27     111              183           191           215             297   \n",
       "2   24     116              183           191           215             314   \n",
       "3   30     117              183           191           215             307   \n",
       "4    8     173              183           191           215             304   \n",
       "\n",
       "   workclass  education  marital-status  occupation  relationship  race  \\\n",
       "0        369        386             391         401           413   424   \n",
       "1        367        383             389         405           417   424   \n",
       "2        366        383             394         404           414   422   \n",
       "3        366        383             389         401           417   424   \n",
       "4        366        383             393         400           416   424   \n",
       "\n",
       "   gender  native-country  income  \n",
       "0     427             468     473  \n",
       "1     426             468     472  \n",
       "2     427             468     472  \n",
       "3     426             468     472  \n",
       "4     427             468     472  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def POOL_preprocess_inference(df: pd.DataFrame,\n",
    "                              inference_package: tuple,\n",
    "                                # ct: ColumnTransformer,\n",
    "                                # OE_list: dict,\n",
    "                                # NUM: list,\n",
    "                                # CAT: list,\n",
    "                                # existing_values: dict,\n",
    "                              ):\n",
    "    '''Preprocess the DataFrame when inference\n",
    "    \n",
    "    Args:\n",
    "        `df`: DataFrame to be processed.\\n\n",
    "        `inference_package`: tuple, containing the following objects.\n",
    "            `ct`: ColumnTransformer object required for inference, which makes sure values are in the same range as training data\n",
    "            `OE_list`: dict, {column name: OrdinalEncoder object}\\n\n",
    "            `NUM`: list of numerical columns \\n\n",
    "            `CAT`: list of categorical columns\\n\n",
    "            `existing_values`: dict, {column name: sorted list of existing values}\n",
    "    '''\n",
    "    (ct, OE_list, NUM, CAT, existing_values) = inference_package\n",
    "    X_trans_ori = ct.transform(df)\n",
    "    \n",
    "    # caculate the loaction of unseen values\n",
    "    unseen_node_indexs = {}\n",
    "    offset = 0\n",
    "    for col in NUM + CAT:\n",
    "        unseen_node_indexs[col] = (int(len(existing_values[col])) + offset )\n",
    "        offset += int(len(existing_values[col])) + 1\n",
    "    \n",
    "    X_trans = X_trans_ori\n",
    "    \n",
    "    # apply Ordinal encoding on columns, and make all columns' catagory unique\n",
    "    offset = 0\n",
    "    for column in NUM + CAT:\n",
    "        OE = OE_list[column]\n",
    "        X_trans[column] = OE.transform(X_trans[[column]]) # use fitted OE to transform, the unseen values will be encoded as -1\n",
    "        if -1 in X_trans[column].tolist():\n",
    "            print('[preprocess]: detected unseen values in column', column)\n",
    "        X_trans[column] = X_trans[column].apply(lambda x: x + offset if x != -1 else unseen_node_indexs[column])\n",
    "        offset = unseen_node_indexs[column] + 1  \n",
    "\n",
    "    \n",
    "    X_trans = X_trans.astype(int).reset_index(drop = True) \n",
    "    return X_trans, unseen_node_indexs \n",
    "X_trans_ , unseen_node_indexs= POOL_preprocess_inference(main_df_SHUFFLE[:48842//5], inference_package)\n",
    "X_trans_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   min    max nunique\n",
      "            age      0     73     72\n",
      "         fnlwgt     75    174    100\n",
      "educational-num    176    189     14\n",
      "   capital-gain    191    213     19\n",
      "   capital-loss    215    262     36\n",
      " hours-per-week    265    360     83\n",
      "      workclass    362    370      9\n",
      "      education    372    387     16\n",
      " marital-status    389    395      7\n",
      "     occupation    397    411     15\n",
      "   relationship    413    418      6\n",
      "           race    420    424      5\n",
      "         gender    426    427      2\n",
      " native-country    429    470     41\n",
      "         income    472    473      2\n"
     ]
    }
   ],
   "source": [
    "check_DataFrame_distribution(X_trans_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   min    max nunique\n",
      "            age      0     73     74\n",
      "         fnlwgt     75    174    100\n",
      "educational-num    176    189     14\n",
      "   capital-gain    191    213     23\n",
      "   capital-loss    215    263     49\n",
      " hours-per-week    265    360     96\n",
      "      workclass    362    370      9\n",
      "      education    372    387     16\n",
      " marital-status    389    395      7\n",
      "     occupation    397    411     15\n",
      "   relationship    413    418      6\n",
      "           race    420    424      5\n",
      "         gender    426    427      2\n",
      " native-country    429    470     42\n",
      "         income    472    473      2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[74, 175, 190, 214, 264, 360, 370, 387, 395, 411, 418, 424, 427, 470, 473]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_DataFrame_distribution(X_trans)\n",
    "'[74, 175, 190, 214, 264, 360, 370, 387, 395, 411, 418, 424, 427, 470, 473]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Notations\\n  node: number of all nodes = L + S + C + F\\n  L: number of lable nodes + 1 (for unseen lable)\\n  S: number of sample nodes + 1 (for inference)\\n  C: number of catagory nodes + F (for each field(column)\\n  F: number of field(column) nodes (no unseen field is allowed)\\n  hidden: number of hidden representation\\n\\ndata size = \\nmask size =\\nuse nn.transformerDecoder(data,mask) to get the output\\nuse the above output as input of MLP to predict the lable   \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Notations\n",
    "  node: number of all nodes = L + S + C + F\n",
    "  L: number of lable nodes + 1 (for unseen lable)\n",
    "  S: number of sample nodes + 1 (for inference)\n",
    "  C: number of catagory nodes + F (for each field(column)\n",
    "  F: number of field(column) nodes (no unseen field is allowed)\n",
    "  hidden: number of hidden representation\n",
    "\n",
    "data size = \n",
    "mask size =\n",
    "use nn.transformerDecoder(data,mask) to get the output\n",
    "use the above output as input of MLP to predict the lable   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNN_dataset():\n",
    "    def __init__(self,\n",
    "                 data_df : pd.DataFrame,\n",
    "                 label_column : str,\n",
    "                 test_df : pd.DataFrame = None,\n",
    "                 split_ratio : float = None,\n",
    "                 embedding_dim : int = 128,\n",
    "                 ):\n",
    "        if test_df is None:\n",
    "            # shuffle and cut data\n",
    "            data_df = data_df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "            test_size = math.ceil(data_df.shape[0] * (1-split_ratio))\n",
    "            train_pool = data_df[test_size:]\n",
    "            test_pool = data_df[:test_size]\n",
    "            print('total data num:' , data_df.shape[0])\n",
    "            print('trian data num:' , train_pool.shape[0])\n",
    "            print('test data num:' , test_pool.shape[0])\n",
    "        else: \n",
    "            # given train and test data, seperated (K-fold)\n",
    "            train_pool = data_df\n",
    "            test_pool = test_df\n",
    "            print('trian data num:' , train_pool.shape[0])\n",
    "            print('test data num:' , test_pool.shape[0])\n",
    "\n",
    "        \n",
    "        # to-dos:\n",
    "        # train\n",
    "        #   \n",
    "        N_BINS = 100\n",
    "        TRAIN_POOL, self.inference_package, self.NUM_vs_CAT = POOL_preprocess(train_pool, N_BINS = N_BINS)\n",
    "        TEST_POOL, self.unseen_node_indexs_C = POOL_preprocess_inference(test_pool, self.inference_package)\n",
    "        LABEL_COLUMN = label_column\n",
    "\n",
    "        # cut feature and lable\n",
    "        FEATURE_POOL = TRAIN_POOL.drop(LABEL_COLUMN, axis=1)\n",
    "        LABEL_POOL = TRAIN_POOL[LABEL_COLUMN]\n",
    "        TEST_LABEL_POOL = TEST_POOL[LABEL_COLUMN]\n",
    "        \n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        enc = OneHotEncoder()\n",
    "        LABEL_POOL = enc.fit_transform(LABEL_POOL.values.reshape(-1,1)).toarray()\n",
    "        TEST_LABEL_POOL = enc.fit_transform(TEST_LABEL_POOL.values.reshape(-1,1)).toarray()\n",
    "\n",
    "        # L: number of lable nodes, the last node of Lable nodes is served as unknown lable node\n",
    "        L = LABEL_POOL.shape[1] + 1\n",
    "\n",
    "        # S: number of sample nodes, the last node of sample nodes is served as infering node\n",
    "        S = FEATURE_POOL.shape[0] + 1\n",
    "        \n",
    "        # F: number of field (column) nodes\n",
    "        F = FEATURE_POOL.shape[1]\n",
    "\n",
    "        # C: number of catagory nodes, each field(column) has its own \"unseen\" catagory nodes\n",
    "        self.nodes_of_fields = []\n",
    "        for column in FEATURE_POOL.columns:\n",
    "            self.nodes_of_fields.append(FEATURE_POOL[column].nunique()+1)\n",
    "        C = sum(self.nodes_of_fields) # the total number of nodes equals to the sum of nodes of each field\n",
    "        C_POOL = range(int(C))\n",
    "\n",
    "        nodes_num = {'L':L, 'S':S, 'C':C, 'F':F}\n",
    "        print('node_nums', nodes_num)\n",
    "        # print('total', L+S+C+F, 'nodes')\n",
    "        \n",
    "        # get samples indexs for each label\n",
    "        self.labe_to_index = {}\n",
    "        tmp_pool = TRAIN_POOL.copy().reset_index(drop=True)\n",
    "        for label in tmp_pool['income'].unique():\n",
    "            self.labe_to_index[label] = (tmp_pool[tmp_pool['income'] == label].index).tolist()\n",
    "        \n",
    "        self.TRAIN_POOL = TRAIN_POOL\n",
    "        self.TEST_POOL = TEST_POOL\n",
    "        self.TEST_LABEL_POOL = TEST_LABEL_POOL\n",
    "        self.LABEL_COLUMN = LABEL_COLUMN\n",
    "        self.FEATURE_POOL = FEATURE_POOL\n",
    "        self.LABEL_POOL = LABEL_POOL\n",
    "        self.C_POOL = C_POOL   \n",
    "        self.nodes_num = nodes_num\n",
    "        self.N_BINS = N_BINS\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.make_input_tensor()\n",
    "        # self.get_sample(10)        \n",
    "        self.make_mask_all()\n",
    "        \n",
    "        # self.make_mask()\n",
    "        \n",
    "        \n",
    "    def make_mask_subgraph(self,\n",
    "                  sample_indices: Optional[list] = None,\n",
    "                  query_indices: Optional[list] = None,\n",
    "                ):\n",
    "        '''Makeing masks for subgraph. Mask values are 1 if two nodes are connected, otherwise 0.\n",
    "        \n",
    "        Args:\n",
    "            sample_indices: list of list of sample node indices, in shape of `[batch_size, sample_size]`\n",
    "            query_indices: list of query node indices for each batch, in shape of `[batch_size]`\n",
    "        \n",
    "        for example, with:\n",
    "            {'L': 3, 'S': 39074, 'C': 470, 'F': 14, 'K': 10}\n",
    "            \n",
    "        the masks will be:\n",
    "            masks['L2S'] = torch.Size([16, 8]), values in torch.Size([10, 3])\\\\\n",
    "            masks['S2C'] = torch.Size([472, 16]), values in torch.Size([470, 10])\\\\\n",
    "            masls['C2F'] = torch.Size([16, 472]), values in torch.Size([14, 470])\\\\\n",
    "        Notice: xformer require the mask's tensor must align on memory, and should be slice of a tensor if shape cannot be divided by 8\n",
    "        '''\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "\n",
    "        sample_size = len(sample_indices[0])\n",
    "        # caculate masking\n",
    "        masks = {}\n",
    "        \n",
    "        tmp_L2S = []\n",
    "        tmp_S2C = []\n",
    "        for batch_indices in sample_indices:\n",
    "            # masked_POOL = self.TRAIN_POOL.iloc[batch_indices] # sample dataframe into shape (10,14)\n",
    "            \n",
    "            # label to sample\n",
    "            tmp = self.MASKS_FULL['L2S']\n",
    "            tmp = torch.index_select(tmp, 0, torch.tensor(batch_indices, device=DEVICE)) #The returned tensor does not use the same storage as the original tensor\n",
    "            # tmp = torch.zeros([math.ceil(sample_size/8) * 8, math.ceil(L/8) * 8], dtype=torch.float, device=DEVICE) \n",
    "            # label_value = masked_POOL[self.LABEL_COLUMN].values\n",
    "            # tmp[torch.arange(sample_size, device=DEVICE), torch.tensor(label_value - min(label_value), device=DEVICE)] = 1\n",
    "            shape = (math.ceil(sample_size/8) * 8, math.ceil(L/8) * 8)\n",
    "            new_tensor = torch.zeros(*shape, device=DEVICE)\n",
    "            new_tensor[:tmp.shape[0], :tmp.shape[1]] = tmp\n",
    "            tmp = new_tensor.view(*shape)\n",
    "            tmp_L2S.append(tmp)\n",
    "            # masks['L2S'] = tmp.repeat(batch_size,1,1)\n",
    "            \n",
    "\n",
    "            # sample to catagory\n",
    "            tmp = self.MASKS_FULL['S2C']\n",
    "            tmp = torch.index_select(tmp, 1, torch.tensor(batch_indices, device=DEVICE)) # The returned tensor does not use the same storage as the original tensor\n",
    "            # tmp = torch.zeros([math.ceil(C/8) * 8, math.ceil(sample_size/8) * 8], dtype=torch.float, device=DEVICE).T\n",
    "            # tmp_df = masked_POOL.drop(self.LABEL_COLUMN, axis=1)\n",
    "            # tmp[torch.arange(sample_size, device=DEVICE).unsqueeze(-1), torch.tensor(tmp_df.values, device=DEVICE)] = 1\n",
    "            # tmp = tmp.T.contiguous()\n",
    "            shape = (math.ceil(C/8) * 8, math.ceil(sample_size/8) * 8)\n",
    "            new_tensor = torch.zeros(*shape,device=DEVICE)\n",
    "            new_tensor[:tmp.shape[0], :tmp.shape[1]] = tmp\n",
    "            tmp = new_tensor.view(*shape)\n",
    "            tmp_S2C.append(tmp)\n",
    "        \n",
    "        masks['L2S'] = torch.stack(tmp_L2S, dim = 0)\n",
    "        masks['S2C'] = torch.stack(tmp_S2C, dim = 0)\n",
    "        # tmp_ = []\n",
    "        # for index, query in enumerate(query_indices):\n",
    "        #     tmp = torch.zeros([math.ceil(C/8) * 8, math.ceil(sample_size/8) * 8], dtype=torch.float, device=DEVICE).T\n",
    "        #     tmp_df = masked_POOL.drop(self.LABEL_COLUMN, axis=1)\n",
    "        #     tmp[torch.arange(sample_size, device=DEVICE).unsqueeze(-1), torch.tensor(tmp_df.values, device=DEVICE)] = 1\n",
    "        #     tmp = tmp.T.contiguous()\n",
    "        #     tmp_.append(tmp)\n",
    "        \n",
    "        # masks['S2C'] = Tensor.contiguous(tmp.repeat(batch_size,1,1))\n",
    "        # masks['S2C'] = torch.stack(tmp_, dim = 0)\n",
    "\n",
    "        # catagory to field\n",
    "        masks['C2F'] = self.MASKS_FULL['C2F'].repeat(len(query_indices),1,1)\n",
    "        self.MASKS = masks\n",
    "        self.nodes_num['K'] = sample_size\n",
    "        \n",
    "    def make_mask_all(self):\n",
    "        '''Makeing masks for the entire graph. Mask values are 1 if two nodes are connected, otherwise 0.\n",
    "\n",
    "        for example, with:\n",
    "            {'L': 3, 'S': 39074, 'C': 470, 'F': 14, 'K': 10}.\n",
    "            \n",
    "        the masks will be:\n",
    "            masks['L2S']: torch.Size([39080, 8]), values in torch.Size([39074, 3]).\\\\\n",
    "            masks['S2C']: torch.Size([472, 39080]), values in torch.Size([470, 39074]).\\\\\n",
    "            masls['C2F']: torch.Size([16, 472]), values in torch.Size([14, 470]).\\\\\n",
    "            \n",
    "        Notice: xformer require the mask's tensor must align on memory, and should be slice of a tensor if shape cannot be divided by 8\n",
    "        '''\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        # caculate masking\n",
    "        masks = {}\n",
    "        \n",
    "        # label to sample \n",
    "        tmp = torch.zeros([math.ceil(S/8) * 8, math.ceil(L/8) * 8], dtype=torch.float, device=DEVICE)\n",
    "        label_ids = self.TRAIN_POOL[self.LABEL_COLUMN].unique()\n",
    "        for i, value_df in enumerate(self.TRAIN_POOL[self.LABEL_COLUMN]):\n",
    "            for j, value_label in enumerate(label_ids):\n",
    "                if value_label == value_df:\n",
    "                    tmp[i][j] = 1\n",
    "                    break\n",
    "        masks['L2S'] = tmp\n",
    "\n",
    "        # sample to catagory\n",
    "        tmp = torch.zeros([math.ceil(C/8) * 8, math.ceil(S/8) * 8], dtype=torch.float, device=DEVICE).T\n",
    "        tmp_df = self.TRAIN_POOL.drop(self.LABEL_COLUMN, axis=1)\n",
    "        tmp[torch.arange(len(self.TRAIN_POOL), device=DEVICE).unsqueeze(-1), torch.tensor(tmp_df.values, device=DEVICE)] = 1\n",
    "        tmp = tmp.T.contiguous()\n",
    "        masks['S2C'] = tmp\n",
    "\n",
    "        # catagory to field\n",
    "        # to do : this is wrong , should connect all catagory nodes (even unseen nodes))\n",
    "        tmp = torch.zeros([math.ceil(F/8) * 8, math.ceil(C/8) * 8], dtype=torch.float, device=DEVICE)\n",
    "        unique_items = [sorted(self.FEATURE_POOL[column].unique()) for column in (self.FEATURE_POOL.columns)]\n",
    "        for i in range(F):\n",
    "            for j in (unique_items[i]):\n",
    "                tmp[i][j] = 1\n",
    "        masks['C2F'] = tmp\n",
    "        self.MASKS = masks\n",
    "        self.MASKS_FULL = masks\n",
    "        \n",
    "    def make_mask_test(self, \n",
    "                       indexs_in_test_pool : list\n",
    "                       ):\n",
    "        '''Make mask tensor for the testing scenario. \\n\n",
    "        In testing scenario, L, S, C, F remain the same, while all INPUTs are the same (sience they are initialized fixed vlaues\\n\n",
    "        All we need to do is to update masks(L2S, S2C) for the new inference node\n",
    "        '''\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        \n",
    "        masks = {}\n",
    "        tmp_L2S = []\n",
    "        tmp_S2C = []\n",
    "        # for i query nodes\n",
    "        for index_in_test_pool in indexs_in_test_pool:\n",
    "            # L2S shape: torch.Size([39080, 8]), values in torch.Size([39074, 3]).\n",
    "            # number of sample nodes : 39073 + 1 (inference node)\n",
    "            # S = 39074, -1 to convert to index of last node\n",
    "            tmp = self.MASKS_FULL['L2S'].clone().detach()\n",
    "            tmp[S-1, L-1] = 1 # connect inference node to unseen lable nodes\n",
    "            tmp_L2S.append(tmp)\n",
    "            # masks['L2S'] = tmp.unsqueeze(0)\n",
    "        \n",
    "            # S2C shape: torch.Size([472, 39080]), values in torch.Size([470, 39074]).\n",
    "            # self.MASKS_FULL['S2C'].T :[39080, 472], values in [39074, 470]\n",
    "            # self.TEST_POOL.drop(self.LABEL_COLUMN, axis=1).values[index_in_test_pool]\n",
    "            tmp = self.MASKS_FULL['S2C'].T.clone().detach()\n",
    "            # connect the last sample node (inference node) with it's catagory nodes\n",
    "            tmp[S-1, self.TEST_POOL.drop(self.LABEL_COLUMN, axis=1).values[index_in_test_pool]] = 1  \n",
    "            tmp_S2C.append(tmp.T)\n",
    "            \n",
    "            # masks['S2C'] = tmp.T.contiguous().unsqueeze(0)\n",
    "            \n",
    "        masks['L2S'] = torch.stack(tmp_L2S, dim = 0)\n",
    "        masks['S2C'] = torch.stack(tmp_S2C, dim = 0)\n",
    "        # C2F remains the same\n",
    "        masks['C2F'] = self.MASKS_FULL['C2F'].repeat(len(indexs_in_test_pool),1,1)\n",
    "        \n",
    "        self.MASKS = masks\n",
    "        # print('masks[\\'L2S\\']',masks['L2S'].shape)\n",
    "        # print('masks[\\'S2C\\']',masks['S2C'].shape)\n",
    "        # print('masks[\\'C2F\\']',masks['C2F'].shape)\n",
    "        \n",
    "        \n",
    "    def make_input_tensor(self):\n",
    "        '''Makeing input tensor for the entire graph.\n",
    "            \n",
    "        for example, with:\n",
    "            {'L': 3, 'S': 39074, 'C': 470, 'F': 14, 'K': 10}.\n",
    "                \n",
    "        the input tensor will be:\n",
    "            L_input: torch.Size([3, 1]).\n",
    "            S_input: torch.Size([39074, 128]).\n",
    "            C_input: torch.Size([470, 1]).\n",
    "            F_input: torch.Size([14, 1]).\n",
    "        '''\n",
    "        # make input tensor\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        # L\n",
    "        L_input = torch.tensor([range(L)], device=DEVICE).reshape(-1,1)\n",
    "        # print('L_input', L_input.type(), L_input.shape)\n",
    "        \n",
    "        # S (normalized by standard scaler)\n",
    "        # features = torch.tensor(self.FEATURE_POOL.values, device=DEVICE).float()\n",
    "        # normalized_features = (features - torch.mean(features, dim = 0)) / torch.std(features, dim = 0)\n",
    "        # S_input = torch.cat([normalized_features, torch.tensor([[0]*F], device=DEVICE)],dim = 0).float() # add infering node\n",
    "        \n",
    "        # S (initialize by random)\n",
    "        S_input = torch.rand(self.embedding_dim, device=DEVICE).repeat(S,1)\n",
    "        # print('S_input', S_input.type(), S_input.shape)\n",
    "        # C \n",
    "        C_input = torch.tensor([self.C_POOL], device=DEVICE).reshape(-1,1)\n",
    "        # print('C_input', C_input.type(), C_input.shape)\n",
    "        # F \n",
    "        F_input = torch.tensor([range(F)], device=DEVICE).reshape(-1,1)\n",
    "        # print('F_input', F_input.type(), F_input.shape)\n",
    "        # \n",
    "        self.INPUTS = (L_input, S_input, C_input, F_input)\n",
    "        self.INPUT_DIMS = (L_input.size(1), S_input.size(1), C_input.size(1), F_input.size(1))\n",
    "\n",
    "    def sample_with_distrubution(self, sample_size):\n",
    "        '''\n",
    "        Sample equally from each label with required sample size\\\\\n",
    "        forced to make balenced sample\n",
    "        '''\n",
    "        # decide each label's number of samples (fourced to be balenced if possible) \n",
    "        label_list = []\n",
    "        label_unique = list(self.labe_to_index.keys())\n",
    "        count = sample_size // len(label_unique)\n",
    "        remainder = sample_size % len(label_unique)\n",
    "        label_list = [item for item in label_unique for _ in range(count)]\n",
    "        label_list.extend(random.sample(label_unique, remainder))\n",
    "        # sample from indexes\n",
    "        indices = [random.choice(self.labe_to_index[label]) for label in label_list]\n",
    "        return indices     \n",
    "        \n",
    "    def get_sample(self, sample_size, query_indices = []):\n",
    "        '''get sample nodes indices, and update mask and input tensor\n",
    "        \n",
    "        Args:\n",
    "            sample_size: number of sample nodes required for each batch.\n",
    "            query_indices (optional): list of nodes indices that must be included in nodes indices (one for each batch).\n",
    "        Return:\n",
    "            sample_indices: list of sample nodes indices, in shape of `[batch_size, sample_size]`\n",
    "        \n",
    "        For example, with `sample_size = 3`, `query_indices = [1,2,3]`\\n\n",
    "        means that there are `batch_size = 3` batches, each batch has `3` nodes.\\n\n",
    "        particularly, the three batches' `sample_indices` could be:\\n\n",
    "        `[1,324,656]`, `[2, 435, 9867]`, `[3, 789, 1343]`\n",
    "        \n",
    "        The included nodes shold not and will not be repeated, in case of the lable leakage.\n",
    "        '''\n",
    "        # include specific nodes (e.g. query nodes), while remaining sample_size\n",
    "        sample_indices = []\n",
    "        if query_indices is not []:\n",
    "            for query in query_indices:\n",
    "                indices = self.sample_with_distrubution(sample_size - 1)\n",
    "                while query in indices:\n",
    "                    indices = self.sample_with_distrubution(sample_size - 1)\n",
    "                # add query nodes into sample_indices\n",
    "                indices.append(query)\n",
    "                sample_indices.append(sorted(indices))\n",
    "        else:\n",
    "            indices = self.sample_with_distrubution(sample_size - len(query_indices))\n",
    "            sample_indices.append(sorted(indices))\n",
    "        # update mask\n",
    "        # modify input tensor\n",
    "        L_input, S_input, C_input, F_input = self.INPUTS\n",
    "        S_input_masked = []\n",
    "        for i in range(len(query_indices)):\n",
    "            S_input_masked.append(torch.index_select(S_input, 0, torch.tensor(sample_indices[i], device=DEVICE)))\n",
    "        S_input_masked = torch.stack(S_input_masked, dim = 0) # convert back to tensor\n",
    "        self.MASKED_INPUTS = (L_input, S_input_masked, C_input, F_input) \n",
    "          \n",
    "        return sample_indices\n",
    "            \n",
    "# Train_data = HGNN_dataset( main_df, 'income', split_ratio = 0.8)\n",
    "# query_indices=[100,111]\n",
    "# # Train_data.get_sample(10, query_indices=query_indices)\n",
    "# Train_data.make_mask_subgraph(Train_data.get_sample(10, query_indices=query_indices), query_indices = query_indices)\n",
    "# Train_data.make_mask_subgraph(Train_data.get_sample(10, query_indices=query_indices), query_indices = query_indices)\n",
    "# Train_data.make_mask_test([0,1,3])\n",
    "# # print(Train_data.MASKS['L2S'].shape)\n",
    "# # print(Train_data.MASKS['S2C'].shape)\n",
    "# # print(Train_data.MASKS['C2F'].shape)\n",
    "# # print(Train_data.MASKED_INPUTS[1].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Optional, Any, Union, Callable\n",
    "class TabTransformerDecoder(nn.TransformerDecoder):\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_ori:Tensor, tgt_mask: Tensor | None = None, memory_mask: Tensor | None = None, tgt_key_padding_mask: Tensor | None = None, memory_key_padding_mask: Tensor | None = None) -> Tensor:\n",
    "        '''Pass the inputs (and mask) through the decoder layer in turn.\n",
    "\n",
    "        Args:\n",
    "            tgt: the sequence to the decoder (required).\n",
    "            memory: the sequence from the last layer of the encoder (required).\n",
    "            tgt_mask: the mask for the tgt sequence (optional).\n",
    "            memory_mask: the mask for the memory sequence (optional).\n",
    "            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        '''\n",
    "        output = tgt\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, memory, tgt_ori, tgt_mask=tgt_mask,\n",
    "                         memory_mask=memory_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                         memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "class TabHyperformer_Layer(nn.TransformerDecoderLayer):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=512, dropout=0, activation='relu'):\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        # remove defined modules\n",
    "        delattr(self, 'self_attn')\n",
    "        delattr(self, 'norm1')\n",
    "        delattr(self, 'dropout1')\n",
    "    \n",
    "    def forward(self, tgt, memory, tgt_ori, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        x = tgt\n",
    "        # x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))\n",
    "        x = self.norm2(tgt_ori + self._mha_block(x, memory, memory_mask))\n",
    "        # x =  x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask)\n",
    "        # x = self.norm3(x + self._ff_block(x))\n",
    "\n",
    "        return x\n",
    "    def _mha_block(self, x: Tensor, mem: Tensor,\n",
    "                   attn_mask: Optional[Tensor],) -> Tensor:\n",
    "        x = xops.memory_efficient_attention(x, mem, mem, attn_mask)\n",
    "        # return self.dropout2(x)\n",
    "        return (x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baic transformer decoder model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "class TransformerDecoderModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dataset : HGNN_dataset, \n",
    "                 num_layers = 1, \n",
    "                 embedding_dim = 128, \n",
    "                 propagation_steps = 1,\n",
    "                 ):\n",
    "        super(TransformerDecoderModel, self).__init__()\n",
    "\n",
    "        L_dim, S_dim, C_dim, F_dim = dataset.INPUT_DIMS\n",
    "        L, S, C, F = dataset.nodes_num['L'], dataset.nodes_num['S'], dataset.nodes_num['C'], dataset.nodes_num['F']\n",
    "        num_NUM , num_CAT = dataset.NUM_vs_CAT\n",
    "        \n",
    "\n",
    "        self.Lable_embedding = nn.Embedding(L, embedding_dim, dtype=torch.float)\n",
    "    \n",
    "        # self.Catagory_embedding_num = nn.Linear(C_dim, embedding_dim, dtype=torch.float)\n",
    "        # for every numrical filed, construct it's own Linear embedding layer\n",
    "        self.Catagory_embedding_nums = []\n",
    "        for i in range(num_NUM):\n",
    "            self.Catagory_embedding_nums.append(\n",
    "                nn.Linear(C_dim, embedding_dim, dtype=torch.float, device=DEVICE)\n",
    "            )\n",
    "        catagories = dataset.nodes_of_fields[-num_CAT:] # number of all possible catagories nodes\n",
    "        self.Catagory_embedding_cat = nn.Embedding(sum(catagories), embedding_dim, dtype=torch.float)\n",
    "        \n",
    "        self.Field_embedding = nn.Embedding(F, embedding_dim, dtype=torch.float)\n",
    "        \n",
    "        self.transformer_decoder = TabTransformerDecoder(\n",
    "            TabHyperformer_Layer(embedding_dim,  nhead = 2 ),\n",
    "            num_layers\n",
    "        )\n",
    "        \n",
    "        # downstream task\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 2),\n",
    "        )\n",
    "        \n",
    "        # initialize MASK_FULL\n",
    "        dataset.make_mask_all()\n",
    "        # dataset.make_input_tensor()\n",
    "        \n",
    "        self.tmpmask_L2S = dataset.MASKS['L2S'].clone()\n",
    "\n",
    "        self.propagation_steps = propagation_steps\n",
    "\n",
    "    def maskout_lable(self,\n",
    "                      dataset: HGNN_dataset,\n",
    "                      query_indices: list, # must be sorted\n",
    "                      sample_indices: Optional[list] = None, \n",
    "                      ):\n",
    "        L = dataset.nodes_num['L']\n",
    "        if sample_indices is not None:\n",
    "            self.tmpmask_L2S = dataset.MASKS['L2S'].clone().detach()\n",
    "            for index, sample_indice in enumerate(sample_indices): # sample_indice in length K\n",
    "                # modify the mask to mask out the queries node's edge to it's label node\n",
    "                query_index = sample_indice.index(query_indices[index]) # query_index: index of query node in sample_indice of the batch\n",
    "                # L2S mask shape : B, S, L\n",
    "                self.tmpmask_L2S[index, query_index] = 0\n",
    "                self.tmpmask_L2S[index, query_index][L-1] = 1 # make it as unseen label\n",
    "        else:\n",
    "            self.tmpmask_L2S = dataset.MASKS['L2S'].clone().detach()\n",
    "            for index, query in enumerate(query_indices):\n",
    "                self.tmpmask_L2S[index, query] = 0\n",
    "                self.tmpmask_L2S[index, query][L-1] = 1 # make it as unseen label\n",
    "    def forward(self, \n",
    "                dataset: HGNN_dataset, \n",
    "                mode : str = 'train',\n",
    "                query_indices: list = None,  # must be sorted\n",
    "                K : Optional[int] = 10,\n",
    "                ):\n",
    "        L, S, C, F = dataset.nodes_num['L'], dataset.nodes_num['S'], dataset.nodes_num['C'], dataset.nodes_num['F']\n",
    "        num_NUM, num_CAT = dataset.NUM_vs_CAT\n",
    "        batch_size = len(query_indices)\n",
    "        # decide scenario\n",
    "        if mode == 'train':\n",
    "            if batch_size > 1:\n",
    "                # batch mode\n",
    "                # to-do: batch mode\n",
    "                \n",
    "                pass \n",
    "            # generate subgraph with K nodes, including query_indices\n",
    "            # update mask and input tensor\n",
    "            sample_indices = dataset.get_sample(K, query_indices = query_indices) # update mask\n",
    "            dataset.make_mask_subgraph(sample_indices, query_indices)\n",
    "            masks = dataset.MASKS\n",
    "            \n",
    "            # get updated masked input tensor and mask \n",
    "            L_input, S_input, C_input, F_input = dataset.MASKED_INPUTS\n",
    "            L_input = L_input.clone().detach().repeat(batch_size,1,1)\n",
    "            # S_input is already in shape [batch_size, sample_size, embedding_dim], see get_sample()\n",
    "            S_input = S_input.clone().detach()\n",
    "            C_input = C_input.clone().detach().repeat(batch_size,1,1)\n",
    "            F_input = F_input.clone().detach().repeat(batch_size,1,1)\n",
    "            \n",
    "            # mask out the queries node's edge to it's label node, prevent label leakage\n",
    "            self.maskout_lable(dataset, query_indices, sample_indices)\n",
    "            \n",
    "            # the query node's indexs in sample_indices\n",
    "            query_indexs = [sample_indices[i].index(query) for i, query in enumerate(query_indices)]\n",
    "            S_ = K # the S used in transformer decoder\n",
    "            \n",
    "        elif mode == 'inferring':\n",
    "            # use all nodes in the graph \n",
    "            # get input tensor (no need to update)\n",
    "            L_input, S_input, C_input, F_input = dataset.INPUTS\n",
    "            L_input = L_input.repeat(batch_size,1,1)\n",
    "            S_input = S_input.repeat(batch_size,1,1)\n",
    "            C_input = C_input.repeat(batch_size,1,1)\n",
    "            F_input = F_input.repeat(batch_size,1,1)\n",
    "            # updata mask for inference node\n",
    "            dataset.make_mask_test(query_indices) # query node equal to inference node, only one query node is allowed\n",
    "            masks = dataset.MASKS\n",
    "            \n",
    "            self.maskout_lable(dataset, query_indices, None)\n",
    "            \n",
    "            \n",
    "            # the query node's indexs in sample_indices\n",
    "            # query_indexs = [S-1]\n",
    "            query_indexs = [S-1]*batch_size\n",
    "            S_ = S # the S used in transformer decoder\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # for S and C, we use two different embedding methods, for CAT and NUM, respectively\n",
    "        # Squeeze for making batch dimantion\n",
    "        L_embedded = self.Lable_embedding(L_input.long()).squeeze(2).float()\n",
    "        \n",
    "        S_embedded = S_input.float()\n",
    "\n",
    "        # for every numrical filed, use it's own Linear embedding layer\n",
    "        C_embedded_nums = []\n",
    "        field = dataset.nodes_of_fields\n",
    "        start = 0\n",
    "        for index, nodes in enumerate(field[:num_NUM]): # pick numrical fields\n",
    "            end = start + nodes\n",
    "            C_embedded_nums.append(self.Catagory_embedding_nums[index](C_input[:,start:end].float()))\n",
    "            start = end\n",
    "        \n",
    "        C_embedded_num = torch.cat(C_embedded_nums, dim = 1)\n",
    "        \n",
    "        catagorical_filed_nodes = sum(field[-num_CAT:]) # pick catagory fields\n",
    "        C_embedded_cat = self.Catagory_embedding_cat(C_input[:,-catagorical_filed_nodes:].squeeze(2).long() - sum(field[:num_NUM])).float() # - sum(field[:num_NUM] because the embedding index should start from 0\n",
    "        # print(C_embedded_num.shape, C_embedded_cat.shape)\n",
    "        C_embedded = torch.cat([C_embedded_num, C_embedded_cat], dim = 1)\n",
    "        \n",
    "        F_embedded = self.Field_embedding(F_input.long()).squeeze(2).float()\n",
    "        \n",
    "        # print(query_indices, K)\n",
    "        # print(L_embedded.shape, S_embedded.shape, C_embedded.shape, F_embedded.shape)\n",
    "        \n",
    "        \n",
    "        # propagate steps: LSCF\n",
    "        #                  LSC\n",
    "        # more steps more menory usage\n",
    "        PROPAGATE_STEPS = self.propagation_steps\n",
    "        origin_S = S_embedded.clone()\n",
    "        origin_C = C_embedded.clone()\n",
    "        origin_F = F_embedded.clone()\n",
    "        origin_L = L_embedded.clone()\n",
    "        for i in range(PROPAGATE_STEPS):\n",
    "            S_embedded = self.transformer_decoder(S_embedded,L_embedded, origin_S,\n",
    "                                                memory_mask = self.tmpmask_L2S.clone().detach()[:,:S_,:L])# + S_embedded\n",
    "            C_embedded = self.transformer_decoder(C_embedded,S_embedded, origin_C,\n",
    "                                                memory_mask = masks['S2C'].clone().detach()[:,:C,:S_])# + C_embedded   \n",
    "            F_embedded = self.transformer_decoder(F_embedded,C_embedded, origin_F,\n",
    "                                                memory_mask = masks['C2F'].clone().detach()[:,:F,:C])# + F_embedded\n",
    "            C_embedded = self.transformer_decoder(C_embedded,F_embedded, origin_C,\n",
    "                                                memory_mask = Tensor.contiguous(masks['C2F'].clone().detach().transpose(1, 2))[:,:C,:F])# + C_embedded\n",
    "            S_embedded = self.transformer_decoder(S_embedded,C_embedded, origin_S,\n",
    "                                                memory_mask = Tensor.contiguous(masks['S2C'].clone().detach().transpose(1, 2))[:,:S_,:C])# + S_embedded\n",
    "            L_embedded = self.transformer_decoder(L_embedded,S_embedded, origin_L,\n",
    "                                                memory_mask = Tensor.contiguous(self.tmpmask_L2S.clone().detach().transpose(1, 2))[:,:L,:S_])# + L_embedded\n",
    "        \n",
    "        # print('after',S_embedded[0][0])\n",
    "        output = self.MLP(S_embedded)\n",
    "        outputs = []\n",
    "        for index, query in enumerate(query_indexs):\n",
    "            outputs.append(output[index, query])\n",
    "        outputs = torch.stack(outputs, dim = 0)\n",
    "        # output_batch = [output[:,query_indexs][query_indexs[i]] for i in range(batch_size)]\n",
    "        # print(output_batch)\n",
    "        return outputs\n",
    "  \n",
    "\n",
    "# # \n",
    "# num_layers = 1  # TransformerDecoder \n",
    "# embedding_dim = 128  # \n",
    "# hidden_dim = 64  \n",
    "\n",
    "# print('input_dims', Train_data.INPUT_DIMS)\n",
    "# model = TransformerDecoderModel(Train_data, num_layers, embedding_dim).to(DEVICE)\n",
    "\n",
    "\n",
    "# outputs = model(Train_data, mode = 'inferring', query_indices = [10,20,3000], K = 50)\n",
    "# # outputs = model(Train_data, mode = 'inferring', query_indices = [10], K = 50)\n",
    "# print(\"[q,2]:\", outputs.shape)\n",
    "# print(outputs)\n",
    "# print(outputs.softmax(dim=1))\n",
    "# output_labels = torch.argmax(outputs.softmax(dim=1), dim=1)\n",
    "# output_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "from torch import autograd\n",
    "from torcheval.metrics.aggregation.auc import AUC\n",
    "from torcheval.metrics import BinaryAUROC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "tmp_log = []\n",
    "tmp__log = []\n",
    "def train(model : nn.Module, \n",
    "          datset : HGNN_dataset, \n",
    "          epochs : int = 20,\n",
    "          batch_size : int = 8,\n",
    "          batch_size_test : int = 2,\n",
    "          lr : float = 0.0001,\n",
    "          K : int = 10,\n",
    "          verbose : int = 1,\n",
    "            # verbose = 0: no printed log\n",
    "            # verbose = 1: print loss and AUC per train\n",
    "            # verbose = 2: print loss and AUC per epoch\n",
    "          wandb_log : bool = False,\n",
    "            # inited outside\n",
    "          log_name : str = 'unnamed',\n",
    "          ):\n",
    "    LABEL_POOL = datset.LABEL_POOL\n",
    "    TEST_LABEL_POOL = datset.TEST_LABEL_POOL\n",
    "    weight = torch.from_numpy(np.array([0.2, 1])).float().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    epoch_AUC = 0\n",
    "    epoch_AUC_test = 0\n",
    "    \n",
    "    if verbose == 1:\n",
    "        stepper_epoch = trange(epochs)\n",
    "    else:\n",
    "        stepper_epoch = range(epochs) \n",
    "    for epoch in stepper_epoch:\n",
    "        \n",
    "        '''------------------------training------------------------'''\n",
    "        \n",
    "        QUERY_POOL = list(range(len(datset.FEATURE_POOL)))\n",
    "        random.shuffle(QUERY_POOL)\n",
    "        # train\n",
    "        model.train()\n",
    "        # logs\n",
    "        loss_log = 0\n",
    "        AUC_metric = BinaryAUROC().to(DEVICE)\n",
    "        AUC_metric_test = BinaryAUROC().to(DEVICE)\n",
    "        \n",
    "        iter = 0\n",
    "        if verbose >= 2:\n",
    "            stepper = trange(len(datset.FEATURE_POOL)//batch_size)\n",
    "        else:\n",
    "            stepper = range(len(datset.FEATURE_POOL)//batch_size)\n",
    "        for index in stepper: # query through all sample nodes (not infering node)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # pick query nodes\n",
    "            query_indices = QUERY_POOL[:batch_size]\n",
    "            QUERY_POOL = QUERY_POOL[batch_size:]\n",
    "\n",
    "            outputs = model(datset, mode = 'train', query_indices = query_indices, K = K)\n",
    "            # output shape:[q,2], example: torch.Size( 2, 2]\n",
    "            # tensor([[-0.6845, -0.6323],\n",
    "            #          [-0.7770, -0.4703]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
    "                \n",
    "            # for trainning, only the query node's output is used\n",
    "            # caculate loss\n",
    "            LABEL_POOL_ = LABEL_POOL[query_indices] # shape:[q,2] ,example [[1. 0.], [1. 0.]]\n",
    "                        \n",
    "            # caculate loss\n",
    "            batch_loss = criterion(outputs, torch.tensor(LABEL_POOL_,device=DEVICE))\n",
    "            loss_log += batch_loss.item()\n",
    "            # backpropagation\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            TRUE = np.argmax(LABEL_POOL_,axis=1)\n",
    "            \n",
    "            outputs = outputs.softmax(dim=1)\n",
    "            # print(query_indices)\n",
    "            # print(outputs)\n",
    "            # print(TRUE)\n",
    "            # break\n",
    "            pred_prob_of_is_1 = [probs[1] for probs in outputs] \n",
    "            # the probability of the query node is 1 (from model output)\n",
    "            \n",
    "            # tmp_log.append(float(pred_prob_of_is_1))\n",
    "            # tmp__log.append((TRUE))\n",
    "            AUC_metric.update(torch.Tensor(pred_prob_of_is_1),torch.Tensor(TRUE))\n",
    "            torch.cuda.empty_cache()\n",
    "            # break\n",
    "            iter += 1\n",
    "            # if iter >= 100:\n",
    "            #     break\n",
    "            if index == len(datset.FEATURE_POOL)//batch_size -1 and verbose >= 2:\n",
    "                stepper.set_postfix(AUC=float(AUC_metric.compute()))\n",
    "                stepper.update()\n",
    "                \n",
    "        epoch_loss = loss_log / batch_size\n",
    "        epoch_AUC = float(AUC_metric.compute()) \n",
    "        if verbose == 1:\n",
    "            stepper_epoch.set_postfix({'loss_train':epoch_loss, 'AUC_train':epoch_AUC, 'AUC_test':epoch_AUC_test})\n",
    "        \n",
    "        '''------------------------evaluate------------------------'''\n",
    "        # evaluate\n",
    "        model.eval()\n",
    "        iter = 0\n",
    "        with torch.no_grad():\n",
    "            if verbose >= 2:\n",
    "                stepper = trange(len(datset.TEST_POOL)//batch_size_test)\n",
    "            else:\n",
    "                stepper = range(len(datset.TEST_POOL)//batch_size_test)\n",
    "            for index in stepper:\n",
    "                QUERY_POOL = list(range(len(datset.TEST_POOL)))\n",
    "                random.shuffle(QUERY_POOL)\n",
    "                query_indices = QUERY_POOL[:batch_size_test]\n",
    "                QUERY_POOL = QUERY_POOL[batch_size_test:]\n",
    "                outputs = model(datset, mode = 'inferring', query_indices = query_indices, K = None)\n",
    "                LABEL_POOL_ = TEST_LABEL_POOL[query_indices]\n",
    "                TRUE = np.argmax(LABEL_POOL_,axis=1)\n",
    "                outputs = outputs.softmax(dim=1)\n",
    "                pred_prob_of_is_1 = [probs[1] for probs in outputs] \n",
    "                AUC_metric_test.update(torch.Tensor(pred_prob_of_is_1),torch.Tensor(TRUE))\n",
    "                torch.cuda.empty_cache()\n",
    "                iter += 1\n",
    "                # if iter >= 100:\n",
    "                #     break\n",
    "        epoch_AUC_test = float(AUC_metric_test.compute()) \n",
    "        if verbose == 1:\n",
    "            stepper_epoch.set_postfix({'loss_train':epoch_loss, 'AUC_train':epoch_AUC, 'AUC_test':epoch_AUC_test})\n",
    "\n",
    "        AUC_metric.reset()\n",
    "        AUC_metric_test.reset()\n",
    "        # break\n",
    "        del loss_log, AUC_metric\n",
    "        tmp_log.append(float(epoch_loss))\n",
    "        tmp__log.append(float(epoch_AUC))\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({'loss': epoch_loss, 'AUC_train': epoch_AUC, 'AUC_test': epoch_AUC_test, 'epoch': epoch})\n",
    "        \n",
    "        # print(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC: {epoch_AUC} |\")\n",
    "        if verbose >= 2:\n",
    "            print(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC_train: {epoch_AUC} | AUC_test: {epoch_AUC_test}\")\n",
    "        \n",
    "        \n",
    "        with open('logs/' + log_name + '.txt', 'a') as f:\n",
    "            # f.write(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC: {epoch_AUC}| \")\n",
    "            f.write(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC_train: {epoch_AUC}| AUC_test: {epoch_AUC_test}\\n \")\n",
    "    if verbose >=1:\n",
    "        print(f\"{log_name} | Loss: {epoch_loss} | AUC_train: {epoch_AUC} | AUC_test: {epoch_AUC_test}\")\n",
    "# model = TransformerDecoderModel(Train_data, num_layers, embedding_dim).to(DEVICE)\n",
    "# train(model, Train_data,\n",
    "#       epochs= 20,\n",
    "#       lr = 0.0005,\n",
    "#       batch_size = 128,\n",
    "#       batch_size_test = 32,\n",
    "#       K = 100,\n",
    "#       verbose = 1,\n",
    "#       log_name = 'notuseful',\n",
    "#       wandb_log = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# plt.plot(tmp_log)\n",
    "# plt.plot(tmp__log)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 fold + wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 fold] processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trian data num: 39073\n",
      "test data num: 9769\n",
      "[preprocess]: detected unseen values in column capital-loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsou/.conda/envs/xformers/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:313: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_nums {'L': 3, 'S': 39074, 'C': 471, 'F': 14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 58\u001b[0m\n\u001b[1;32m     52\u001b[0m Main_data \u001b[39m=\u001b[39m HGNN_dataset( train_pool, label_column \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mincome\u001b[39m\u001b[39m'\u001b[39m, test_df \u001b[39m=\u001b[39m test_pool, embedding_dim \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39membedding_dim\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     53\u001b[0m model \u001b[39m=\u001b[39m TransformerDecoderModel(Main_data, \n\u001b[1;32m     54\u001b[0m                                 config[\u001b[39m'\u001b[39m\u001b[39mnum_layers\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m     55\u001b[0m                                 config[\u001b[39m'\u001b[39m\u001b[39membedding_dim\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     56\u001b[0m                                 config[\u001b[39m'\u001b[39m\u001b[39mpropagation_steps\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     57\u001b[0m                                 )\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 58\u001b[0m train(model, Main_data,\n\u001b[1;32m     59\u001b[0m     epochs\u001b[39m=\u001b[39;49m config[\u001b[39m'\u001b[39;49m\u001b[39mMax_epoch\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     60\u001b[0m     lr \u001b[39m=\u001b[39;49m config[\u001b[39m'\u001b[39;49m\u001b[39mlearning_rate\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     61\u001b[0m     batch_size \u001b[39m=\u001b[39;49m config[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     62\u001b[0m     batch_size_test \u001b[39m=\u001b[39;49m config[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size_test\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     63\u001b[0m     K \u001b[39m=\u001b[39;49m config[\u001b[39m'\u001b[39;49m\u001b[39mK\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     64\u001b[0m     verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m     65\u001b[0m     log_name \u001b[39m=\u001b[39;49m config[\u001b[39m'\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     66\u001b[0m     wandb_log \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     67\u001b[0m \u001b[39mdel\u001b[39;00m Main_data, model\n\u001b[1;32m     68\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[11], line 61\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, datset, epochs, batch_size, batch_size_test, lr, K, verbose, wandb_log, log_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m query_indices \u001b[39m=\u001b[39m QUERY_POOL[:batch_size]\n\u001b[1;32m     59\u001b[0m QUERY_POOL \u001b[39m=\u001b[39m QUERY_POOL[batch_size:]\n\u001b[0;32m---> 61\u001b[0m outputs \u001b[39m=\u001b[39m model(datset, mode \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, query_indices \u001b[39m=\u001b[39;49m query_indices, K \u001b[39m=\u001b[39;49m K)\n\u001b[1;32m     62\u001b[0m \u001b[39m# output shape:[q,2], example: torch.Size( 2, 2]\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m# tensor([[-0.6845, -0.6323],\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m#          [-0.7770, -0.4703]], device='cuda:0', grad_fn=<IndexBackward0>)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \n\u001b[1;32m     66\u001b[0m \u001b[39m# for trainning, only the query node's output is used\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m# caculate loss\u001b[39;00m\n\u001b[1;32m     68\u001b[0m LABEL_POOL_ \u001b[39m=\u001b[39m LABEL_POOL[query_indices] \u001b[39m# shape:[q,2] ,example [[1. 0.], [1. 0.]]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[10], line 143\u001b[0m, in \u001b[0;36mTransformerDecoderModel.forward\u001b[0;34m(self, dataset, mode, query_indices, K)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m index, nodes \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(field[:num_NUM]): \u001b[39m# pick numrical fields\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     end \u001b[39m=\u001b[39m start \u001b[39m+\u001b[39m nodes\n\u001b[0;32m--> 143\u001b[0m     C_embedded_nums\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mCatagory_embedding_nums[index](C_input[:,start:end]\u001b[39m.\u001b[39;49mfloat()))\n\u001b[1;32m    144\u001b[0m     start \u001b[39m=\u001b[39m end\n\u001b[1;32m    146\u001b[0m C_embedded_num \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(C_embedded_nums, dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "torch.random.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "main_df = pd.read_csv('adult.csv')\n",
    "DEVICE = 'cuda'\n",
    "# 5-fold\n",
    "for index, (train_index, test_index) in enumerate(kf.split(main_df)):\n",
    "    # if index <= 2:\n",
    "    #     continue\n",
    "    print('[', index+1, 'fold] processing...')\n",
    "    train_pool, test_pool = main_df.iloc[train_index], main_df.iloc[test_index]\n",
    "    os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "    config = {\n",
    "        \"project\": \"K_fold test 0727 \",\n",
    "        \"name\" : \"residual \" + str(index+1),\n",
    "        \"Max_epoch\" : 20,\n",
    "        \"group\": \"residual \",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 128,\n",
    "        \"batch_size_test\": 12,\n",
    "        \"K\" : 100,\n",
    "        \"num_layers\": 1,\n",
    "        \"embedding_dim\": 128,\n",
    "        \"propagation_steps\": 3,\n",
    "        \"notes\": 'v3',\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "        project = config['project'], \n",
    "        name = config['name'],\n",
    "        notes = config['notes'],\n",
    "        entity = 'tabhyperformer',\n",
    "        group = config['group'],\n",
    "        # track hyperparameters and run metadata\n",
    "        config = config\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    Main_data = HGNN_dataset( train_pool, label_column = 'income', test_df = test_pool, embedding_dim = config['embedding_dim'])\n",
    "    model = TransformerDecoderModel(Main_data, \n",
    "                                    config['num_layers'], \n",
    "                                    config['embedding_dim'],\n",
    "                                    config['propagation_steps']\n",
    "                                    ).to(DEVICE)\n",
    "    train(model, Main_data,\n",
    "        epochs= config['Max_epoch'],\n",
    "        lr = config['learning_rate'],\n",
    "        batch_size = config['batch_size'],\n",
    "        batch_size_test = config['batch_size_test'],\n",
    "        K = config['K'],\n",
    "        verbose = 1,\n",
    "        log_name = config['name'],\n",
    "        wandb_log = True)\n",
    "    del Main_data, model\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
