{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# baic transformer Decoder model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import xformers.ops as xops\n",
    "import math \n",
    "from typing import Optional, Union\n",
    "from torch import Tensor\n",
    "import random\n",
    "\n",
    "main_df = pd.read_csv('adult.csv')\n",
    "main_df.head()\n",
    "DEVICE = 'cuda'\n",
    "# DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_DataFrame_distribution(X_trans):\n",
    "    columns_range = {}\n",
    "    print('%15s' % '', '%6s' % 'min','%6s' % 'max', '%6s' % 'nunique')\n",
    "    \n",
    "    for column in X_trans.columns:\n",
    "        print('%15s' % column, '%6s' % X_trans[column].min(),'%6s' % X_trans[column].max(), '%6s' % X_trans[column].nunique())\n",
    "        columns_range[column] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsou/.conda/envs/xformers/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:313: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>140</td>\n",
       "      <td>176</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>304</td>\n",
       "      <td>366</td>\n",
       "      <td>375</td>\n",
       "      <td>395</td>\n",
       "      <td>405</td>\n",
       "      <td>417</td>\n",
       "      <td>424</td>\n",
       "      <td>426</td>\n",
       "      <td>462</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>78</td>\n",
       "      <td>184</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>294</td>\n",
       "      <td>366</td>\n",
       "      <td>387</td>\n",
       "      <td>389</td>\n",
       "      <td>405</td>\n",
       "      <td>414</td>\n",
       "      <td>424</td>\n",
       "      <td>427</td>\n",
       "      <td>468</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>181</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>294</td>\n",
       "      <td>366</td>\n",
       "      <td>373</td>\n",
       "      <td>393</td>\n",
       "      <td>405</td>\n",
       "      <td>416</td>\n",
       "      <td>424</td>\n",
       "      <td>427</td>\n",
       "      <td>468</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>89</td>\n",
       "      <td>184</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>304</td>\n",
       "      <td>363</td>\n",
       "      <td>387</td>\n",
       "      <td>393</td>\n",
       "      <td>398</td>\n",
       "      <td>416</td>\n",
       "      <td>421</td>\n",
       "      <td>426</td>\n",
       "      <td>468</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>168</td>\n",
       "      <td>179</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>306</td>\n",
       "      <td>366</td>\n",
       "      <td>378</td>\n",
       "      <td>391</td>\n",
       "      <td>404</td>\n",
       "      <td>413</td>\n",
       "      <td>424</td>\n",
       "      <td>427</td>\n",
       "      <td>468</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  educational-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0   29     140              176           191           215             304   \n",
       "1   39      78              184           191           215             294   \n",
       "2    1      82              181           191           215             294   \n",
       "3   16      89              184           191           215             304   \n",
       "4   11     168              179           191           215             306   \n",
       "\n",
       "   workclass  education  marital-status  occupation  relationship  race  \\\n",
       "0        366        375             395         405           417   424   \n",
       "1        366        387             389         405           414   424   \n",
       "2        366        373             393         405           416   424   \n",
       "3        363        387             393         398           416   421   \n",
       "4        366        378             391         404           413   424   \n",
       "\n",
       "   gender  native-country  income  \n",
       "0     426             462     472  \n",
       "1     427             468     472  \n",
       "2     427             468     472  \n",
       "3     426             468     472  \n",
       "4     427             468     472  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "def POOL_preprocess(df, N_BINS = 100):\n",
    "    '''\n",
    "    Preprocess the DataFrame \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        N_BINS: number of bins for each numerical column (will not be the exact number of bins, differ by distribution)\n",
    "    Return:\n",
    "        X_trans: DataFrame after preprocessing\n",
    "        ct: ColumnTransformer object, for inference and inverse transform\n",
    "        NUM_vs_CAT: tuple, (number of numerical columns, number of categorical columns - 1) \"in feature field, do not include label column\"\n",
    "        existing_values: dict, {column name: sorted list of existing values}\n",
    "    '''\n",
    "    \n",
    "    CAT = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country', 'income']\n",
    "    NUM = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "    \n",
    "    num_CAT = len(CAT)\n",
    "    num_NUM = len(NUM)  \n",
    "    \n",
    "    ct = ColumnTransformer([\n",
    "        (\"age\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"age\"]),\n",
    "        (\"fnlwgt\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='quantile', subsample=None), [\"fnlwgt\"]),\n",
    "        (\"educational-num\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='quantile', subsample=None), [\"educational-num\"]),\n",
    "        (\"capital-gain\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"capital-gain\"]),\n",
    "        (\"capital-loss\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"capital-loss\"]),\n",
    "        (\"hours-per-week\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"hours-per-week\"]),\n",
    "         ],remainder = 'passthrough', verbose_feature_names_out = False) # make sure columns are unique\n",
    "    ct.set_output(transform = 'pandas')\n",
    "    X_trans = ct.fit_transform(df) \n",
    "    \n",
    "    # store the numrical columns' existing values for identifying unseen values\n",
    "    existing_values = {}\n",
    "    for column in NUM:\n",
    "        existing_values[column] = sorted(X_trans[column].unique().astype(int))\n",
    "    for column in CAT:\n",
    "        existing_values[column] = sorted(X_trans[column].unique().astype(str))\n",
    "    \n",
    "    # apply Ordinal encoding on columns\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    OE_list = {}\n",
    "    for column in NUM + CAT:\n",
    "        OE = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value = -1)\n",
    "        X_trans[column] = OE.fit_transform(X_trans[[column]])\n",
    "        OE_list[column] = OE\n",
    "    \n",
    "    # make all columns' catagory unique\n",
    "    # 7/19: each NUM column has its own number of unique values, plus 1 for unseen values\n",
    "    # each column has it's own number of unique values. '+1' is for unseen values\n",
    "    offset = 0\n",
    "    for column in NUM + CAT:\n",
    "        X_trans[column] = X_trans[column].apply(lambda x: x + offset)\n",
    "        offset += (X_trans[column].max() - X_trans[column].min() + 1) + 1\n",
    "    \n",
    "    X_trans = X_trans.astype(int).reset_index(drop = True)\n",
    "    return X_trans, (ct, OE_list, NUM, CAT, existing_values), (num_NUM, num_CAT - 1)\n",
    "    # -1 is for the income column (label)\n",
    "main_df_SHUFFLE = main_df.sample(frac=1).reset_index(drop=True)\n",
    "X_trans, inference_package , _  = POOL_preprocess(main_df_SHUFFLE[48842//5:])\n",
    "X_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>135</td>\n",
       "      <td>184</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>294</td>\n",
       "      <td>366</td>\n",
       "      <td>387</td>\n",
       "      <td>393</td>\n",
       "      <td>409</td>\n",
       "      <td>416</td>\n",
       "      <td>424</td>\n",
       "      <td>427</td>\n",
       "      <td>429</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>167</td>\n",
       "      <td>185</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>304</td>\n",
       "      <td>364</td>\n",
       "      <td>380</td>\n",
       "      <td>393</td>\n",
       "      <td>400</td>\n",
       "      <td>416</td>\n",
       "      <td>424</td>\n",
       "      <td>426</td>\n",
       "      <td>468</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31</td>\n",
       "      <td>91</td>\n",
       "      <td>183</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>309</td>\n",
       "      <td>368</td>\n",
       "      <td>383</td>\n",
       "      <td>394</td>\n",
       "      <td>405</td>\n",
       "      <td>417</td>\n",
       "      <td>424</td>\n",
       "      <td>426</td>\n",
       "      <td>468</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>101</td>\n",
       "      <td>188</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>299</td>\n",
       "      <td>368</td>\n",
       "      <td>384</td>\n",
       "      <td>391</td>\n",
       "      <td>404</td>\n",
       "      <td>413</td>\n",
       "      <td>424</td>\n",
       "      <td>427</td>\n",
       "      <td>468</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>184</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>283</td>\n",
       "      <td>369</td>\n",
       "      <td>387</td>\n",
       "      <td>391</td>\n",
       "      <td>401</td>\n",
       "      <td>418</td>\n",
       "      <td>424</td>\n",
       "      <td>426</td>\n",
       "      <td>468</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  educational-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0    3     135              184           191           215             294   \n",
       "1   17     167              185           191           215             304   \n",
       "2   31      91              183           191           215             309   \n",
       "3   41     101              188           191           215             299   \n",
       "4    8     100              184           191           215             283   \n",
       "\n",
       "   workclass  education  marital-status  occupation  relationship  race  \\\n",
       "0        366        387             393         409           416   424   \n",
       "1        364        380             393         400           416   424   \n",
       "2        368        383             394         405           417   424   \n",
       "3        368        384             391         404           413   424   \n",
       "4        369        387             391         401           418   424   \n",
       "\n",
       "   gender  native-country  income  \n",
       "0     427             429     472  \n",
       "1     426             468     472  \n",
       "2     426             468     472  \n",
       "3     427             468     473  \n",
       "4     426             468     472  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def POOL_preprocess_inference(df: pd.DataFrame,\n",
    "                              inference_package: tuple,\n",
    "                                # ct: ColumnTransformer,\n",
    "                                # OE_list: dict,\n",
    "                                # NUM: list,\n",
    "                                # CAT: list,\n",
    "                                # existing_values: dict,\n",
    "                              ):\n",
    "    '''Preprocess the DataFrame when inference\n",
    "    \n",
    "    Args:\n",
    "        `df`: DataFrame to be processed.\\n\n",
    "        `inference_package`: tuple, containing the following objects.\n",
    "            `ct`: ColumnTransformer object required for inference, which makes sure values are in the same range as training data\n",
    "            `OE_list`: dict, {column name: OrdinalEncoder object}\\n\n",
    "            `NUM`: list of numerical columns \\n\n",
    "            `CAT`: list of categorical columns\\n\n",
    "            `existing_values`: dict, {column name: sorted list of existing values}\n",
    "    '''\n",
    "    (ct, OE_list, NUM, CAT, existing_values) = inference_package\n",
    "    X_trans_ori = ct.transform(df)\n",
    "    \n",
    "    # caculate the loaction of unseen values\n",
    "    unseen_node_indexs = {}\n",
    "    offset = 0\n",
    "    for col in NUM + CAT:\n",
    "        unseen_node_indexs[col] = (int(len(existing_values[col])) + offset )\n",
    "        offset += int(len(existing_values[col])) + 1\n",
    "    \n",
    "    X_trans = X_trans_ori\n",
    "    \n",
    "    # apply Ordinal encoding on columns, and make all columns' catagory unique\n",
    "    offset = 0\n",
    "    for column in NUM + CAT:\n",
    "        OE = OE_list[column]\n",
    "        X_trans[column] = OE.transform(X_trans[[column]]) # use fitted OE to transform, the unseen values will be encoded as -1\n",
    "        if -1 in X_trans[column].tolist():\n",
    "            print('[preprocess]: detected unseen values in column', column)\n",
    "        X_trans[column] = X_trans[column].apply(lambda x: x + offset if x != -1 else unseen_node_indexs[column])\n",
    "        offset = unseen_node_indexs[column] + 1  \n",
    "\n",
    "    \n",
    "    X_trans = X_trans.astype(int).reset_index(drop = True) \n",
    "    return X_trans, unseen_node_indexs \n",
    "X_trans_ , unseen_node_indexs= POOL_preprocess_inference(main_df_SHUFFLE[:48842//5], inference_package)\n",
    "X_trans_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   min    max nunique\n",
      "            age      0     73     71\n",
      "         fnlwgt     75    174    100\n",
      "educational-num    176    189     14\n",
      "   capital-gain    191    213     22\n",
      "   capital-loss    215    261     36\n",
      " hours-per-week    265    360     88\n",
      "      workclass    362    370      9\n",
      "      education    372    387     16\n",
      " marital-status    389    395      7\n",
      "     occupation    397    411     15\n",
      "   relationship    413    418      6\n",
      "           race    420    424      5\n",
      "         gender    426    427      2\n",
      " native-country    429    470     41\n",
      "         income    472    473      2\n"
     ]
    }
   ],
   "source": [
    "check_DataFrame_distribution(X_trans_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   min    max nunique\n",
      "            age      0     73     74\n",
      "         fnlwgt     75    174    100\n",
      "educational-num    176    189     14\n",
      "   capital-gain    191    213     23\n",
      "   capital-loss    215    263     49\n",
      " hours-per-week    265    360     96\n",
      "      workclass    362    370      9\n",
      "      education    372    387     16\n",
      " marital-status    389    395      7\n",
      "     occupation    397    411     15\n",
      "   relationship    413    418      6\n",
      "           race    420    424      5\n",
      "         gender    426    427      2\n",
      " native-country    429    470     42\n",
      "         income    472    473      2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[74, 175, 190, 214, 264, 360, 370, 387, 395, 411, 418, 424, 427, 470, 473]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_DataFrame_distribution(X_trans)\n",
    "'[74, 175, 190, 214, 264, 360, 370, 387, 395, 411, 418, 424, 427, 470, 473]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Notations\\n  node: number of all nodes = L + S + C + F\\n  L: number of lable nodes + 1 (for unseen lable)\\n  S: number of sample nodes + 1 (for inference)\\n  C: number of catagory nodes + F (for each field(column)\\n  F: number of field(column) nodes (no unseen field is allowed)\\n  hidden: number of hidden representation\\n\\ndata size = \\nmask size =\\nuse nn.transformerDecoder(data,mask) to get the output\\nuse the above output as input of MLP to predict the lable   \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Notations\n",
    "  node: number of all nodes = L + S + C + F\n",
    "  L: number of lable nodes + 1 (for unseen lable)\n",
    "  S: number of sample nodes + 1 (for inference)\n",
    "  C: number of catagory nodes + F (for each field(column)\n",
    "  F: number of field(column) nodes (no unseen field is allowed)\n",
    "  hidden: number of hidden representation\n",
    "\n",
    "data size = \n",
    "mask size =\n",
    "use nn.transformerDecoder(data,mask) to get the output\n",
    "use the above output as input of MLP to predict the lable   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data num: 48842\n",
      "trian data num: 39073\n",
      "test data num: 9769\n",
      "[preprocess]: detected unseen values in column capital-loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsou/.conda/envs/xformers/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:313: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_nums {'L': 3, 'S': 39074, 'C': 471, 'F': 14}\n",
      "total 39562 nodes\n",
      "L_input torch.cuda.LongTensor torch.Size([3, 1])\n",
      "S_input torch.cuda.FloatTensor torch.Size([39074, 128])\n",
      "C_input torch.cuda.LongTensor torch.Size([471, 1])\n",
      "F_input torch.cuda.LongTensor torch.Size([14, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5917, 6797, 10850, 14174, 25840, 28655, 28828, 28920, 32102, 38702]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HGNN_dataset():\n",
    "    def __init__(self,\n",
    "                 data_df : pd.DataFrame,\n",
    "                 split_ratio : float ,\n",
    "                 label_column : str,\n",
    "                 ):\n",
    "        # shuffle and cut data\n",
    "        data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_size = math.ceil(data_df.shape[0] * (1-split_ratio))\n",
    "        train_pool = data_df[test_size:]\n",
    "        test_pool = data_df[:test_size]\n",
    "        print('total data num:' , data_df.shape[0])\n",
    "        print('trian data num:' , train_pool.shape[0])\n",
    "        print('test data num:' , test_pool.shape[0])\n",
    "        \n",
    "        # to-dos:\n",
    "        # train\n",
    "        #   \n",
    "        N_BINS = 100\n",
    "        TRAIN_POOL, self.inference_package, self.NUM_vs_CAT = POOL_preprocess(train_pool, N_BINS = N_BINS)\n",
    "        TEST_POOL, self.unseen_node_indexs_C = POOL_preprocess_inference(test_pool, self.inference_package)\n",
    "        LABEL_COLUMN = label_column\n",
    "\n",
    "        # cut feature and lable\n",
    "        FEATURE_POOL = TRAIN_POOL.drop(LABEL_COLUMN, axis=1)\n",
    "        LABEL_POOL = TRAIN_POOL[LABEL_COLUMN]\n",
    "        TEST_LABEL_POOL = TEST_POOL[LABEL_COLUMN]\n",
    "        \n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        enc = OneHotEncoder()\n",
    "        LABEL_POOL = enc.fit_transform(LABEL_POOL.values.reshape(-1,1)).toarray()\n",
    "        TEST_LABEL_POOL = enc.fit_transform(TEST_LABEL_POOL.values.reshape(-1,1)).toarray()\n",
    "\n",
    "        # L: number of lable nodes, the last node of Lable nodes is served as unknown lable node\n",
    "        L = LABEL_POOL.shape[1] + 1\n",
    "\n",
    "        # S: number of sample nodes, the last node of sample nodes is served as infering node\n",
    "        S = FEATURE_POOL.shape[0] + 1\n",
    "        \n",
    "        # F: number of field (column) nodes\n",
    "        F = FEATURE_POOL.shape[1]\n",
    "\n",
    "        # C: number of catagory nodes, each field(column) has its own \"unseen\" catagory nodes\n",
    "        self.nodes_of_fields = []\n",
    "        for column in FEATURE_POOL.columns:\n",
    "            self.nodes_of_fields.append(FEATURE_POOL[column].nunique()+1)\n",
    "        C = sum(self.nodes_of_fields) # the total number of nodes equals to the sum of nodes of each field\n",
    "        C_POOL = range(int(C))\n",
    "\n",
    "        nodes_num = {'L':L, 'S':S, 'C':C, 'F':F}\n",
    "        print('node_nums', nodes_num)\n",
    "        print('total', L+S+C+F, 'nodes')\n",
    "        \n",
    "        # get samples indexs for each label\n",
    "        self.labe_to_index = {}\n",
    "        tmp_pool = TRAIN_POOL.copy().reset_index(drop=True)\n",
    "        for label in tmp_pool['income'].unique():\n",
    "            self.labe_to_index[label] = (tmp_pool[tmp_pool['income'] == label].index).tolist()\n",
    "        \n",
    "        self.TRAIN_POOL = TRAIN_POOL\n",
    "        self.TEST_POOL = TEST_POOL\n",
    "        self.TEST_LABEL_POOL = TEST_LABEL_POOL\n",
    "        self.LABEL_COLUMN = LABEL_COLUMN\n",
    "        self.FEATURE_POOL = FEATURE_POOL\n",
    "        self.LABEL_POOL = LABEL_POOL\n",
    "        self.C_POOL = C_POOL   \n",
    "        self.nodes_num = nodes_num\n",
    "        self.N_BINS = N_BINS\n",
    "\n",
    "        \n",
    "        self.make_input_tensor()\n",
    "        # self.get_sample(10)        \n",
    "        self.make_mask_all()\n",
    "        \n",
    "        # self.make_mask()\n",
    "        \n",
    "        \n",
    "    def make_mask_subgraph(self,\n",
    "                  sample_indices: Optional[list] = None,\n",
    "                ):\n",
    "        '''Makeing masks for subgraph. Mask values are 1 if two nodes are connected, otherwise 0.\n",
    "        \n",
    "        Args:\n",
    "            sample_indices: list of sample node indices\n",
    "        \n",
    "        for example, with:\n",
    "            {'L': 3, 'S': 39074, 'C': 470, 'F': 14, 'K': 10}\n",
    "            \n",
    "        the masks will be:\n",
    "            masks['L2S'] = torch.Size([16, 8]), values in torch.Size([10, 3])\\\\\n",
    "            masks['S2C'] = torch.Size([472, 16]), values in torch.Size([470, 10])\\\\\n",
    "            masls['C2F'] = torch.Size([16, 472]), values in torch.Size([14, 470])\\\\\n",
    "        Notice: xformer require the mask's tensor must align on memory, and should be slice of a tensor if shape cannot be divided by 8\n",
    "        '''\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "\n",
    "        sample_size = len(sample_indices)\n",
    "        masked_POOL = self.TRAIN_POOL.iloc[sample_indices] # sample dataframe into shape (10,14)\n",
    "        # caculate masking\n",
    "        masks = {}\n",
    "\n",
    "        # label to sample\n",
    "        tmp = torch.zeros([math.ceil(sample_size/8) * 8, math.ceil(L/8) * 8], dtype=torch.float, device=DEVICE) \n",
    "        label_value = masked_POOL[self.LABEL_COLUMN].values\n",
    "        tmp[torch.arange(sample_size), torch.tensor(label_value - min(label_value))] = 1\n",
    "        masks['L2S'] = tmp\n",
    "\n",
    "        # sample to catagory\n",
    "        tmp = torch.zeros([math.ceil(C/8) * 8, math.ceil(sample_size/8) * 8], dtype=torch.float, device=DEVICE).T\n",
    "        tmp_df = masked_POOL.drop(self.LABEL_COLUMN, axis=1)\n",
    "        tmp[torch.arange(sample_size).unsqueeze(-1), torch.tensor(tmp_df.values)] = 1\n",
    "        tmp = tmp.T.contiguous()\n",
    "        \n",
    "        masks['S2C'] = Tensor.contiguous(tmp)\n",
    "\n",
    "        # catagory to field\n",
    "        masks['C2F'] = self.MASKS_FULL['C2F']\n",
    "        \n",
    "        self.MASKS = masks\n",
    "        self.nodes_num['K'] = sample_size\n",
    "        \n",
    "    def make_mask_all(self):\n",
    "        '''Makeing masks for the entire graph. Mask values are 1 if two nodes are connected, otherwise 0.\n",
    "\n",
    "        for example, with:\n",
    "            {'L': 3, 'S': 39074, 'C': 470, 'F': 14, 'K': 10}.\n",
    "            \n",
    "        the masks will be:\n",
    "            masks['L2S']: torch.Size([39080, 8]), values in torch.Size([39074, 3]).\\\\\n",
    "            masks['S2C']: torch.Size([472, 39080]), values in torch.Size([470, 39074]).\\\\\n",
    "            masls['C2F']: torch.Size([16, 472]), values in torch.Size([14, 470]).\\\\\n",
    "            \n",
    "        Notice: xformer require the mask's tensor must align on memory, and should be slice of a tensor if shape cannot be divided by 8\n",
    "        '''\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        # caculate masking\n",
    "        masks = {}\n",
    "        \n",
    "        # label to sample \n",
    "        tmp = torch.zeros([math.ceil(S/8) * 8, math.ceil(L/8) * 8], dtype=torch.float, device=DEVICE)\n",
    "        label_ids = self.TRAIN_POOL[self.LABEL_COLUMN].unique()\n",
    "        for i, value_df in enumerate(self.TRAIN_POOL[self.LABEL_COLUMN]):\n",
    "            for j, value_label in enumerate(label_ids):\n",
    "                if value_label == value_df:\n",
    "                    tmp[i][j] = 1\n",
    "                    break\n",
    "        masks['L2S'] = tmp\n",
    "\n",
    "        # sample to catagory\n",
    "        tmp = torch.zeros([math.ceil(C/8) * 8, math.ceil(S/8) * 8], dtype=torch.float, device=DEVICE).T\n",
    "        tmp_df = self.TRAIN_POOL.drop(self.LABEL_COLUMN, axis=1)\n",
    "        tmp[torch.arange(len(self.TRAIN_POOL)).unsqueeze(-1), torch.tensor(tmp_df.values)] = 1\n",
    "        tmp = tmp.T.contiguous()\n",
    "        masks['S2C'] = tmp\n",
    "\n",
    "        # catagory to field\n",
    "        # to do : this is wrong , should connect all catagory nodes (even unseen nodes))\n",
    "        tmp = torch.zeros([math.ceil(F/8) * 8, math.ceil(C/8) * 8], dtype=torch.float, device=DEVICE)\n",
    "        unique_items = [sorted(self.FEATURE_POOL[column].unique()) for column in (self.FEATURE_POOL.columns)]\n",
    "        for i in range(F):\n",
    "            for j in (unique_items[i]):\n",
    "                tmp[i][j] = 1\n",
    "        masks['C2F'] = tmp\n",
    "        \n",
    "        self.MASKS = masks\n",
    "        self.MASKS_FULL = masks\n",
    "        \n",
    "    def make_mask_test(self, index_in_test_pool ):\n",
    "        '''Make mask tensor for the testing scenario. \\n\n",
    "        In testing scenario, L, S, C, F remain the same, while all INPUTs are the same (sience they are initialized fixed vlaues\\n\n",
    "        All we need to do is to update masks(L2S, S2C) for the new inference node\n",
    "        '''\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        \n",
    "        masks = {}\n",
    "        # L2S shape: torch.Size([39080, 8]), values in torch.Size([39074, 3]).\n",
    "        # number of sample nodes : 39073 + 1 (inference node)\n",
    "        # S = 39074, -1 to convert to index of last node\n",
    "        tmp = self.MASKS_FULL['L2S'].clone().detach()\n",
    "        tmp[S-1, L-1] = 1 # connect inference node to unseen lable nodes\n",
    "        masks['L2S'] = tmp\n",
    "        \n",
    "        # S2C shape: torch.Size([472, 39080]), values in torch.Size([470, 39074]).\n",
    "        # self.MASKS_FULL['S2C'].T :[39080, 472], values in [39074, 470]\n",
    "        # self.TEST_POOL.drop(self.LABEL_COLUMN, axis=1).values[index_in_test_pool]\n",
    "        tmp = self.MASKS_FULL['S2C'].T.clone().detach()\n",
    "        tmp[S-1, self.TEST_POOL.drop(self.LABEL_COLUMN, axis=1).values[index_in_test_pool]] = 1 \n",
    "        masks['S2C'] = tmp.T.contiguous()\n",
    "        \n",
    "        # C2F remains the same\n",
    "        masks['C2F'] = self.MASKS_FULL['C2F']\n",
    "        \n",
    "        self.MASKS = masks\n",
    "        # print('masks[\\'L2S\\']',masks['L2S'].shape)\n",
    "        # print('masks[\\'S2C\\']',masks['S2C'].shape)\n",
    "        # print('masks[\\'C2F\\']',masks['C2F'].shape)\n",
    "        \n",
    "        \n",
    "    def make_input_tensor(self):\n",
    "        '''Makeing input tensor for the entire graph.\n",
    "            \n",
    "        for example, with:\n",
    "            {'L': 3, 'S': 39074, 'C': 470, 'F': 14, 'K': 10}.\n",
    "                \n",
    "        the input tensor will be:\n",
    "            L_input: torch.Size([3, 1]).\n",
    "            S_input: torch.Size([39074, 128]).\n",
    "            C_input: torch.Size([470, 1]).\n",
    "            F_input: torch.Size([14, 1]).\n",
    "        '''\n",
    "        # make input tensor\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        # L\n",
    "        L_input = torch.tensor([range(L)], device=DEVICE).reshape(-1,1)\n",
    "        print('L_input', L_input.type(), L_input.shape)\n",
    "        \n",
    "        # S (normalized by standard scaler)\n",
    "        # features = torch.tensor(self.FEATURE_POOL.values, device=DEVICE).float()\n",
    "        # normalized_features = (features - torch.mean(features, dim = 0)) / torch.std(features, dim = 0)\n",
    "        # S_input = torch.cat([normalized_features, torch.tensor([[0]*F], device=DEVICE)],dim = 0).float() # add infering node\n",
    "        \n",
    "        # S (initialize by random)\n",
    "        S_input = torch.rand(128, device=DEVICE).repeat(S,1)\n",
    "        \n",
    "        print('S_input', S_input.type(), S_input.shape)\n",
    "        # C \n",
    "        C_input = torch.tensor([self.C_POOL], device=DEVICE).reshape(-1,1)\n",
    "        print('C_input', C_input.type(), C_input.shape)\n",
    "        # F \n",
    "        F_input = torch.tensor([range(F)], device=DEVICE).reshape(-1,1)\n",
    "        print('F_input', F_input.type(), F_input.shape)\n",
    "        # \n",
    "        self.INPUTS = (L_input, S_input, C_input, F_input)\n",
    "        self.INPUT_DIMS = (L_input.size(1), S_input.size(1), C_input.size(1), F_input.size(1))\n",
    "\n",
    "    def sample_with_distrubution(self, sample_size):\n",
    "        '''\n",
    "        Sample equally from each label with required sample size\\\\\n",
    "        forced to make balenced sample\n",
    "        '''\n",
    "        # decide each label's number of samples (fourced to be balenced if possible) \n",
    "        label_list = []\n",
    "        label_unique = list(self.labe_to_index.keys())\n",
    "        count = sample_size // len(label_unique)\n",
    "        remainder = sample_size % len(label_unique)\n",
    "        label_list = [item for item in label_unique for _ in range(count)]\n",
    "        label_list.extend(random.sample(label_unique, remainder))\n",
    "        # sample from indexes\n",
    "        indices = [random.choice(self.labe_to_index[label]) for label in label_list]\n",
    "        return indices     \n",
    "        \n",
    "    def get_sample(self, sample_size, inculde = []):\n",
    "        '''get sample nodes indices, and update mask and input tensor\n",
    "        \n",
    "        Args:\n",
    "            sample_size: number of sample nodes required.\n",
    "            inculde (optional): list of nodes indices that must be included in the nodes indices.\n",
    "        \n",
    "        The inculded nodes shold not and will not be repeated, in case of the lable leakage.\n",
    "        '''\n",
    "        # inculde specific nodes (e.g. query nodes), while remaining sample_size\n",
    "        sample_indices = self.sample_with_distrubution(sample_size - len(inculde))\n",
    "        if inculde is not []:\n",
    "            while inculde in sample_indices:\n",
    "                sample_indices = self.sample_with_distrubution(sample_size - len(inculde))\n",
    "            # add inculde nodes into sample_indices\n",
    "            for node in inculde:\n",
    "                sample_indices.append(node)\n",
    "            sample_indices = sorted(sample_indices)\n",
    "        # update mask\n",
    "        sample_indices = sorted(sample_indices)\n",
    "        \n",
    "        # modify input tensor\n",
    "        L_input, S_input, C_input, F_input = self.INPUTS\n",
    "        S_input_masked = torch.index_select(S_input, 0, torch.tensor(sample_indices, device=DEVICE))\n",
    "        self.MASKED_INPUTS = (L_input, S_input_masked, C_input, F_input) \n",
    "          \n",
    "        return sample_indices\n",
    "            \n",
    "Train_data = HGNN_dataset( main_df, 0.8, 'income')\n",
    "Train_data.get_sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Optional, Any, Union, Callable\n",
    "\n",
    "class TabHyperformer_Layer(nn.TransformerDecoderLayer):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu'):\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        # remove defined modules\n",
    "        delattr(self, 'self_attn')\n",
    "        delattr(self, 'norm1')\n",
    "        delattr(self, 'dropout1')\n",
    "    \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        x = tgt\n",
    "        # x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))\n",
    "        x = self.norm2(x + self._mha_block(x, memory, memory_mask))\n",
    "        # x =  x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask)\n",
    "        # x = self.norm3(x + self._ff_block(x))\n",
    "\n",
    "        return x\n",
    "    def _mha_block(self, x: Tensor, mem: Tensor,\n",
    "                   attn_mask: Optional[Tensor],) -> Tensor:\n",
    "        x = xops.memory_efficient_attention(x, mem, mem, attn_mask)\n",
    "        # return self.dropout2(x)\n",
    "        return (x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dims (1, 128, 1, 1)\n",
      "L_input torch.cuda.LongTensor torch.Size([3, 1])\n",
      "S_input torch.cuda.FloatTensor torch.Size([39074, 128])\n",
      "C_input torch.cuda.LongTensor torch.Size([471, 1])\n",
      "F_input torch.cuda.LongTensor torch.Size([14, 1])\n",
      "模型輸出的大小[q,2]: torch.Size([2, 2])\n",
      "tensor([[0.4606, 0.6746],\n",
      "        [0.6646, 0.6910]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([[0.4467, 0.5533],\n",
      "        [0.4934, 0.5066]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baic transformer decoder model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "from tqdm import trange\n",
    "\n",
    "class TransformerDecoderModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dataset : HGNN_dataset, \n",
    "                 num_layers, \n",
    "                 embedding_dim, \n",
    "                 ):\n",
    "        super(TransformerDecoderModel, self).__init__()\n",
    "\n",
    "        L_dim, S_dim, C_dim, F_dim = dataset.INPUT_DIMS\n",
    "        L, S, C, F = dataset.nodes_num['L'], dataset.nodes_num['S'], dataset.nodes_num['C'], dataset.nodes_num['F']\n",
    "        num_NUM , num_CAT = dataset.NUM_vs_CAT\n",
    "        \n",
    "\n",
    "        self.Lable_embedding = nn.Embedding(L, embedding_dim, dtype=torch.float)\n",
    "    \n",
    "        # self.Catagory_embedding_num = nn.Linear(C_dim, embedding_dim, dtype=torch.float)\n",
    "        # for every numrical filed, construct it's own Linear embedding layer\n",
    "        self.Catagory_embedding_nums = []\n",
    "        for i in range(num_NUM):\n",
    "            self.Catagory_embedding_nums.append(\n",
    "                nn.Linear(C_dim, embedding_dim, dtype=torch.float, device=DEVICE)\n",
    "            )\n",
    "        catagories = dataset.nodes_of_fields[-num_CAT:] # number of all possible catagories nodes\n",
    "        self.Catagory_embedding_cat = nn.Embedding(sum(catagories), embedding_dim, dtype=torch.float)\n",
    "        \n",
    "        self.Field_embedding = nn.Embedding(F, embedding_dim, dtype=torch.float)\n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            TabHyperformer_Layer(embedding_dim,  nhead = 2 ),\n",
    "            num_layers\n",
    "        )\n",
    "        \n",
    "        # downstream task\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 2),\n",
    "        )\n",
    "        \n",
    "        # initialize MASK_FULL\n",
    "        dataset.make_mask_all()\n",
    "        dataset.make_input_tensor()\n",
    "        \n",
    "        self.tmpmask_L2S = dataset.MASKS['L2S'].clone()\n",
    "\n",
    "    def maskout_lable(self,\n",
    "                      dataset: HGNN_dataset,\n",
    "                      query_indices: list, # must be sorted\n",
    "                      sample_indices: Optional[list] = None, \n",
    "                      ):\n",
    "        if sample_indices is not None:\n",
    "            for query in query_indices:\n",
    "                # modify the mask to mask out the queries node's edge to it's label node\n",
    "                L = dataset.nodes_num['L']\n",
    "                query_index = sample_indices.index(query) # query_index: index of query node in sample_indices\n",
    "                self.tmpmask_L2S = dataset.MASKS['L2S'].clone().detach()\n",
    "                self.tmpmask_L2S[query_index] = 0\n",
    "                self.tmpmask_L2S[query_index][L-1] = 1 # make it as unseen label\n",
    "        else:\n",
    "            for query in query_indices:\n",
    "                L = dataset.nodes_num['L']\n",
    "                self.tmpmask_L2S = dataset.MASKS['L2S'].clone().detach()\n",
    "                self.tmpmask_L2S[query] = 0\n",
    "                self.tmpmask_L2S[query][L-1] = 1 # make it as unseen label\n",
    "    def forward(self, \n",
    "                dataset: HGNN_dataset, \n",
    "                mode : str = 'train',\n",
    "                query_indices: list = None,  # must be sorted\n",
    "                K : Optional[int] = 10,\n",
    "                ):\n",
    "        L, S, C, F = dataset.nodes_num['L'], dataset.nodes_num['S'], dataset.nodes_num['C'], dataset.nodes_num['F']\n",
    "        num_NUM, num_CAT = dataset.NUM_vs_CAT\n",
    "        \n",
    "        # decide scenario\n",
    "        if mode == 'train':\n",
    "            # generate subgraph with K nodes, including query_indices\n",
    "            # update mask and input tensor\n",
    "            self.sample_indices = Train_data.get_sample(K, inculde = query_indices) # update mask\n",
    "            Train_data.make_mask_subgraph(self.sample_indices)\n",
    "            # get updated masked input tensor and mask \n",
    "            L_input, S_input, C_input, F_input = dataset.MASKED_INPUTS\n",
    "            masks = dataset.MASKS\n",
    "            # mask out the queries node's edge to it's label node, prevent label leakage\n",
    "            self.maskout_lable(dataset, query_indices, self.sample_indices)\n",
    "            \n",
    "            # the query node's indexs in sample_indices\n",
    "            query_indexs = [self.sample_indices.index(query) for query in query_indices]\n",
    "            S_ = K # the S used in transformer decoder\n",
    "            \n",
    "        elif mode == 'inferring':\n",
    "            # use all nodes in the graph \n",
    "            # get input tensor (no need to update)\n",
    "            L_input, S_input, C_input, F_input = dataset.INPUTS\n",
    "            # updata mask for inference node\n",
    "            dataset.make_mask_test(query_indices[0]) # query node equal to inference node\n",
    "            self.maskout_lable(dataset, query_indices)\n",
    "            \n",
    "            masks = dataset.MASKS\n",
    "            \n",
    "            # the query node's indexs in sample_indices\n",
    "            query_indexs = [S-1]\n",
    "            S_ = S # the S used in transformer decoder\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # for S and C, we use two different embedding methods, for CAT and NUM, respectively\n",
    "        # Squeeze for making batch dimantion\n",
    "        L_embedded = self.Lable_embedding(L_input.long()).squeeze(1).unsqueeze(0).float()\n",
    "        \n",
    "        S_embedded = S_input.unsqueeze(0).float()\n",
    "\n",
    "        # for every numrical filed, use it's own Linear embedding layer\n",
    "        C_embedded_nums = []\n",
    "        field = dataset.nodes_of_fields\n",
    "        start = 0\n",
    "        for index, nodes in enumerate(field[:num_NUM]): # pick numrical fields\n",
    "            end = start + nodes\n",
    "            C_embedded_nums.append(self.Catagory_embedding_nums[index](C_input[start:end].float()).unsqueeze(0))\n",
    "            start = end\n",
    "        C_embedded_num = torch.cat(C_embedded_nums, dim = 1)\n",
    "        \n",
    "        catagorical_filed_nodes = sum(field[-num_CAT:]) # pick catagory fields\n",
    "        C_embedded_cat = self.Catagory_embedding_cat(C_input[-catagorical_filed_nodes:].squeeze(1).long() - sum(field[:num_NUM])).unsqueeze(0).float() # - sum(field[:num_NUM] because the embedding index should start from 0\n",
    "        C_embedded = torch.cat([C_embedded_num, C_embedded_cat], dim = 1)\n",
    "        \n",
    "        F_embedded = self.Field_embedding(F_input.long()).squeeze(1).unsqueeze(0).float()\n",
    "        \n",
    "        # print(L_embedded.shape, S_embedded.shape, C_embedded.shape, F_embedded.shape)\n",
    "        \n",
    "        \n",
    "        # propagate steps: L→S→C→F\n",
    "        #                  L←S←C←\n",
    "        # more steps more menory usage\n",
    "        PROPAGATE_STEPS = 1\n",
    "        for i in range(PROPAGATE_STEPS):\n",
    "            S_embedded = self.transformer_decoder(S_embedded,L_embedded, \n",
    "                                                memory_mask = self.tmpmask_L2S[:S_,:L]) \n",
    "            C_embedded = self.transformer_decoder(C_embedded,S_embedded,\n",
    "                                                memory_mask = masks['S2C'][:C,:S_])\n",
    "            F_embedded = self.transformer_decoder(F_embedded,C_embedded,\n",
    "                                                memory_mask = masks['C2F'][:F,:C])\n",
    "            C_embedded = self.transformer_decoder(C_embedded,F_embedded,\n",
    "                                                memory_mask = Tensor.contiguous(masks['C2F'].transpose(0, 1))[:C,:F])\n",
    "            S_embedded = self.transformer_decoder(S_embedded,C_embedded,\n",
    "                                                memory_mask = Tensor.contiguous(masks['S2C'].transpose(0, 1))[:S_,:C])\n",
    "            L_embedded = self.transformer_decoder(L_embedded,S_embedded, \n",
    "                                                memory_mask = Tensor.contiguous(self.tmpmask_L2S.transpose(0, 1))[:L,:S_])\n",
    "        \n",
    "        # print('after',S_embedded[0][0])\n",
    "        output = self.MLP(S_embedded)\n",
    "        # print(query_indexs)\n",
    "        # print(output.shape)\n",
    "        # print(output[:,query_indexs].shape)\n",
    "        # print(output[:,query_indexs][0].shape)\n",
    "        \n",
    "        return output[:,query_indexs][0]\n",
    "  \n",
    "\n",
    "# 測試模型\n",
    "num_layers = 1  # TransformerDecoder 的層數\n",
    "embedding_dim = 128  # 嵌入維度\n",
    "hidden_dim = 64  \n",
    "\n",
    "print('input_dims', Train_data.INPUT_DIMS)\n",
    "model = TransformerDecoderModel(Train_data, num_layers, embedding_dim).to(DEVICE)\n",
    "\n",
    "\n",
    "outputs = model(Train_data, mode = 'train', query_indices = [2000,9999], K = 50)\n",
    "# outputs = model(Train_data, mode = 'inferring', query_indices = [10], K = 50)\n",
    "print(\"模型輸出的大小[q,2]:\", outputs.shape)\n",
    "print(outputs)\n",
    "print(outputs.softmax(dim=1))\n",
    "output_labels = torch.argmax(outputs.softmax(dim=1), dim=1)\n",
    "output_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_input torch.cuda.LongTensor torch.Size([3, 1])\n",
      "S_input torch.cuda.FloatTensor torch.Size([39074, 128])\n",
      "C_input torch.cuda.LongTensor torch.Size([471, 1])\n",
      "F_input torch.cuda.LongTensor torch.Size([14, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:50<00:00, 229.13it/s]\n",
      "100%|██████████| 9769/9769 [02:44<00:00, 59.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1/200 | Loss: 0.22636116182994367 | AUC: 0.7314444186756823 | AUC_test: 0.8378634021438341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:51<00:00, 227.90it/s]\n",
      "100%|██████████| 9769/9769 [02:45<00:00, 59.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch2/200 | Loss: 0.19019759020266003 | AUC: 0.8219623874044409 | AUC_test: 0.8455196684736428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:50<00:00, 228.89it/s]\n",
      "100%|██████████| 9769/9769 [02:45<00:00, 59.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch3/200 | Loss: 0.18557253408115468 | AUC: 0.8330594141598903 | AUC_test: 0.848727524503233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:50<00:00, 229.58it/s]\n",
      "100%|██████████| 9769/9769 [02:45<00:00, 59.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch4/200 | Loss: 0.18233816709890602 | AUC: 0.839945611543096 | AUC_test: 0.8532031132851372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:50<00:00, 229.44it/s]\n",
      "100%|██████████| 9769/9769 [02:45<00:00, 59.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch5/200 | Loss: 0.17871759898270012 | AUC: 0.8472226877088529 | AUC_test: 0.8585864406356073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:50<00:00, 228.64it/s]\n",
      "100%|██████████| 9769/9769 [02:45<00:00, 59.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch6/200 | Loss: 0.1745424237225589 | AUC: 0.8554495064475646 | AUC_test: 0.8615468503632886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:50<00:00, 228.87it/s]\n",
      "100%|██████████| 9769/9769 [02:45<00:00, 59.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch7/200 | Loss: 0.17312456983279656 | AUC: 0.8581637316094718 | AUC_test: 0.8631752588913678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:50<00:00, 229.47it/s]\n",
      "100%|██████████| 9769/9769 [02:45<00:00, 59.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch8/200 | Loss: 0.1726178600518902 | AUC: 0.8591787765104867 | AUC_test: 0.8641469535108804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:50<00:00, 228.99it/s]\n",
      "100%|██████████| 9769/9769 [02:45<00:00, 59.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch9/200 | Loss: 0.17215604467512344 | AUC: 0.8600878330845703 | AUC_test: 0.8649920140135319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:50<00:00, 229.40it/s]\n",
      "100%|██████████| 9769/9769 [02:45<00:00, 59.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch10/200 | Loss: 0.17174329805298294 | AUC: 0.8609421090736458 | AUC_test: 0.8653823958413799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:50<00:00, 228.74it/s]\n",
      "100%|██████████| 9769/9769 [02:45<00:00, 59.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch11/200 | Loss: 0.1714535840932466 | AUC: 0.8613882197857081 | AUC_test: 0.8658052100893794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:50<00:00, 229.20it/s]\n",
      "100%|██████████| 9769/9769 [02:45<00:00, 59.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch12/200 | Loss: 0.17118246019745514 | AUC: 0.8618929998670158 | AUC_test: 0.8661694358235676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:50<00:00, 229.21it/s]\n",
      "100%|██████████| 9769/9769 [02:45<00:00, 59.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch13/200 | Loss: 0.17094913248991925 | AUC: 0.8623370018975606 | AUC_test: 0.8665103170311669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:50<00:00, 228.86it/s]\n",
      "100%|██████████| 9769/9769 [02:45<00:00, 59.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch14/200 | Loss: 0.17046424710855693 | AUC: 0.8631123666191238 | AUC_test: 0.8670334957036984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:50<00:00, 229.12it/s]\n",
      "100%|██████████| 9769/9769 [02:45<00:00, 59.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch15/200 | Loss: 0.17027443999527989 | AUC: 0.8634636065220508 | AUC_test: 0.8676177336611034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39073/39073 [02:50<00:00, 228.67it/s]\n",
      " 17%|█▋        | 1650/9769 [00:27<02:16, 59.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1722/9769 [00:29<02:20, 57.13it/s]"
     ]
    }
   ],
   "source": [
    "# training\n",
    "from torch import autograd\n",
    "from torcheval.metrics.aggregation.auc import AUC\n",
    "from torcheval.metrics import BinaryAUROC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "tmp_log = []\n",
    "tmp__log = []\n",
    "def train(model, datset):\n",
    "    LABEL_POOL = datset.LABEL_POOL\n",
    "    TEST_LABEL_POOL = datset.TEST_LABEL_POOL\n",
    "    weight = torch.from_numpy(np.array([0.2, 1])).float().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    epochs = 200\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        # logs\n",
    "        loss_log = []\n",
    "        AUC_metric = BinaryAUROC().to(DEVICE)\n",
    "        AUC_metric_test = BinaryAUROC().to(DEVICE)\n",
    "        \n",
    "        iter = 0\n",
    "        for index in trange(len(datset.FEATURE_POOL)): # query through all sample nodes (not infering node)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(datset, mode = 'train', query_indices = [index], K = 100)\n",
    "            # output shape:[q,2], example: torch.Size( 2, 2]\n",
    "            # tensor([[-0.6845, -0.6323],\n",
    "            #          [-0.7770, -0.4703]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
    "                \n",
    "            # for trainning, only the query node's output is used\n",
    "            # caculate loss\n",
    "            LABEL_POOL_ = LABEL_POOL[[index]] # shape:[q,2] ,example [[1. 0.], [1. 0.]]\n",
    "                        \n",
    "            # caculate loss\n",
    "            batch_loss = criterion(outputs, torch.tensor(LABEL_POOL_,device=DEVICE))\n",
    "            loss_log.append(batch_loss.item())\n",
    "            # backpropagation\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            TRUE = np.argmax(LABEL_POOL_,axis=1)\n",
    "            \n",
    "            outputs = outputs.softmax(dim=1)\n",
    "\n",
    "            pred_prob_of_is_1 = [probs[1] for probs in outputs] \n",
    "            # the probability of the query node is 1 (from model output)\n",
    "            \n",
    "            # tmp_log.append(float(pred_prob_of_is_1))\n",
    "            # tmp__log.append((TRUE))\n",
    "            AUC_metric.update(torch.Tensor(pred_prob_of_is_1),torch.Tensor(TRUE))\n",
    "            torch.cuda.empty_cache()\n",
    "            # break\n",
    "            iter += 1\n",
    "            # if iter >= 100:\n",
    "            #     break\n",
    "        # break\n",
    "        # evaluate\n",
    "        model.eval()\n",
    "        iter = 0\n",
    "        with torch.no_grad():\n",
    "            for index in trange(len(datset.TEST_POOL)):\n",
    "                outputs = model(datset, mode = 'inferring', query_indices = [index], K = None)\n",
    "                LABEL_POOL_ = TEST_LABEL_POOL[[index]]\n",
    "                batch_loss = criterion(outputs, torch.tensor(LABEL_POOL_,device=DEVICE))\n",
    "                # print(LABEL_POOL_)\n",
    "                TRUE = np.argmax(LABEL_POOL_,axis=1)\n",
    "                outputs = outputs.softmax(dim=1)\n",
    "                pred_prob_of_is_1 = [probs[1] for probs in outputs] \n",
    "                AUC_metric_test.update(torch.Tensor(pred_prob_of_is_1),torch.Tensor(TRUE))\n",
    "                torch.cuda.empty_cache()\n",
    "                iter += 1\n",
    "                # if iter >= 100:\n",
    "                #     break\n",
    "        \n",
    "        epoch_loss = sum(loss_log) / len(loss_log)\n",
    "        epoch_AUC = float(AUC_metric.compute()) \n",
    "        epoch_AUC_test = float(AUC_metric_test.compute()) \n",
    "\n",
    "        AUC_metric.reset()\n",
    "        AUC_metric_test.reset()\n",
    "        # break\n",
    "        del loss_log, AUC_metric\n",
    "        tmp_log.append(float(epoch_loss))\n",
    "        tmp__log.append(float(epoch_AUC))\n",
    "        \n",
    "        # print(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC: {epoch_AUC} |\")\n",
    "        print(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC: {epoch_AUC} | AUC_test: {epoch_AUC_test}\")\n",
    "        \n",
    "        \n",
    "        with open('logs/0724_log_1.txt', 'a') as f:\n",
    "            # f.write(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC: {epoch_AUC}| \")\n",
    "            f.write(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC: {epoch_AUC}| AUC_test: {epoch_AUC_test}\\n \")\n",
    "\n",
    "model = TransformerDecoderModel(Train_data, num_layers, embedding_dim).to(DEVICE)\n",
    "train(model, Train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(tmp_log)\n",
    "plt.plot(tmp__log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "print(input.shape, target.shape)\n",
    "print(input)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "print(output.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
