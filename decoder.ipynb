{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsou/.conda/envs/GNN/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# baic transformer Decoder model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "import numpy as np\n",
    "\n",
    "main_df = pd.read_csv('adult.csv')\n",
    "main_df.head()\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>78</td>\n",
       "      <td>97</td>\n",
       "      <td>164</td>\n",
       "      <td>185</td>\n",
       "      <td>199</td>\n",
       "      <td>209</td>\n",
       "      <td>220</td>\n",
       "      <td>225</td>\n",
       "      <td>229</td>\n",
       "      <td>230</td>\n",
       "      <td>253</td>\n",
       "      <td>341</td>\n",
       "      <td>437</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>78</td>\n",
       "      <td>88</td>\n",
       "      <td>174</td>\n",
       "      <td>187</td>\n",
       "      <td>197</td>\n",
       "      <td>207</td>\n",
       "      <td>217</td>\n",
       "      <td>227</td>\n",
       "      <td>229</td>\n",
       "      <td>230</td>\n",
       "      <td>253</td>\n",
       "      <td>351</td>\n",
       "      <td>437</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>76</td>\n",
       "      <td>104</td>\n",
       "      <td>170</td>\n",
       "      <td>190</td>\n",
       "      <td>197</td>\n",
       "      <td>213</td>\n",
       "      <td>217</td>\n",
       "      <td>227</td>\n",
       "      <td>229</td>\n",
       "      <td>230</td>\n",
       "      <td>253</td>\n",
       "      <td>341</td>\n",
       "      <td>437</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>78</td>\n",
       "      <td>93</td>\n",
       "      <td>178</td>\n",
       "      <td>188</td>\n",
       "      <td>197</td>\n",
       "      <td>209</td>\n",
       "      <td>217</td>\n",
       "      <td>225</td>\n",
       "      <td>229</td>\n",
       "      <td>237</td>\n",
       "      <td>253</td>\n",
       "      <td>341</td>\n",
       "      <td>437</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>89</td>\n",
       "      <td>178</td>\n",
       "      <td>188</td>\n",
       "      <td>199</td>\n",
       "      <td>202</td>\n",
       "      <td>220</td>\n",
       "      <td>227</td>\n",
       "      <td>228</td>\n",
       "      <td>230</td>\n",
       "      <td>253</td>\n",
       "      <td>331</td>\n",
       "      <td>437</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt  education  educational-num  marital-status  \\\n",
       "0    8         78      97        164              185             199   \n",
       "1   21         78      88        174              187             197   \n",
       "2   11         76     104        170              190             197   \n",
       "3   27         78      93        178              188             197   \n",
       "4    1         74      89        178              188             199   \n",
       "\n",
       "   occupation  relationship  race  gender  capital-gain  capital-loss  \\\n",
       "0         209           220   225     229           230           253   \n",
       "1         207           217   227     229           230           253   \n",
       "2         213           217   227     229           230           253   \n",
       "3         209           217   225     229           237           253   \n",
       "4         202           220   227     228           230           253   \n",
       "\n",
       "   hours-per-week  native-country  income  \n",
       "0             341             437     440  \n",
       "1             351             437     440  \n",
       "2             341             437     441  \n",
       "3             341             437     441  \n",
       "4             331             437     440  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def POOL_preprocess(df):\n",
    "    '''\n",
    "    input the original dataframe, output the dataframe after preprocessing,\n",
    "    change the numerical columns to categorical columns by qcut and cut\n",
    "    then apply label encoding to all columns\n",
    "    \n",
    "    '''\n",
    "    df = df.copy()\n",
    "    CAT = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country', 'income']\n",
    "    NUM = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "    # qcut on numerical columns\n",
    "    for column in NUM:\n",
    "        if column in ['educational-num','capital-gain','capital-loss','hours-per-week']:\n",
    "            df[column] = pd.cut(df[column], 100)\n",
    "        else:\n",
    "            df[column] = pd.cut(df[column], 100)\n",
    "    # make income column binary\n",
    "    df['income'] = df['income'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "\n",
    "    # lable encoding categorical columns\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    lb = LabelEncoder()\n",
    "    df = df.apply(lambda x: lb.fit_transform(x))\n",
    "\n",
    "    # make all catagory in every column unique\n",
    "\n",
    "    # 迴圈處理多個欄位\n",
    "    offset = 0\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].apply(lambda x: x + offset)\n",
    "        offset += df[column].nunique()\n",
    "    \n",
    "    return df\n",
    "tmp = POOL_preprocess(main_df)\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "      <td>48842.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>21.643585</td>\n",
       "      <td>77.870439</td>\n",
       "      <td>94.497113</td>\n",
       "      <td>173.288420</td>\n",
       "      <td>188.078089</td>\n",
       "      <td>197.618750</td>\n",
       "      <td>208.577700</td>\n",
       "      <td>218.443287</td>\n",
       "      <td>226.668052</td>\n",
       "      <td>228.668482</td>\n",
       "      <td>230.626858</td>\n",
       "      <td>254.102596</td>\n",
       "      <td>341.396728</td>\n",
       "      <td>434.749355</td>\n",
       "      <td>440.239282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.710510</td>\n",
       "      <td>1.464234</td>\n",
       "      <td>7.118204</td>\n",
       "      <td>3.874492</td>\n",
       "      <td>2.570973</td>\n",
       "      <td>1.507703</td>\n",
       "      <td>4.230509</td>\n",
       "      <td>1.602151</td>\n",
       "      <td>0.845986</td>\n",
       "      <td>0.470764</td>\n",
       "      <td>2.670115</td>\n",
       "      <td>5.196314</td>\n",
       "      <td>12.295271</td>\n",
       "      <td>7.775343</td>\n",
       "      <td>0.426649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>440.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>440.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>209.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>229.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>440.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>229.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>440.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>73.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>229.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>439.000000</td>\n",
       "      <td>441.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age     workclass        fnlwgt     education  \\\n",
       "count  48842.000000  48842.000000  48842.000000  48842.000000   \n",
       "mean      21.643585     77.870439     94.497113    173.288420   \n",
       "std       13.710510      1.464234      7.118204      3.874492   \n",
       "min        0.000000     74.000000     83.000000    163.000000   \n",
       "25%       11.000000     78.000000     90.000000    172.000000   \n",
       "50%       20.000000     78.000000     94.000000    174.000000   \n",
       "75%       31.000000     78.000000     98.000000    175.000000   \n",
       "max       73.000000     82.000000    162.000000    178.000000   \n",
       "\n",
       "       educational-num  marital-status    occupation  relationship  \\\n",
       "count     48842.000000    48842.000000  48842.000000  48842.000000   \n",
       "mean        188.078089      197.618750    208.577700    218.443287   \n",
       "std           2.570973        1.507703      4.230509      1.602151   \n",
       "min         179.000000      195.000000    202.000000    217.000000   \n",
       "25%         187.000000      197.000000    205.000000    217.000000   \n",
       "50%         188.000000      197.000000    209.000000    218.000000   \n",
       "75%         190.000000      199.000000    212.000000    220.000000   \n",
       "max         194.000000      201.000000    216.000000    222.000000   \n",
       "\n",
       "               race        gender  capital-gain  capital-loss  hours-per-week  \\\n",
       "count  48842.000000  48842.000000  48842.000000  48842.000000    48842.000000   \n",
       "mean     226.668052    228.668482    230.626858    254.102596      341.396728   \n",
       "std        0.845986      0.470764      2.670115      5.196314       12.295271   \n",
       "min      223.000000    228.000000    230.000000    253.000000      302.000000   \n",
       "25%      227.000000    228.000000    230.000000    253.000000      341.000000   \n",
       "50%      227.000000    229.000000    230.000000    253.000000      341.000000   \n",
       "75%      227.000000    229.000000    230.000000    253.000000      346.000000   \n",
       "max      227.000000    229.000000    252.000000    301.000000      397.000000   \n",
       "\n",
       "       native-country        income  \n",
       "count    48842.000000  48842.000000  \n",
       "mean       434.749355    440.239282  \n",
       "std          7.775343      0.426649  \n",
       "min        398.000000    440.000000  \n",
       "25%        437.000000    440.000000  \n",
       "50%        437.000000    440.000000  \n",
       "75%        437.000000    440.000000  \n",
       "max        439.000000    441.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data num: 48842\n",
      "trian data num: 39074\n",
      "test data num: 9768\n"
     ]
    }
   ],
   "source": [
    "train_size = 4*48842//5\n",
    "test_size = 48842//5\n",
    "train_pool = main_df[test_size:]\n",
    "test_pool = main_df[:test_size]\n",
    "print('total data num:' , main_df.shape[0])\n",
    "print('trian data num:' , train_pool.shape[0])\n",
    "print('test data num:' , test_pool.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notations\n",
    "#   node: number of all nodes = L + S + C + F\n",
    "#   L: number of lable nodes\n",
    "#   S: number of sample nodes\n",
    "#   C: number of catagory nodes\n",
    "#   F: number of field(column) nodes\n",
    "#   hidden: number of hidden representation\n",
    "\n",
    "# data size = (node, hidden)\n",
    "# mask size = (node, node - L) without lable nodes\n",
    "#             for each node, real mask = cat[mask,(node,L)] = (node, node)\n",
    "#             cannot see it's label node\n",
    "\n",
    "# use nn.transformerDecoder(data,mask) to get the output\n",
    "# use the above output as input of MLP to predict the lable   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature pool shape: (48842, 14)\n",
      "label pool shape: (48842, 2)\n",
      "L: 2\n",
      "S: 48842\n",
      "C: 440\n",
      "F: 14\n",
      "total 49298\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439]\n",
      "440\n"
     ]
    }
   ],
   "source": [
    "TARGET_POOL = POOL_preprocess(main_df)\n",
    "LABEL_COLUMN = 'income'\n",
    "# \n",
    "HIDDEN = 64\n",
    "\n",
    "# cut feature and lable\n",
    "FEATURE_POOL = TARGET_POOL.drop(LABEL_COLUMN, axis=1)\n",
    "LABEL_POOL = TARGET_POOL[LABEL_COLUMN]\n",
    "\n",
    "# trasform label into one-hot\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "LABEL_POOL = enc.fit_transform(LABEL_POOL.values.reshape(-1,1)).toarray()\n",
    "\n",
    "print('feature pool shape:', FEATURE_POOL.shape)\n",
    "print('label pool shape:', LABEL_POOL.shape)\n",
    "\n",
    "# L: number of lable nodes\n",
    "L = LABEL_POOL.shape[1]\n",
    "\n",
    "# S: number of sample nodes\n",
    "S = FEATURE_POOL.shape[0]\n",
    "\n",
    "# C: number of catagory nodes\n",
    "C = FEATURE_POOL.apply(lambda x: x.nunique()).sum() # total_unique_labels\n",
    "C_POOL = list(set(FEATURE_POOL.values.flatten()))\n",
    "\n",
    "# F: number of field(column) nodes\n",
    "F = FEATURE_POOL.shape[1]\n",
    "\n",
    "print('L:', L)\n",
    "print('S:', S)\n",
    "print('C:', C)\n",
    "print('F:', F)\n",
    "print('total', L+S+C+F)\n",
    "print(C_POOL)\n",
    "print(len(C_POOL))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL_POOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caculate masking\n",
    "masks = {}\n",
    "\n",
    "# label to sample \n",
    "edge_x_L2S = []\n",
    "edge_y_L2S = []\n",
    "label_ids = TARGET_POOL[LABEL_COLUMN].unique()\n",
    "for i, value_df in enumerate(TARGET_POOL[LABEL_COLUMN]):\n",
    "    for j, value_label in enumerate(label_ids):\n",
    "        if value_label == value_df:\n",
    "            edge_x_L2S.append(i)\n",
    "            edge_y_L2S.append(j)\n",
    "# make value on edge as 1\n",
    "value_on_edge = [1]*len(edge_x_L2S)\n",
    "\n",
    "indices = torch.tensor([edge_x_L2S, edge_y_L2S])\n",
    "values = torch.tensor(value_on_edge)\n",
    "size = torch.Size([S, L])\n",
    "masks['L2S'] = torch.sparse_coo_tensor(indices, values, size)\n",
    "\n",
    "# sample to catagory\n",
    "edge_x_S2C = []\n",
    "edge_y_S2C = []\n",
    "tmp_df = TARGET_POOL.drop(LABEL_COLUMN, axis=1)\n",
    "for i, value_df in enumerate(tmp_df.values):\n",
    "    for j, value in enumerate(value_df):\n",
    "        edge_x_S2C.append(value)\n",
    "        edge_y_S2C.append(i)\n",
    "# make value on edge as 1\n",
    "value_on_edge = [1]*len(edge_x_S2C)\n",
    "indices = torch.tensor([edge_x_S2C, edge_y_S2C])\n",
    "values = torch.tensor(value_on_edge)\n",
    "size = torch.Size([C, S])\n",
    "masks['S2C'] = torch.sparse_coo_tensor(indices, values, size)\n",
    "\n",
    "\n",
    "# catagory to field\n",
    "edge_x_C2F = []\n",
    "edge_y_C2F = []\n",
    "unique_items = [(TARGET_POOL[column].unique()) for column in (TARGET_POOL.columns)]\n",
    "for i in range(F):\n",
    "    for j in (unique_items[i]):\n",
    "        edge_x_C2F.append(i)\n",
    "        edge_y_C2F.append(j)\n",
    "# make value on edge as 1\n",
    "value_on_edge = [1]*len(edge_x_C2F)\n",
    "indices = torch.tensor([edge_x_C2F, edge_y_C2F])\n",
    "values = torch.tensor(value_on_edge)\n",
    "size = torch.Size([F, C])\n",
    "masks['C2F'] = torch.sparse_coo_tensor(indices, values, size)\n",
    "\n",
    "# to-do: unseen data issue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_input torch.cuda.FloatTensor torch.Size([2, 2])\n",
      "S_input torch.cuda.LongTensor torch.Size([48842, 14])\n",
      "C_input torch.cuda.LongTensor torch.Size([440, 440])\n",
      "F_input torch.cuda.FloatTensor torch.Size([14, 14])\n",
      "torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# make input tensor\n",
    "# L\n",
    "L_input = torch.eye(L).to(DEVICE)\n",
    "print('L_input', L_input.type(), L_input.shape)\n",
    "# S\n",
    "S_input = torch.tensor(FEATURE_POOL.values).to(DEVICE)\n",
    "print('S_input', S_input.type(), S_input.shape)\n",
    "# C random init\n",
    "C_input = torch.tensor(np.diag(C_POOL)).to(DEVICE)\n",
    "print('C_input', C_input.type(), C_input.shape)\n",
    "# F random init\n",
    "F_input = torch.eye(F).to(DEVICE)\n",
    "print('F_input', F_input.type(), F_input.shape)\n",
    "print(L_input.type())\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Optional, Any, Union, Callable\n",
    "\n",
    "class CustomTransformerDecoderLayer(nn.TransformerDecoderLayer):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu'):\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        # remove defined modules\n",
    "        delattr(self, 'self_attn')\n",
    "        delattr(self, 'norm1')\n",
    "        delattr(self, 'dropout1')\n",
    "    \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        x = tgt\n",
    "        if self.norm_first:\n",
    "            # x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)\n",
    "            x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)\n",
    "            x = x + self._ff_block(self.norm3(x))\n",
    "        else:\n",
    "            # x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))\n",
    "            x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))\n",
    "            x = self.norm3(x + self._ff_block(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes_num (2, 14, 440, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型輸出的大小: torch.Size([48842, 2])\n"
     ]
    }
   ],
   "source": [
    "# baic transformer decoder model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "\n",
    "class TransformerDecoderModel(nn.Module):\n",
    "    def __init__(self, nodes_num, num_layers, embedding_dim, hidden_dim):\n",
    "        super(TransformerDecoderModel, self).__init__()\n",
    "\n",
    "        L_dim, S_dim, C_dim, F_dim = nodes_num\n",
    "        \n",
    "        # 目前b卡在embedding的怎麼用\n",
    "        # Catagory_embedding => 數值類Qcut後用linear來做embedding, 類別用nn.Embedding\n",
    "\n",
    "        self.Lable_embedding = nn.Linear(L_dim, embedding_dim, dtype=torch.float)\n",
    "        self.Sample_embedding = nn.Linear(S_dim, embedding_dim, dtype=torch.float)\n",
    "        self.Catagory_embedding = nn.Linear(C_dim, embedding_dim, dtype=torch.float)\n",
    "        self.Field_embedding = nn.Linear(F_dim, embedding_dim, dtype=torch.float)\n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            CustomTransformerDecoderLayer(embedding_dim,  nhead = 8 ),\n",
    "            num_layers\n",
    "        )\n",
    "        \n",
    "        # downstream task\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim//2, embedding_dim//3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim//3, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, L_input, S_input, C_input, F_input, masks):\n",
    "        L_embedded = self.Lable_embedding(L_input.float())\n",
    "        S_embedded = self.Sample_embedding(S_input.float())\n",
    "        C_embedded = self.Catagory_embedding(C_input.float())\n",
    "        F_embedded = self.Field_embedding(F_input.float())\n",
    "        \n",
    "        for mask in masks.keys():\n",
    "            masks[mask] = masks[mask].float().to_dense().to(DEVICE)\n",
    "        \n",
    "        # propagate steps: L→S→C→F→C→S→L\n",
    "        # more steps more menory usage\n",
    "        PROPAGATE_STEPS = 2\n",
    "        for i in range(PROPAGATE_STEPS):\n",
    "            S_embedded = self.transformer_decoder(S_embedded,L_embedded, \n",
    "                                                memory_mask = masks['L2S']) \n",
    "            C_embedded = self.transformer_decoder(C_embedded,S_embedded,\n",
    "                                                memory_mask = masks['S2C'])\n",
    "            F_embedded = self.transformer_decoder(F_embedded,C_embedded,\n",
    "                                                memory_mask = masks['C2F'])\n",
    "            C_embedded = self.transformer_decoder(C_embedded,F_embedded,\n",
    "                                                memory_mask = masks['C2F'].transpose(0, 1))\n",
    "            S_embedded = self.transformer_decoder(S_embedded,C_embedded,\n",
    "                                                memory_mask = masks['S2C'].transpose(0, 1))\n",
    "            L_embedded = self.transformer_decoder(L_embedded,S_embedded, \n",
    "                                                memory_mask = masks['L2S'].transpose(0, 1))\n",
    "        \n",
    "        # S_embedded = self.transformer_decoder(S_embedded,L_embedded, memory_key_padding_mask   = masks['L2S'].float().to(DEVICE))  # 使用 TransformerDecoder 編碼\n",
    "        output = self.MLP(S_embedded)\n",
    "        return output\n",
    "        # return encoded.permute(1, 0, 2)  # 改變 tensor 的維度順序回來\n",
    "\n",
    "# 測試模型\n",
    "num_layers = 1  # TransformerDecoder 的層數\n",
    "embedding_dim = 128  # 嵌入維度\n",
    "hidden_dim = 64  # TransformerDecoderLayer 的隱藏層維度\n",
    "\n",
    "nodes_num = (L_input.size(1), S_input.size(1), C_input.size(1), F_input.size(1))\n",
    "print('nodes_num', nodes_num)\n",
    "model = TransformerDecoderModel(nodes_num, num_layers, embedding_dim, hidden_dim).to(DEVICE)\n",
    "output = model(L_input, S_input, C_input, F_input, masks)\n",
    "\n",
    "print(\"模型輸出的大小:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(48839, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_label = torch.argmax(output, dim=1)\n",
    "output_label.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23926129151140413"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caculate accuracy\n",
    "acc = torch.sum(output_label == torch.argmax(torch.tensor(LABEL_POOL).to(DEVICE), dim=1)).item() / len(output_label)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
