{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# baic transformer Decoder model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import xformers.ops as xops\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "from torch import Tensor\n",
    "import random\n",
    "\n",
    "main_df = pd.read_csv('adult.csv')\n",
    "main_df.head()\n",
    "DEVICE = 'cuda'\n",
    "# DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsou/.conda/envs/xformers/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:313: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>131</td>\n",
       "      <td>188</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>334</td>\n",
       "      <td>367</td>\n",
       "      <td>383</td>\n",
       "      <td>392</td>\n",
       "      <td>400</td>\n",
       "      <td>413</td>\n",
       "      <td>421</td>\n",
       "      <td>425</td>\n",
       "      <td>467</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>116</td>\n",
       "      <td>178</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>279</td>\n",
       "      <td>365</td>\n",
       "      <td>376</td>\n",
       "      <td>390</td>\n",
       "      <td>408</td>\n",
       "      <td>412</td>\n",
       "      <td>423</td>\n",
       "      <td>426</td>\n",
       "      <td>467</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>168</td>\n",
       "      <td>189</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>304</td>\n",
       "      <td>365</td>\n",
       "      <td>381</td>\n",
       "      <td>390</td>\n",
       "      <td>409</td>\n",
       "      <td>412</td>\n",
       "      <td>420</td>\n",
       "      <td>426</td>\n",
       "      <td>452</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>136</td>\n",
       "      <td>180</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>334</td>\n",
       "      <td>365</td>\n",
       "      <td>371</td>\n",
       "      <td>390</td>\n",
       "      <td>410</td>\n",
       "      <td>412</td>\n",
       "      <td>423</td>\n",
       "      <td>426</td>\n",
       "      <td>467</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>139</td>\n",
       "      <td>187</td>\n",
       "      <td>191</td>\n",
       "      <td>215</td>\n",
       "      <td>304</td>\n",
       "      <td>367</td>\n",
       "      <td>380</td>\n",
       "      <td>390</td>\n",
       "      <td>409</td>\n",
       "      <td>412</td>\n",
       "      <td>423</td>\n",
       "      <td>426</td>\n",
       "      <td>467</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  educational-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0   16     131              188           191           215             334   \n",
       "1   62     116              178           191           215             279   \n",
       "2   16     168              189           191           215             304   \n",
       "3   19     136              180           191           215             334   \n",
       "4   29     139              187           191           215             304   \n",
       "\n",
       "   workclass  education  marital-status  occupation  relationship  race  \\\n",
       "0        367        383             392         400           413   421   \n",
       "1        365        376             390         408           412   423   \n",
       "2        365        381             390         409           412   420   \n",
       "3        365        371             390         410           412   423   \n",
       "4        367        380             390         409           412   423   \n",
       "\n",
       "   gender  native-country  income  \n",
       "0     425             467     472  \n",
       "1     426             467     471  \n",
       "2     426             452     471  \n",
       "3     426             467     471  \n",
       "4     426             467     472  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "def POOL_preprocess(df, N_BINS = ):\n",
    "    \n",
    "    CAT = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country', 'income']\n",
    "    NUM = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "    \n",
    "    num_CAT = len(CAT)\n",
    "    num_NUM = len(NUM)  \n",
    "    \n",
    "    ct = ColumnTransformer([\n",
    "        (\"age\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"age\"]),\n",
    "        (\"fnlwgt\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='quantile', subsample=None), [\"fnlwgt\"]),\n",
    "        (\"educational-num\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='quantile', subsample=None), [\"educational-num\"]),\n",
    "        (\"capital-gain\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"capital-gain\"]),\n",
    "        (\"capital-loss\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"capital-loss\"]),\n",
    "        (\"hours-per-week\", KBinsDiscretizer(n_bins = N_BINS, encode='ordinal', strategy='uniform', subsample=None), [\"hours-per-week\"]),\n",
    "         ],remainder = 'passthrough', verbose_feature_names_out = False) # make sure columns are unique\n",
    "    ct.set_output(transform = 'pandas')\n",
    "    X_trans = ct.fit_transform(df) \n",
    "    \n",
    "    # make catagoy in NUM columns unique\n",
    "    # each NUM column has N_BINS unique values, that is, each NUM column represents as N_BINS node\n",
    "    # 7/19: each NUM column has its own number of unique values, plus 1 for unseen values\n",
    "    offset = 0\n",
    "    # for column in NUM:\n",
    "    #     X_trans[column] = X_trans[column].apply(lambda x: x + offset)\n",
    "    #     # offset += N_BINS\n",
    "    #     offset += X_trans[column].nunique() + 1\n",
    "    \n",
    "    # apply lable encoding on CAT columns\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    lb = LabelEncoder()\n",
    "    X_trans[NUM] = X_trans[NUM].apply(lambda x: lb.fit_transform(x))\n",
    "    X_trans[CAT] = X_trans[CAT].apply(lambda x: lb.fit_transform(x))\n",
    "    \n",
    "    # make catagoy all columns unique\n",
    "    # each column has it's own number of unique values. '+1' is for unseen values\n",
    "    # offset = len(NUM) * N_BINS\n",
    "    for column in NUM + CAT:\n",
    "        X_trans[column] = X_trans[column].apply(lambda x: x + offset)\n",
    "        offset += (X_trans[column].max() - X_trans[column].min() + 1) + 1\n",
    "    \n",
    "    X_trans = X_trans.astype(int).reset_index(drop = True)\n",
    "    return X_trans, ct, (num_NUM, num_CAT - 1) # -1 is for the income column (label)\n",
    "X_trans, _, _= POOL_preprocess(main_df[48842//5:])\n",
    "X_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age 0 73\n",
      "fnlwgt 75 174\n",
      "educational-num 176 189\n",
      "capital-gain 191 213\n",
      "capital-loss 215 263\n",
      "hours-per-week 265 359\n",
      "workclass 361 369\n",
      "education 371 386\n",
      "marital-status 388 394\n",
      "occupation 396 410\n",
      "relationship 412 417\n",
      "race 419 423\n",
      "gender 425 426\n",
      "native-country 428 469\n",
      "income 471 472\n"
     ]
    }
   ],
   "source": [
    "for column in X_trans.columns:\n",
    "    print(column, X_trans[column].min(),X_trans[column].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data num: 48842\n",
      "trian data num: 39074\n",
      "test data num: 9768\n"
     ]
    }
   ],
   "source": [
    "train_size = 4*48842//5\n",
    "test_size = 48842//5\n",
    "train_pool = main_df[test_size:]\n",
    "test_pool = main_df[:test_size]\n",
    "print('total data num:' , main_df.shape[0])\n",
    "print('trian data num:' , train_pool.shape[0])\n",
    "print('test data num:' , test_pool.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notations\n",
    "#   node: number of all nodes = L + S + C + F\n",
    "#   L: number of lable nodes\n",
    "#   S: number of sample nodes\n",
    "#   C: number of catagory nodes\n",
    "#   F: number of field(column) nodes\n",
    "#   hidden: number of hidden representation\n",
    "\n",
    "# data size = (node, hidden)\n",
    "# mask size = (node, node - L) without lable nodes\n",
    "#             for each node, real mask = cat[mask,(node,L)] = (node, node)\n",
    "#             cannot see it's label node\n",
    "\n",
    "# use nn.transformerDecoder(data,mask) to get the output\n",
    "# use the above output as input of MLP to predict the lable   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data num: 48842\n",
      "trian data num: 39073\n",
      "test data num: 9769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsou/.conda/envs/xformers/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:313: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_nums {'L': 3, 'S': 39074, 'C': 470, 'F': 14}\n",
      "total 39561 nodes\n",
      "L_input torch.cuda.LongTensor torch.Size([3, 1])\n",
      "S_input torch.cuda.FloatTensor torch.Size([39074, 128])\n",
      "C_input torch.cuda.LongTensor torch.Size([470, 1])\n",
      "F_input torch.cuda.LongTensor torch.Size([14, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5577, 7265, 10782, 16173, 19070, 24081, 24266, 27913, 32180, 33960]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HGNN_DataSet():\n",
    "    def __init__(self,\n",
    "                 data_df : pd.DataFrame,\n",
    "                 split_ratio : float ,\n",
    "                 label_column : str,\n",
    "                 ):\n",
    "        # shuffle data\n",
    "        data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
    "        test_size = math.ceil(data_df.shape[0] * (1-split_ratio))\n",
    "        train_pool = data_df[test_size:]\n",
    "        test_pool = data_df[:test_size]\n",
    "        print('total data num:' , data_df.shape[0])\n",
    "        print('trian data num:' , train_pool.shape[0])\n",
    "        print('test data num:' , test_pool.shape[0])\n",
    "        \n",
    "        # to-dos:\n",
    "        # train\n",
    "        #   \n",
    "        N_BINS = 100\n",
    "        TARGET_POOL, self.CT, self.NUM_vs_CAT = POOL_preprocess(train_pool, N_BINS = N_BINS)\n",
    "        # TEST_POOL = POOL_preprocess(test_pool)\n",
    "        LABEL_COLUMN = label_column\n",
    "\n",
    "        # cut feature and lable\n",
    "        FEATURE_POOL = TARGET_POOL.drop(LABEL_COLUMN, axis=1)\n",
    "        LABEL_POOL = TARGET_POOL[LABEL_COLUMN]\n",
    "        \n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        enc = OneHotEncoder()\n",
    "        LABEL_POOL = enc.fit_transform(LABEL_POOL.values.reshape(-1,1)).toarray()\n",
    "\n",
    "        # L: number of lable nodes\n",
    "        # the last node of Lable nodes is served as unknown lable node\n",
    "        L = LABEL_POOL.shape[1] + 1\n",
    "\n",
    "        # S: number of sample nodes\n",
    "        S = FEATURE_POOL.shape[0] + 1\n",
    "        # the last node of sample nodes is served as infering node\n",
    "        \n",
    "        # F: number of field(column) nodes\n",
    "        F = FEATURE_POOL.shape[1]\n",
    "\n",
    "        # C: number of catagory nodes\n",
    "        # first, caculate nodes of each field, including unseen node\n",
    "        self.nodes_of_fields = []\n",
    "        for column in FEATURE_POOL.columns:\n",
    "            self.nodes_of_fields.append(FEATURE_POOL[column].nunique()+1)\n",
    "        C = sum(self.nodes_of_fields) # the total number of nodes equals to the sum of nodes of each field\n",
    "        C_POOL = range(int(C))\n",
    "\n",
    "        nodes_num = {'L':L, 'S':S, 'C':C, 'F':F}\n",
    "        print('node_nums', nodes_num)\n",
    "        print('total', L+S+C+F, 'nodes')\n",
    "        \n",
    "        # get samples index for each label\n",
    "        self.labe_to_index = {}\n",
    "        tmp_pool = TARGET_POOL.copy().reset_index(drop=True)\n",
    "        for label in tmp_pool['income'].unique():\n",
    "            self.labe_to_index[label] = (tmp_pool[tmp_pool['income'] == label].index).tolist()\n",
    "        \n",
    "        self.TARGET_POOL = TARGET_POOL\n",
    "        # self.TEST_POOL = TEST_POOL\n",
    "        self.LABEL_COLUMN = LABEL_COLUMN\n",
    "        self.FEATURE_POOL = FEATURE_POOL\n",
    "        self.LABEL_POOL = LABEL_POOL\n",
    "        self.C_POOL = C_POOL   \n",
    "        self.nodes_num = nodes_num\n",
    "        self.N_BINS = N_BINS\n",
    "\n",
    "        \n",
    "        self.make_input_tensor()\n",
    "        # self.get_sample(10)        \n",
    "        self.make_mask_all()\n",
    "        \n",
    "        # self.make_mask()\n",
    "        \n",
    "        \n",
    "    def make_mask(self,\n",
    "                  sample_indices: Optional[list] = None,\n",
    "                ):\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "\n",
    "        sample_size = len(sample_indices)\n",
    "        masked_POOL = self.TARGET_POOL.iloc[sample_indices]\n",
    "        # caculate masking\n",
    "        masks = {}\n",
    "\n",
    "        # label to sample \n",
    "        tmp = torch.zeros([math.ceil(sample_size/8) * 8, math.ceil(L/8) * 8], dtype=torch.float, device=DEVICE)\n",
    "        label_ids = self.TARGET_POOL[self.LABEL_COLUMN].unique()\n",
    "        for i, value_df in enumerate(masked_POOL[self.LABEL_COLUMN]):\n",
    "            for j, value_label in enumerate(label_ids):\n",
    "                if value_label == value_df:\n",
    "                    tmp[i][j] = 1\n",
    "                    break\n",
    "        masks['L2S'] = Tensor.contiguous(tmp)\n",
    "\n",
    "        # sample to catagory\n",
    "        tmp = torch.zeros([math.ceil(C/8) * 8, math.ceil(sample_size/8) * 8], dtype=torch.float, device=DEVICE).T\n",
    "        tmp_df = masked_POOL.drop(self.LABEL_COLUMN, axis=1)\n",
    "        tmp[torch.arange(sample_size).unsqueeze(-1), torch.tensor(tmp_df.values)] = 1\n",
    "        tmp = tmp.T.contiguous()\n",
    "        \n",
    "        # tmp = (S, C)\n",
    "        # df = (S, col) \n",
    "        #\n",
    "        # [\n",
    "        #   [10, 20, 25, ...]\n",
    "        #   [10, 20, 25, ...]\n",
    "        # ]\n",
    "        \n",
    "        # indexes = [....]\n",
    "        # CC[indexes] = 1\n",
    "        \n",
    "        # tmp[torch.arange(S).unsqueeze(-1), torch.tensor(df.values)] = 1\n",
    "        \n",
    "                \n",
    "        masks['S2C'] = Tensor.contiguous(tmp)\n",
    "\n",
    "        # catagory to field\n",
    "        masks['C2F'] = self.MASKS_FULL['C2F']\n",
    "        \n",
    "        self.MASKS = masks\n",
    "        self.nodes_num['K'] = sample_size\n",
    "        \n",
    "    def make_mask_all(self,\n",
    "                  sample_indices: Optional[torch.tensor] = None,\n",
    "                ):\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        # caculate masking\n",
    "        masks = {}\n",
    "        \n",
    "        # label to sample \n",
    "        tmp = torch.zeros([math.ceil(S/8) * 8, math.ceil(L/8) * 8], dtype=torch.float, device=DEVICE)\n",
    "        label_ids = self.TARGET_POOL[self.LABEL_COLUMN].unique()\n",
    "        for i, value_df in enumerate(self.TARGET_POOL[self.LABEL_COLUMN]):\n",
    "            for j, value_label in enumerate(label_ids):\n",
    "                if value_label == value_df:\n",
    "                    tmp[i][j] = 1\n",
    "                    break\n",
    "        masks['L2S'] = tmp\n",
    "\n",
    "        # sample to catagory\n",
    "        tmp = torch.zeros([math.ceil(C/8) * 8, math.ceil(S/8) * 8], dtype=torch.float, device=DEVICE).T\n",
    "        tmp_df = self.TARGET_POOL.drop(self.LABEL_COLUMN, axis=1)\n",
    "        # for i, value_df in enumerate(tmp_df.values):\n",
    "        #     for j, value in enumerate(value_df):\n",
    "        #         tmp[value][i] = 1\n",
    "        tmp[torch.arange(len(self.TARGET_POOL)).unsqueeze(-1), torch.tensor(tmp_df.values)] = 1\n",
    "        tmp = tmp.T.contiguous()\n",
    "        masks['S2C'] = tmp\n",
    "\n",
    "        # catagory to field\n",
    "        tmp = torch.zeros([math.ceil(F/8) * 8, math.ceil(C/8) * 8], dtype=torch.float, device=DEVICE)\n",
    "        unique_items = [sorted(self.FEATURE_POOL[column].unique()) for column in (self.FEATURE_POOL.columns)]\n",
    "        for i in range(F):\n",
    "            for j in (unique_items[i]):\n",
    "                tmp[i][j] = 1\n",
    "        masks['C2F'] = tmp\n",
    "        \n",
    "        \n",
    "        self.MASKS = masks\n",
    "        self.MASKS_FULL = masks\n",
    "\n",
    "    \n",
    "    def make_input_tensor(self):\n",
    "        # make input tensor\n",
    "        L, S, C, F = self.nodes_num['L'], self.nodes_num['S'], self.nodes_num['C'], self.nodes_num['F']\n",
    "        # L\n",
    "        L_input = torch.tensor([range(L)], device=DEVICE).reshape(-1,1)\n",
    "        print('L_input', L_input.type(), L_input.shape)\n",
    "        \n",
    "        # S (normalized by standard scaler)\n",
    "        # features = torch.tensor(self.FEATURE_POOL.values, device=DEVICE).float()\n",
    "        # normalized_features = (features - torch.mean(features, dim = 0)) / torch.std(features, dim = 0)\n",
    "        # S_input = torch.cat([normalized_features, torch.tensor([[0]*F], device=DEVICE)],dim = 0).float() # add infering node\n",
    "        \n",
    "        # S (initialize by random)\n",
    "        S_input = torch.rand(128, device=DEVICE).repeat(S,1)\n",
    "        \n",
    "        print('S_input', S_input.type(), S_input.shape)\n",
    "        # C \n",
    "        C_input = torch.tensor([self.C_POOL], device=DEVICE).reshape(-1,1)\n",
    "        print('C_input', C_input.type(), C_input.shape)\n",
    "        # F \n",
    "        F_input = torch.tensor([range(F)], device=DEVICE).reshape(-1,1)\n",
    "        print('F_input', F_input.type(), F_input.shape)\n",
    "        # \n",
    "        self.INPUTS = (L_input, S_input, C_input, F_input)\n",
    "        self.INPUT_DIMS = (L_input.size(1), S_input.size(1), C_input.size(1), F_input.size(1))\n",
    "        \n",
    "    def sample_with_distrubution(self, sample_size):\n",
    "        # sample with distrubution\n",
    "        \"\"\"\n",
    "        currently, only support binary label\n",
    "        forced to make balenced sample\n",
    "        \"\"\"\n",
    "        \n",
    "        # decide each label's number of samples (fourced to be balenced if possible) \n",
    "        label_list = []\n",
    "        label_unique = list(self.labe_to_index.keys())\n",
    "        count = sample_size // len(label_unique)\n",
    "        remainder = sample_size % len(label_unique)\n",
    "        label_list = [item for item in label_unique for _ in range(count)]\n",
    "        label_list.extend(random.sample(label_unique, remainder))\n",
    "        # sample from indexes\n",
    "        indices = [random.choice(self.labe_to_index[label]) for label in label_list]\n",
    "        return indices     \n",
    "        \n",
    "    def get_sample(self, sample_size, inculde = []):\n",
    "        # get K samples from S\n",
    "        # return sample node indices\n",
    "        \n",
    "        # inculde specific nodes (e.g. query nodes), while remaining sample_size\n",
    "        sample_indices = self.sample_with_distrubution(sample_size - len(inculde))\n",
    "        if inculde is not []:\n",
    "            while inculde in sample_indices:\n",
    "                sample_indices = self.sample_with_distrubution(sample_size - len(inculde))\n",
    "            # add inculde nodes into sample_indices\n",
    "            for node in inculde:\n",
    "                sample_indices.append(node)\n",
    "            sample_indices = sorted(sample_indices)\n",
    "        # 作傻事\n",
    "        # sample_indices = [random.choice(Train_data.labe_to_index[472]) for i in range(sample_size-1)]\n",
    "        # sample_indices.append(inculde[0])\n",
    "        # update mask\n",
    "        sample_indices = sorted(sample_indices)\n",
    "        self.make_mask(sample_indices)\n",
    "        \n",
    "        # modify input tensor\n",
    "        L_input, S_input, C_input, F_input = self.INPUTS\n",
    "        S_input_masked = torch.index_select(S_input, 0, torch.tensor(sample_indices, device=DEVICE))\n",
    "        self.MASKED_INPUTS = (L_input, S_input_masked, C_input, F_input) \n",
    "          \n",
    "        return sample_indices\n",
    "            \n",
    "Train_data = HGNN_DataSet( main_df, 0.8, 'income')\n",
    "Train_data.get_sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.INPUTS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2946, 0.1288, 0.5390,  ..., 0.9143, 0.3863, 0.6151],\n",
       "        [0.2946, 0.1288, 0.5390,  ..., 0.9143, 0.3863, 0.6151],\n",
       "        [0.2946, 0.1288, 0.5390,  ..., 0.9143, 0.3863, 0.6151],\n",
       "        ...,\n",
       "        [0.2946, 0.1288, 0.5390,  ..., 0.9143, 0.3863, 0.6151],\n",
       "        [0.2946, 0.1288, 0.5390,  ..., 0.9143, 0.3863, 0.6151],\n",
       "        [0.2946, 0.1288, 0.5390,  ..., 0.9143, 0.3863, 0.6151]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.INPUTS[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_data.get_sample(100, inculde=[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Optional, Any, Union, Callable\n",
    "\n",
    "class CustomTransformerDecoderLayer(nn.TransformerDecoderLayer):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu'):\n",
    "        super().__init__(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        # remove defined modules\n",
    "        delattr(self, 'self_attn')\n",
    "        delattr(self, 'norm1')\n",
    "        delattr(self, 'dropout1')\n",
    "    \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        x = tgt\n",
    "        # x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))\n",
    "        x = self.norm2(x + self._mha_block(x, memory, memory_mask))\n",
    "        # x =  x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask)\n",
    "        # x = self.norm3(x + self._ff_block(x))\n",
    "\n",
    "        return x\n",
    "    def _mha_block(self, x: Tensor, mem: Tensor,\n",
    "                   attn_mask: Optional[Tensor],) -> Tensor:\n",
    "        x = xops.memory_efficient_attention(x, mem, mem, attn_mask)\n",
    "        # return self.dropout2(x)\n",
    "        return (x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dims (1, 128, 1, 1)\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]], device='cuda:0')\n",
      "tensor([[0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]], device='cuda:0')\n",
      "模型輸出的大小: torch.Size([4, 2])\n",
      "tensor([[-0.4088, -0.4096],\n",
      "        [-0.4112, -0.2992],\n",
      "        [-0.4241, -0.3049],\n",
      "        [-0.4226, -0.3054]], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baic transformer decoder model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "from tqdm import trange\n",
    "\n",
    "class TransformerDecoderModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 target_dataset, \n",
    "                 num_layers, \n",
    "                 embedding_dim,  \n",
    "                 subgraph_masked: Optional[bool] = False,\n",
    "                 K : Optional[int] = 10,\n",
    "                 ):\n",
    "        super(TransformerDecoderModel, self).__init__()\n",
    "\n",
    "        L_dim, S_dim, C_dim, F_dim = target_dataset.INPUT_DIMS\n",
    "        L, S, C, F = target_dataset.nodes_num['L'], target_dataset.nodes_num['S'], target_dataset.nodes_num['C'], target_dataset.nodes_num['F']\n",
    "        num_NUM , num_CAT = target_dataset.NUM_vs_CAT\n",
    "        \n",
    "        # check input dims\n",
    "        # if num_CAT + num_NUM != S_dim:\n",
    "        #     raise ValueError('num_CAT + num_NUM != number of columns (S_dim)   {} + {} != {}'.format(num_CAT, num_NUM, S_dim))\n",
    "        \n",
    "        # 目前b卡在embedding的怎麼用\n",
    "        # Catagory_embedding => 數值類Qcut後用linear來做embedding, 類別用nn.Embedding\n",
    "\n",
    "        \n",
    "        # nn.Embedding( number of possible catagories, embedding_dim, )\n",
    "        # nn.Linear( number of input dimantion, embedding_dim, )\n",
    "\n",
    "        self.Lable_embedding = nn.Embedding(L, embedding_dim, dtype=torch.float)\n",
    "        \n",
    "        # self.Sample_embedding_num = nn.Linear(num_NUM, embedding_dim, dtype=torch.float)\n",
    "        # # use MLP projector to project sample feature from 8 dim to 128 dim\n",
    "        # # self.Sample_embedding_cat = nn.Linear(num_CAT, embedding_dim, dtype=torch.float)\n",
    "        # self.Sample_embedding_cat = nn.Sequential(\n",
    "        #                                 nn.Linear(num_CAT, 64),  \n",
    "        #                                 nn.ReLU(),        \n",
    "        #                                 nn.Linear(64, embedding_dim) \n",
    "        #                             )\n",
    "                                    \n",
    "        \n",
    "        # self.Catagory_embedding_num = nn.Linear(C_dim, embedding_dim, dtype=torch.float)\n",
    "        # for every numrical filed, construct it's own Linear embedding layer\n",
    "        self.Catagory_embedding_nums = []\n",
    "        for i in range(num_NUM):\n",
    "            self.Catagory_embedding_nums.append(\n",
    "                nn.Linear(C_dim, embedding_dim, dtype=torch.float, device=DEVICE)\n",
    "            )\n",
    "        catagories = target_dataset.nodes_of_fields[-num_CAT:] # number of all possible catagories nodes\n",
    "        self.Catagory_embedding_cat = nn.Embedding(sum(catagories), embedding_dim, dtype=torch.float)\n",
    "        \n",
    "        self.Field_embedding = nn.Embedding(F, embedding_dim, dtype=torch.float)\n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            CustomTransformerDecoderLayer(embedding_dim,  nhead = 2 ),\n",
    "            num_layers\n",
    "        )\n",
    "        \n",
    "        # downstream task\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 2),\n",
    "        )\n",
    "        \n",
    "        self.subgraph_masked = subgraph_masked\n",
    "        if subgraph_masked: \n",
    "            self.K = K\n",
    "            target_dataset.get_sample(self.K)\n",
    "        else:\n",
    "            # init mask\n",
    "            target_dataset.make_mask_all()\n",
    "        \n",
    "        self.tmpmask_L2S = target_dataset.MASKS['L2S'].clone()\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "                target_dataset: HGNN_DataSet, \n",
    "                ):\n",
    "        L, S, C, F = target_dataset.nodes_num['L'], target_dataset.nodes_num['S'], target_dataset.nodes_num['C'], target_dataset.nodes_num['F']\n",
    "        num_NUM, num_CAT = target_dataset.NUM_vs_CAT\n",
    "        \n",
    "        if self.subgraph_masked: \n",
    "            # sample K nodes from S to construct subgraph\n",
    "            # target_dataset.get_sample(self.K)  ==> do not do this again!!! for every query, only one \"get_sample\" is needed\n",
    "            L_input, S_input, C_input, F_input = target_dataset.MASKED_INPUTS\n",
    "            masks = target_dataset.MASKS\n",
    "            # K = target_dataset.nodes_num['K'] \n",
    "            S_ = self.K\n",
    "        else:\n",
    "            # to-do: this could make model slow\n",
    "            target_dataset.make_input_tensor()\n",
    "            L_input, S_input, C_input, F_input = target_dataset.INPUTS\n",
    "            target_dataset.make_mask_all()\n",
    "            masks = target_dataset.MASKS\n",
    "            S_ = S\n",
    "\n",
    "        # for S and C, we use two different embedding methods, for CAT and NUM, respectively\n",
    "        # Squeeze for making batch dimantion\n",
    "        L_embedded = self.Lable_embedding(L_input.long()).squeeze(1).unsqueeze(0).float()\n",
    "        \n",
    "        S_embedded = S_input.unsqueeze(0).float()\n",
    "\n",
    "\n",
    "\n",
    "        # for every numrical filed, use it's own Linear embedding layer\n",
    "        C_embedded_nums = []\n",
    "        field = target_dataset.nodes_of_fields\n",
    "        start = 0\n",
    "        for index, nodes in enumerate(field[:num_NUM]): # pick numrical fields\n",
    "            end = start + nodes\n",
    "            C_embedded_nums.append(self.Catagory_embedding_nums[index](C_input[start:end].float()).unsqueeze(0))\n",
    "            start = end\n",
    "        C_embedded_num = torch.cat(C_embedded_nums, dim = 1)\n",
    "        \n",
    "        catagorical_filed_nodes = sum(field[-num_CAT:]) # pick catagory fields\n",
    "        C_embedded_cat = self.Catagory_embedding_cat(C_input[-catagorical_filed_nodes:].squeeze(1).long() - sum(field[:num_NUM])).unsqueeze(0).float() # - sum(field[:num_NUM] because the embedding index should start from 0\n",
    "        C_embedded = torch.cat([C_embedded_num, C_embedded_cat], dim = 1)\n",
    "        \n",
    "        F_embedded = self.Field_embedding(F_input.long()).squeeze(1).unsqueeze(0).float()\n",
    "        \n",
    "        # print(L_embedded.shape, S_embedded.shape, C_embedded.shape, F_embedded.shape)\n",
    "        \n",
    "        \n",
    "        # propagate steps: L→S→C→F\n",
    "        #                  L←S←C←\n",
    "        # more steps more menory usage\n",
    "        PROPAGATE_STEPS = 1\n",
    "        for i in range(PROPAGATE_STEPS):\n",
    "            S_embedded = self.transformer_decoder(S_embedded,L_embedded, \n",
    "                                                memory_mask = self.tmpmask_L2S[:S_,:L]) \n",
    "            C_embedded = self.transformer_decoder(C_embedded,S_embedded,\n",
    "                                                memory_mask = masks['S2C'][:C,:S_])\n",
    "            F_embedded = self.transformer_decoder(F_embedded,C_embedded,\n",
    "                                                memory_mask = masks['C2F'][:F,:C])\n",
    "            C_embedded = self.transformer_decoder(C_embedded,F_embedded,\n",
    "                                                memory_mask = Tensor.contiguous(masks['C2F'].transpose(0, 1))[:C,:F])\n",
    "            S_embedded = self.transformer_decoder(S_embedded,C_embedded,\n",
    "                                                memory_mask = Tensor.contiguous(masks['S2C'].transpose(0, 1))[:S_,:C])\n",
    "            L_embedded = self.transformer_decoder(L_embedded,S_embedded, \n",
    "                                                memory_mask = Tensor.contiguous(self.tmpmask_L2S.transpose(0, 1))[:L,:S_])\n",
    "        \n",
    "        # print('after',S_embedded[0][0])\n",
    "        output = self.MLP(S_embedded)[0]\n",
    "        return output\n",
    "\n",
    "# 測試模型\n",
    "num_layers = 1  # TransformerDecoder 的層數\n",
    "embedding_dim = 128  # 嵌入維度\n",
    "hidden_dim = 64  \n",
    "\n",
    "print('input_dims', Train_data.INPUT_DIMS)\n",
    "model = TransformerDecoderModel(Train_data, num_layers, embedding_dim, subgraph_masked = True, K = 4).to(DEVICE)\n",
    "\n",
    "model.tmpmask_L2S = Train_data.MASKS['L2S'].clone().detach()\n",
    "model.tmpmask_L2S[0] = 0\n",
    "model.tmpmask_L2S[0][3-1] = 1 # make it as unseen label\n",
    "print(Train_data.MASKS['L2S'][:4,:3])\n",
    "print(model.tmpmask_L2S[:4,:3])\n",
    "\n",
    "outputs = model(Train_data)\n",
    "print(\"模型輸出的大小:\", outputs.shape)\n",
    "print(outputs)\n",
    "output_label = torch.argmax(outputs.softmax(dim=0), dim=1)\n",
    "output_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 20,  40,  90,  80, 150])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = torch.tensor([1,2,3])\n",
    "mask = torch.tensor([[1,0,0],\n",
    "                     [1,0,0],\n",
    "                     [0,1,0],\n",
    "                     [1,0,0],\n",
    "                     [0,1,0],])\n",
    "sample = torch.tensor([10,20,30,40,50])\n",
    "\n",
    "selected_labels = torch.zeros(mask.size(0), dtype=label.dtype)\n",
    "for i in range(mask.size(0)):\n",
    "    selected_labels[i] = label[mask[i].nonzero(as_tuple=True)]\n",
    "\n",
    "selected_labels * sample + sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.TARGET_POOL.iloc[3].income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/39073 [00:00<04:14, 153.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 5468/39073 [00:34<03:29, 160.34it/s]"
     ]
    }
   ],
   "source": [
    "# training\n",
    "from torch import autograd\n",
    "from torcheval.metrics.aggregation.auc import AUC\n",
    "from torcheval.metrics import BinaryAUROC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "tmp_log = []\n",
    "tmp__log = []\n",
    "def train(model, datset):\n",
    "    LABEL_POOL = datset.LABEL_POOL\n",
    "    weight=torch.from_numpy(np.array([0.2, 1])).float().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "    epochs = 200\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        # logs\n",
    "        loss_log = []\n",
    "        AUC_metric = BinaryAUROC().to(DEVICE)\n",
    "        \n",
    "        iter = 0\n",
    "        for index in trange(len(datset.FEATURE_POOL)): # query through all sample nodes (not infering node)\n",
    "            # for all query, input = sample K + 1 query smaple\n",
    "            sample_indices = datset.get_sample(model.K, inculde = [index])\n",
    "            \n",
    "            # modify the mask to mask out the queries node's edge to it's label node\n",
    "            L = datset.nodes_num['L']\n",
    "            query_index = sample_indices.index(index) # query_index: index of query node in sample_indices\n",
    "            model.tmpmask_L2S = datset.MASKS['L2S'].clone().detach()\n",
    "            model.tmpmask_L2S[query_index] = 0\n",
    "            model.tmpmask_L2S[query_index][L-1] = 1 # make it as unseen label\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(datset)\n",
    "            \n",
    "                \n",
    "            outputs = outputs[query_index]\n",
    "            # for trainning, only the query node's output is used\n",
    "            # caculate loss\n",
    "            if model.subgraph_masked:\n",
    "                # get the real label fo query node\n",
    "                LABEL_POOL_ = LABEL_POOL[index]\n",
    "            else:\n",
    "                LABEL_POOL_ = LABEL_POOL\n",
    "            \n",
    "            # print((LABEL_POOL_),Train_data.TARGET_POOL.iloc[index].income - 471)\n",
    "            # caculate loss\n",
    "            batch_loss = criterion(outputs, torch.tensor(LABEL_POOL_,device=DEVICE))\n",
    "            loss_log.append(batch_loss.item())\n",
    "            \n",
    "            # backpropagation\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            TRUE = (torch.argmax(torch.tensor(LABEL_POOL_,device=DEVICE)))\n",
    "            \n",
    "            outputs = outputs.softmax(dim=0)\n",
    "            pred_prob_of_is_1 = outputs[1] # the probability of the query node is 1 (from model output)\n",
    "            \n",
    "            \n",
    "            # tmp_log.append(float(pred_prob_of_is_1))\n",
    "            # tmp__log.append((TRUE))\n",
    "            \n",
    "            AUC_metric.update(torch.Tensor([pred_prob_of_is_1]),torch.Tensor([TRUE]))\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            iter += 1\n",
    "            # if iter >= 100:\n",
    "            #     break\n",
    "        # print('1 rate pre:',sum(tmp_log)/len(tmp_log),len(tmp_log))\n",
    "        # print('1 rate tru:',float(sum(tmp__log)/len(tmp__log)),len(tmp__log))\n",
    "        # print(tmp__log)\n",
    "        # print(TRUE)\n",
    "        # print(float(AUC_metric.compute()))\n",
    "        # AUC_metric.reset()\n",
    "        # AUC_metric.update(torch.Tensor(tmp_log),torch.Tensor(tmp__log))\n",
    "        # print(float(AUC_metric.compute()))\n",
    "        \n",
    "        \n",
    "\n",
    "        epoch_loss = sum(loss_log) / len(loss_log)\n",
    "        epoch_AUC = float(AUC_metric.compute()) \n",
    "\n",
    "        AUC_metric.reset()\n",
    "        # break\n",
    "        del loss_log, AUC_metric\n",
    "        tmp_log.append(float(epoch_loss))\n",
    "        tmp__log.append(float(epoch_AUC))\n",
    "        \n",
    "        print(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC: {epoch_AUC}\")\n",
    "        \n",
    "        \n",
    "        with open('logs/log.txt', 'a') as f:\n",
    "            f.write(f\"Epoch{epoch+1}/{epochs} | Loss: {epoch_loss} | AUC: {epoch_AUC}\\n\")\n",
    "\n",
    "model = TransformerDecoderModel(Train_data, num_layers, embedding_dim, subgraph_masked = True, K = 200).to(DEVICE)\n",
    "train(model, Train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(tmp_log)\n",
    "plt.plot(tmp__log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5]) torch.Size([3, 5])\n",
      "tensor([[ 0.5443,  0.3086,  0.7909,  0.6841,  0.5280],\n",
      "        [-2.2931,  0.9872, -0.0502, -0.1197,  0.3200],\n",
      "        [ 1.1290,  1.6344, -1.9714,  0.4158,  1.1178]], requires_grad=True)\n",
      "tensor([[0.6052, 0.0118, 0.0234, 0.2101, 0.1495],\n",
      "        [0.1090, 0.4449, 0.0584, 0.1606, 0.2271],\n",
      "        [0.0216, 0.3409, 0.1344, 0.3297, 0.1734]])\n",
      "1.694624423980713\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "print(input.shape, target.shape)\n",
    "print(input)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "print(output.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
